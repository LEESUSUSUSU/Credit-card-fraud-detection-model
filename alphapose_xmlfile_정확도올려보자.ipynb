{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LEESUSUSUSU/Credit-card-fraud-detection-model/blob/main/alphapose_xmlfile_%EC%A0%95%ED%99%95%EB%8F%84%EC%98%AC%EB%A0%A4%EB%B3%B4%EC%9E%90.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITXFPsQFu_8P"
      },
      "source": [
        "**I modified the Pillow installation part of the AlphaPose Colab example program.**\n",
        "\n",
        "**This is not thoth000's original program.**\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4wquAG3Co_z",
        "outputId": "3cb60e56-2af9-45f4-e069-1d983782c9f0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cxMU0dmlnCT",
        "outputId": "edf8066c-90f4-4bf8-8f5b-cf8dddb5485d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyyaml==5.2\n",
            "  Downloading PyYAML-5.2.tar.gz (265 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/265.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m204.8/265.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.26.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu113\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "2.4.1+cu121\n",
            "6.0.2\n",
            "1.13.1\n"
          ]
        }
      ],
      "source": [
        "! pip install pyyaml==5.2\n",
        "! pip install scipy\n",
        "! pip install numpy\n",
        "! pip3 install torch torchvision --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "import yaml, scipy, os\n",
        "print(yaml.__version__)\n",
        "print(scipy.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VBhQTOSoWab",
        "outputId": "0882bef9-edeb-4802-bde4-098051fd81ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AlphaPose'...\n",
            "remote: Enumerating objects: 2749, done.\u001b[K\n",
            "remote: Counting objects: 100% (10/10), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 2749 (delta 4), reused 1 (delta 0), pack-reused 2739 (from 1)\u001b[K\n",
            "Receiving objects: 100% (2749/2749), 118.82 MiB | 37.66 MiB/s, done.\n",
            "Resolving deltas: 100% (1379/1379), done.\n"
          ]
        }
      ],
      "source": [
        "!rm -rf /content/AlphaPose\n",
        "os.chdir('/content/')\n",
        "!git clone https://github.com/MVIG-SJTU/AlphaPose.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkkNtO8qolbz",
        "outputId": "c533b11c-5e5c-485d-ccd9-fe4d66725b8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (3.0.11)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Suggested packages:\n",
            "  libyaml-doc\n",
            "The following NEW packages will be installed:\n",
            "  libyaml-dev\n",
            "0 upgraded, 1 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 62.8 kB of archives.\n",
            "After this operation, 257 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libyaml-dev amd64 0.2.2-1build2 [62.8 kB]\n",
            "Fetched 62.8 kB in 1s (123 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libyaml-dev:amd64.\n",
            "(Reading database ... 123605 files and directories currently installed.)\n",
            "Preparing to unpack .../libyaml-dev_0.2.2-1build2_amd64.deb ...\n",
            "Unpacking libyaml-dev:amd64 (0.2.2-1build2) ...\n",
            "Setting up libyaml-dev:amd64 (0.2.2-1build2) ...\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install cython\n",
        "!sudo apt-get install libyaml-dev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-Gw3k4coyFD",
        "outputId": "6bbcf962-bc2e-40d6-b413-ec00b847e17e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/AlphaPose\n",
            "Compiling detector/nms/src/soft_nms_cpu.pyx because it changed.\n",
            "[1/1] Cythonizing detector/nms/src/soft_nms_cpu.pyx\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/__init__.py:85: _DeprecatedInstaller: setuptools.installer and fetch_build_eggs are deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Requirements should be satisfied by a PEP 517 installer.\n",
            "        If you are using pip, you can try `pip install --use-pep517`.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  dist.fetch_build_eggs(dist.setup_requires)\n",
            "running build\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib.linux-x86_64-cpython-310\n",
            "creating build/lib.linux-x86_64-cpython-310/trackers\n",
            "copying trackers/__init__.py -> build/lib.linux-x86_64-cpython-310/trackers\n",
            "copying trackers/tracker_api.py -> build/lib.linux-x86_64-cpython-310/trackers\n",
            "copying trackers/tracker_cfg.py -> build/lib.linux-x86_64-cpython-310/trackers\n",
            "creating build/lib.linux-x86_64-cpython-310/alphapose\n",
            "copying alphapose/__init__.py -> build/lib.linux-x86_64-cpython-310/alphapose\n",
            "copying alphapose/version.py -> build/lib.linux-x86_64-cpython-310/alphapose\n",
            "copying alphapose/opt.py -> build/lib.linux-x86_64-cpython-310/alphapose\n",
            "creating build/lib.linux-x86_64-cpython-310/trackers/ReidModels\n",
            "copying trackers/ReidModels/__init__.py -> build/lib.linux-x86_64-cpython-310/trackers/ReidModels\n",
            "copying trackers/ReidModels/ResBnLin.py -> build/lib.linux-x86_64-cpython-310/trackers/ReidModels\n",
            "copying trackers/ReidModels/ResNet.py -> build/lib.linux-x86_64-cpython-310/trackers/ReidModels\n",
            "copying trackers/ReidModels/bn_linear.py -> build/lib.linux-x86_64-cpython-310/trackers/ReidModels\n",
            "copying trackers/ReidModels/osnet.py -> build/lib.linux-x86_64-cpython-310/trackers/ReidModels\n",
            "copying trackers/ReidModels/resnet_fc.py -> build/lib.linux-x86_64-cpython-310/trackers/ReidModels\n",
            "copying trackers/ReidModels/osnet_ain.py -> build/lib.linux-x86_64-cpython-310/trackers/ReidModels\n",
            "copying trackers/ReidModels/net_utils.py -> build/lib.linux-x86_64-cpython-310/trackers/ReidModels\n",
            "creating build/lib.linux-x86_64-cpython-310/trackers/tracking\n",
            "copying trackers/tracking/__init__.py -> build/lib.linux-x86_64-cpython-310/trackers/tracking\n",
            "copying trackers/tracking/matching.py -> build/lib.linux-x86_64-cpython-310/trackers/tracking\n",
            "copying trackers/tracking/basetrack.py -> build/lib.linux-x86_64-cpython-310/trackers/tracking\n",
            "creating build/lib.linux-x86_64-cpython-310/trackers/ReidModels/psroi_pooling\n",
            "copying trackers/ReidModels/psroi_pooling/__init__.py -> build/lib.linux-x86_64-cpython-310/trackers/ReidModels/psroi_pooling\n",
            "copying trackers/ReidModels/psroi_pooling/build.py -> build/lib.linux-x86_64-cpython-310/trackers/ReidModels/psroi_pooling\n",
            "creating build/lib.linux-x86_64-cpython-310/trackers/ReidModels/backbone\n",
            "copying trackers/ReidModels/backbone/__init__.py -> build/lib.linux-x86_64-cpython-310/trackers/ReidModels/backbone\n",
            "copying trackers/ReidModels/backbone/lrn.py -> build/lib.linux-x86_64-cpython-310/trackers/ReidModels/backbone\n",
            "copying trackers/ReidModels/backbone/sqeezenet.py -> build/lib.linux-x86_64-cpython-310/trackers/ReidModels/backbone\n",
            "copying trackers/ReidModels/backbone/googlenet.py -> build/lib.linux-x86_64-cpython-310/trackers/ReidModels/backbone\n",
            "creating build/lib.linux-x86_64-cpython-310/trackers/ReidModels/reid\n",
            "copying trackers/ReidModels/reid/__init__.py -> build/lib.linux-x86_64-cpython-310/trackers/ReidModels/reid\n",
            "copying trackers/ReidModels/reid/image_part_aligned.py -> build/lib.linux-x86_64-cpython-310/trackers/ReidModels/reid\n",
            "creating build/lib.linux-x86_64-cpython-310/trackers/ReidModels/classification\n",
            "copying trackers/ReidModels/classification/__init__.py -> build/lib.linux-x86_64-cpython-310/trackers/ReidModels/classification\n",
            "copying trackers/ReidModels/classification/rfcn_cls.py -> build/lib.linux-x86_64-cpython-310/trackers/ReidModels/classification\n",
            "copying trackers/ReidModels/classification/classifier.py -> build/lib.linux-x86_64-cpython-310/trackers/ReidModels/classification\n",
            "creating build/lib.linux-x86_64-cpython-310/trackers/ReidModels/psroi_pooling/_ext\n",
            "copying trackers/ReidModels/psroi_pooling/_ext/__init__.py -> build/lib.linux-x86_64-cpython-310/trackers/ReidModels/psroi_pooling/_ext\n",
            "creating build/lib.linux-x86_64-cpython-310/trackers/ReidModels/psroi_pooling/functions\n",
            "copying trackers/ReidModels/psroi_pooling/functions/__init__.py -> build/lib.linux-x86_64-cpython-310/trackers/ReidModels/psroi_pooling/functions\n",
            "copying trackers/ReidModels/psroi_pooling/functions/psroi_pooling.py -> build/lib.linux-x86_64-cpython-310/trackers/ReidModels/psroi_pooling/functions\n",
            "creating build/lib.linux-x86_64-cpython-310/trackers/ReidModels/psroi_pooling/modules\n",
            "copying trackers/ReidModels/psroi_pooling/modules/__init__.py -> build/lib.linux-x86_64-cpython-310/trackers/ReidModels/psroi_pooling/modules\n",
            "copying trackers/ReidModels/psroi_pooling/modules/psroi_pool.py -> build/lib.linux-x86_64-cpython-310/trackers/ReidModels/psroi_pooling/modules\n",
            "creating build/lib.linux-x86_64-cpython-310/trackers/ReidModels/psroi_pooling/_ext/psroi_pooling\n",
            "copying trackers/ReidModels/psroi_pooling/_ext/psroi_pooling/__init__.py -> build/lib.linux-x86_64-cpython-310/trackers/ReidModels/psroi_pooling/_ext/psroi_pooling\n",
            "creating build/lib.linux-x86_64-cpython-310/trackers/tracking/utils\n",
            "copying trackers/tracking/utils/__init__.py -> build/lib.linux-x86_64-cpython-310/trackers/tracking/utils\n",
            "copying trackers/tracking/utils/parse_config.py -> build/lib.linux-x86_64-cpython-310/trackers/tracking/utils\n",
            "copying trackers/tracking/utils/utils.py -> build/lib.linux-x86_64-cpython-310/trackers/tracking/utils\n",
            "copying trackers/tracking/utils/timer.py -> build/lib.linux-x86_64-cpython-310/trackers/tracking/utils\n",
            "copying trackers/tracking/utils/kalman_filter.py -> build/lib.linux-x86_64-cpython-310/trackers/tracking/utils\n",
            "copying trackers/tracking/utils/nms.py -> build/lib.linux-x86_64-cpython-310/trackers/tracking/utils\n",
            "copying trackers/tracking/utils/io.py -> build/lib.linux-x86_64-cpython-310/trackers/tracking/utils\n",
            "creating build/lib.linux-x86_64-cpython-310/alphapose/models\n",
            "copying alphapose/models/__init__.py -> build/lib.linux-x86_64-cpython-310/alphapose/models\n",
            "copying alphapose/models/hrnet.py -> build/lib.linux-x86_64-cpython-310/alphapose/models\n",
            "copying alphapose/models/fastpose.py -> build/lib.linux-x86_64-cpython-310/alphapose/models\n",
            "copying alphapose/models/simplepose.py -> build/lib.linux-x86_64-cpython-310/alphapose/models\n",
            "copying alphapose/models/hardnet.py -> build/lib.linux-x86_64-cpython-310/alphapose/models\n",
            "copying alphapose/models/simple3dposeSMPLWithCam.py -> build/lib.linux-x86_64-cpython-310/alphapose/models\n",
            "copying alphapose/models/fastpose_duc_dense.py -> build/lib.linux-x86_64-cpython-310/alphapose/models\n",
            "copying alphapose/models/builder.py -> build/lib.linux-x86_64-cpython-310/alphapose/models\n",
            "copying alphapose/models/fastpose_duc.py -> build/lib.linux-x86_64-cpython-310/alphapose/models\n",
            "copying alphapose/models/criterion.py -> build/lib.linux-x86_64-cpython-310/alphapose/models\n",
            "creating build/lib.linux-x86_64-cpython-310/alphapose/datasets\n",
            "copying alphapose/datasets/halpe_68_noface_det.py -> build/lib.linux-x86_64-cpython-310/alphapose/datasets\n",
            "copying alphapose/datasets/custom.py -> build/lib.linux-x86_64-cpython-310/alphapose/datasets\n",
            "copying alphapose/datasets/__init__.py -> build/lib.linux-x86_64-cpython-310/alphapose/datasets\n",
            "copying alphapose/datasets/halpe_136_det.py -> build/lib.linux-x86_64-cpython-310/alphapose/datasets\n",
            "copying alphapose/datasets/halpe_26.py -> build/lib.linux-x86_64-cpython-310/alphapose/datasets\n",
            "copying alphapose/datasets/mpii.py -> build/lib.linux-x86_64-cpython-310/alphapose/datasets\n",
            "copying alphapose/datasets/halpe_136.py -> build/lib.linux-x86_64-cpython-310/alphapose/datasets\n",
            "copying alphapose/datasets/concat_dataset.py -> build/lib.linux-x86_64-cpython-310/alphapose/datasets\n",
            "copying alphapose/datasets/halpe_coco_wholebody_26.py -> build/lib.linux-x86_64-cpython-310/alphapose/datasets\n",
            "copying alphapose/datasets/coco_wholebody.py -> build/lib.linux-x86_64-cpython-310/alphapose/datasets\n",
            "copying alphapose/datasets/coco_det.py -> build/lib.linux-x86_64-cpython-310/alphapose/datasets\n",
            "copying alphapose/datasets/halpe_coco_wholebody_136_det.py -> build/lib.linux-x86_64-cpython-310/alphapose/datasets\n",
            "copying alphapose/datasets/single_hand.py -> build/lib.linux-x86_64-cpython-310/alphapose/datasets\n",
            "copying alphapose/datasets/single_hand_det.py -> build/lib.linux-x86_64-cpython-310/alphapose/datasets\n",
            "copying alphapose/datasets/halpe_68_noface.py -> build/lib.linux-x86_64-cpython-310/alphapose/datasets\n",
            "copying alphapose/datasets/coco_wholebody_det.py -> build/lib.linux-x86_64-cpython-310/alphapose/datasets\n",
            "copying alphapose/datasets/halpe_coco_wholebody_26_det.py -> build/lib.linux-x86_64-cpython-310/alphapose/datasets\n",
            "copying alphapose/datasets/halpe_coco_wholebody_136.py -> build/lib.linux-x86_64-cpython-310/alphapose/datasets\n",
            "copying alphapose/datasets/mscoco.py -> build/lib.linux-x86_64-cpython-310/alphapose/datasets\n",
            "copying alphapose/datasets/halpe_26_det.py -> build/lib.linux-x86_64-cpython-310/alphapose/datasets\n",
            "creating build/lib.linux-x86_64-cpython-310/alphapose/utils\n",
            "copying alphapose/utils/env.py -> build/lib.linux-x86_64-cpython-310/alphapose/utils\n",
            "copying alphapose/utils/__init__.py -> build/lib.linux-x86_64-cpython-310/alphapose/utils\n",
            "copying alphapose/utils/webcam_detector.py -> build/lib.linux-x86_64-cpython-310/alphapose/utils\n",
            "copying alphapose/utils/render_pytorch3d.py -> build/lib.linux-x86_64-cpython-310/alphapose/utils\n",
            "copying alphapose/utils/detector.py -> build/lib.linux-x86_64-cpython-310/alphapose/utils\n",
            "copying alphapose/utils/transforms.py -> build/lib.linux-x86_64-cpython-310/alphapose/utils\n",
            "copying alphapose/utils/logger.py -> build/lib.linux-x86_64-cpython-310/alphapose/utils\n",
            "copying alphapose/utils/metrics.py -> build/lib.linux-x86_64-cpython-310/alphapose/utils\n",
            "copying alphapose/utils/registry.py -> build/lib.linux-x86_64-cpython-310/alphapose/utils\n",
            "copying alphapose/utils/writer.py -> build/lib.linux-x86_64-cpython-310/alphapose/utils\n",
            "copying alphapose/utils/config.py -> build/lib.linux-x86_64-cpython-310/alphapose/utils\n",
            "copying alphapose/utils/pPose_nms.py -> build/lib.linux-x86_64-cpython-310/alphapose/utils\n",
            "copying alphapose/utils/writer_smpl.py -> build/lib.linux-x86_64-cpython-310/alphapose/utils\n",
            "copying alphapose/utils/bbox.py -> build/lib.linux-x86_64-cpython-310/alphapose/utils\n",
            "copying alphapose/utils/file_detector.py -> build/lib.linux-x86_64-cpython-310/alphapose/utils\n",
            "copying alphapose/utils/vis.py -> build/lib.linux-x86_64-cpython-310/alphapose/utils\n",
            "creating build/lib.linux-x86_64-cpython-310/alphapose/utils/presets\n",
            "copying alphapose/utils/presets/__init__.py -> build/lib.linux-x86_64-cpython-310/alphapose/utils/presets\n",
            "copying alphapose/utils/presets/simple_transform.py -> build/lib.linux-x86_64-cpython-310/alphapose/utils/presets\n",
            "copying alphapose/utils/presets/simple_transform_3d_smpl.py -> build/lib.linux-x86_64-cpython-310/alphapose/utils/presets\n",
            "creating build/lib.linux-x86_64-cpython-310/alphapose/utils/roi_align\n",
            "copying alphapose/utils/roi_align/__init__.py -> build/lib.linux-x86_64-cpython-310/alphapose/utils/roi_align\n",
            "copying alphapose/utils/roi_align/roi_align.py -> build/lib.linux-x86_64-cpython-310/alphapose/utils/roi_align\n",
            "running build_ext\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:495: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "  warnings.warn(msg.format('we could not find ninja.'))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:414: UserWarning: The detected CUDA version (12.2) has a minor version mismatch with the version that was used to compile PyTorch (12.1). Most likely this shouldn't be a problem.\n",
            "  warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:424: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 12.2\n",
            "  warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n",
            "building 'detector.nms.soft_nms_cpu' extension\n",
            "creating build/temp.linux-x86_64-cpython-310\n",
            "creating build/temp.linux-x86_64-cpython-310/detector\n",
            "creating build/temp.linux-x86_64-cpython-310/detector/nms\n",
            "creating build/temp.linux-x86_64-cpython-310/detector/nms/src\n",
            "x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/numpy/core/include -I/usr/include/python3.10 -c detector/nms/src/soft_nms_cpu.cpp -o build/temp.linux-x86_64-cpython-310/detector/nms/src/soft_nms_cpu.o -Wno-unused-function -Wno-write-strings -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=soft_nms_cpu -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/numpy/core/include/numpy/ndarraytypes.h:1929\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/numpy/core/include/numpy/ndarrayobject.h:12\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/numpy/core/include/numpy/arrayobject.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kdetector/nms/src/soft_nms_cpu.cpp:1268\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K#warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wcpp\u0007-Wcpp\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   17 | #\u001b[01;35m\u001b[Kwarning\u001b[m\u001b[K \"Using deprecated NumPy API, disable it with \" \\\n",
            "      |  \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\n",
            "creating build/lib.linux-x86_64-cpython-310/detector\n",
            "creating build/lib.linux-x86_64-cpython-310/detector/nms\n",
            "x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/detector/nms/src/soft_nms_cpu.o -L/usr/lib/x86_64-linux-gnu -o build/lib.linux-x86_64-cpython-310/detector/nms/soft_nms_cpu.cpython-310-x86_64-linux-gnu.so\n",
            "building 'detector.nms.nms_cpu' extension\n",
            "x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c detector/nms/src/nms_cpu.cpp -o build/temp.linux-x86_64-cpython-310/detector/nms/src/nms_cpu.o -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=nms_cpu -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/detector/nms/src/nms_cpu.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/detector/nms/nms_cpu.cpython-310-x86_64-linux-gnu.so\n",
            "building 'detector.nms.nms_cuda' extension\n",
            "x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c detector/nms/src/nms_cuda.cpp -o build/temp.linux-x86_64-cpython-310/detector/nms/src/nms_cuda.o -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=nms_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c detector/nms/src/nms_kernel.cu -o build/temp.linux-x86_64-cpython-310/detector/nms/src/nms_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=nms_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/detector/nms/src/nms_cuda.o build/temp.linux-x86_64-cpython-310/detector/nms/src/nms_kernel.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/detector/nms/nms_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "building 'alphapose.utils.roi_align.roi_align_cuda' extension\n",
            "creating build/temp.linux-x86_64-cpython-310/alphapose\n",
            "creating build/temp.linux-x86_64-cpython-310/alphapose/utils\n",
            "creating build/temp.linux-x86_64-cpython-310/alphapose/utils/roi_align\n",
            "creating build/temp.linux-x86_64-cpython-310/alphapose/utils/roi_align/src\n",
            "x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c alphapose/utils/roi_align/src/roi_align_cuda.cpp -o build/temp.linux-x86_64-cpython-310/alphapose/utils/roi_align/src/roi_align_cuda.o -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=roi_align_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c alphapose/utils/roi_align/src/roi_align_kernel.cu -o build/temp.linux-x86_64-cpython-310/alphapose/utils/roi_align/src/roi_align_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=roi_align_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/alphapose/utils/roi_align/src/roi_align_cuda.o build/temp.linux-x86_64-cpython-310/alphapose/utils/roi_align/src/roi_align_kernel.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/alphapose/utils/roi_align/roi_align_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "building 'alphapose.models.layers.dcn.deform_conv_cuda' extension\n",
            "creating build/temp.linux-x86_64-cpython-310/alphapose/models\n",
            "creating build/temp.linux-x86_64-cpython-310/alphapose/models/layers\n",
            "creating build/temp.linux-x86_64-cpython-310/alphapose/models/layers/dcn\n",
            "creating build/temp.linux-x86_64-cpython-310/alphapose/models/layers/dcn/src\n",
            "x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c alphapose/models/layers/dcn/src/deform_conv_cuda.cpp -o build/temp.linux-x86_64-cpython-310/alphapose/models/layers/dcn/src/deform_conv_cuda.o -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=deform_conv_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c alphapose/models/layers/dcn/src/deform_conv_cuda_kernel.cu -o build/temp.linux-x86_64-cpython-310/alphapose/models/layers/dcn/src/deform_conv_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=deform_conv_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "creating build/lib.linux-x86_64-cpython-310/alphapose/models/layers\n",
            "creating build/lib.linux-x86_64-cpython-310/alphapose/models/layers/dcn\n",
            "x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/alphapose/models/layers/dcn/src/deform_conv_cuda.o build/temp.linux-x86_64-cpython-310/alphapose/models/layers/dcn/src/deform_conv_cuda_kernel.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/alphapose/models/layers/dcn/deform_conv_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "building 'alphapose.models.layers.dcn.deform_pool_cuda' extension\n",
            "x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c alphapose/models/layers/dcn/src/deform_pool_cuda.cpp -o build/temp.linux-x86_64-cpython-310/alphapose/models/layers/dcn/src/deform_pool_cuda.o -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=deform_pool_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c alphapose/models/layers/dcn/src/deform_pool_cuda_kernel.cu -o build/temp.linux-x86_64-cpython-310/alphapose/models/layers/dcn/src/deform_pool_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=deform_pool_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/alphapose/models/layers/dcn/src/deform_pool_cuda.o build/temp.linux-x86_64-cpython-310/alphapose/models/layers/dcn/src/deform_pool_cuda_kernel.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/alphapose/models/layers/dcn/deform_pool_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "running develop\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/command/develop.py:42: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` and ``easy_install``.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  easy_install.initialize_options(self)\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "running egg_info\n",
            "creating alphapose.egg-info\n",
            "writing alphapose.egg-info/PKG-INFO\n",
            "writing dependency_links to alphapose.egg-info/dependency_links.txt\n",
            "writing requirements to alphapose.egg-info/requires.txt\n",
            "writing top-level names to alphapose.egg-info/top_level.txt\n",
            "writing manifest file 'alphapose.egg-info/SOURCES.txt'\n",
            "dependency /usr/local/lib/python3.10/dist-packages/numpy/core/include/numpy/arrayobject.h won't be automatically included in the manifest: the path must be relative\n",
            "dependency /usr/local/lib/python3.10/dist-packages/numpy/core/include/numpy/arrayscalars.h won't be automatically included in the manifest: the path must be relative\n",
            "dependency /usr/local/lib/python3.10/dist-packages/numpy/core/include/numpy/ndarrayobject.h won't be automatically included in the manifest: the path must be relative\n",
            "dependency /usr/local/lib/python3.10/dist-packages/numpy/core/include/numpy/ndarraytypes.h won't be automatically included in the manifest: the path must be relative\n",
            "dependency /usr/local/lib/python3.10/dist-packages/numpy/core/include/numpy/ufuncobject.h won't be automatically included in the manifest: the path must be relative\n",
            "reading manifest file 'alphapose.egg-info/SOURCES.txt'\n",
            "adding license file 'LICENSE'\n",
            "writing manifest file 'alphapose.egg-info/SOURCES.txt'\n",
            "running build_ext\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:414: UserWarning: The detected CUDA version (12.2) has a minor version mismatch with the version that was used to compile PyTorch (12.1). Most likely this shouldn't be a problem.\n",
            "  warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:424: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 12.2\n",
            "  warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n",
            "copying build/lib.linux-x86_64-cpython-310/detector/nms/soft_nms_cpu.cpython-310-x86_64-linux-gnu.so -> detector/nms\n",
            "copying build/lib.linux-x86_64-cpython-310/detector/nms/nms_cpu.cpython-310-x86_64-linux-gnu.so -> detector/nms\n",
            "copying build/lib.linux-x86_64-cpython-310/detector/nms/nms_cuda.cpython-310-x86_64-linux-gnu.so -> detector/nms\n",
            "copying build/lib.linux-x86_64-cpython-310/alphapose/utils/roi_align/roi_align_cuda.cpython-310-x86_64-linux-gnu.so -> alphapose/utils/roi_align\n",
            "copying build/lib.linux-x86_64-cpython-310/alphapose/models/layers/dcn/deform_conv_cuda.cpython-310-x86_64-linux-gnu.so -> alphapose/models/layers/dcn\n",
            "copying build/lib.linux-x86_64-cpython-310/alphapose/models/layers/dcn/deform_pool_cuda.cpython-310-x86_64-linux-gnu.so -> alphapose/models/layers/dcn\n",
            "Creating /usr/local/lib/python3.10/dist-packages/alphapose.egg-link (link to .)\n",
            "Adding alphapose 0.5.0+c60106d to easy-install.pth file\n",
            "\n",
            "Installed /content/AlphaPose\n",
            "Processing dependencies for alphapose==0.5.0+c60106d\n",
            "Searching for timm==0.1.20\n",
            "Reading https://pypi.tuna.tsinghua.edu.cn/simple/timm/\n",
            "Downloading https://pypi.tuna.tsinghua.edu.cn/packages/89/26/ba294669cc5cc4d09efd1964c8df752dc0955ac26f86bdeec582aed77d1d/timm-0.1.20-py3-none-any.whl#sha256=f63fca201f637dfdd169fb187b5c2d06b8e973d537d2517667a80e57ca1bae7a\n",
            "Best match: timm 0.1.20\n",
            "Processing timm-0.1.20-py3-none-any.whl\n",
            "Installing timm-0.1.20-py3-none-any.whl to /usr/local/lib/python3.10/dist-packages\n",
            "Adding timm 0.1.20 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.10/dist-packages/timm-0.1.20-py3.10.egg\n",
            "Searching for munkres\n",
            "Reading https://pypi.tuna.tsinghua.edu.cn/simple/munkres/\n",
            "Downloading https://pypi.tuna.tsinghua.edu.cn/packages/90/ab/0301c945a704218bc9435f0e3c88884f6b19ef234d8899fb47ce1ccfd0c9/munkres-1.1.4-py2.py3-none-any.whl#sha256=6b01867d4a8480d865aea2326e4b8f7c46431e9e55b4a2e32d989307d7bced2a\n",
            "Best match: munkres 1.1.4\n",
            "Processing munkres-1.1.4-py2.py3-none-any.whl\n",
            "Installing munkres-1.1.4-py2.py3-none-any.whl to /usr/local/lib/python3.10/dist-packages\n",
            "Adding munkres 1.1.4 to easy-install.pth file\n",
            "detected new path './timm-0.1.20-py3.10.egg'\n",
            "\n",
            "Installed /usr/local/lib/python3.10/dist-packages/munkres-1.1.4-py3.10.egg\n",
            "Searching for halpecocotools\n",
            "Reading https://pypi.tuna.tsinghua.edu.cn/simple/halpecocotools/\n",
            "Downloading https://pypi.tuna.tsinghua.edu.cn/packages/f9/b0/f2e783a29a30cf0e66e3bbc45cdc4da6e9214bc21f4df948ee9a7d79764e/halpecocotools-0.0.0.tar.gz#sha256=8355964f7d14e69b2a555be7e116eae8979ec2a04f0eef16d140b38d04f933eb\n",
            "Best match: halpecocotools 0.0.0\n",
            "Processing halpecocotools-0.0.0.tar.gz\n",
            "Writing /tmp/easy_install-qjsd9eei/halpecocotools-0.0.0/setup.cfg\n",
            "Running halpecocotools-0.0.0/setup.py -q bdist_egg --dist-dir /tmp/easy_install-qjsd9eei/halpecocotools-0.0.0/egg-dist-tmp-l2fa5mph\n",
            "warning: no files found matching '*.pxd'\n",
            "warning: no files found matching '*.pyx'\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "/usr/local/lib/python3.10/dist-packages/Cython/Compiler/Main.py:381: FutureWarning: Cython directive 'language_level' not set, using '3str' for now (Py3). This has changed from earlier releases! File: /tmp/easy_install-qjsd9eei/halpecocotools-0.0.0/halpecocotools/_mask.pyx\n",
            "  tree = Parsing.p_module(s, pxd, full_module_name)\n",
            "Traceback (most recent call last):\n",
            "  File \"Cython/Compiler/Visitor.py\", line 182, in Cython.Compiler.Visitor.TreeVisitor._visit\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/Cython/Compiler/ParseTreeTransforms.py\", line 3113, in visit_StatListNode\n",
            "    if not self.current_directives['remove_unreachable']:\n",
            "TypeError: 'NoneType' object is not subscriptable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/sandbox.py\", line 158, in save_modules\n",
            "    yield saved\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/sandbox.py\", line 200, in setup_context\n",
            "    yield\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/sandbox.py\", line 261, in run_setup\n",
            "    _execfile(setup_script, ns)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/sandbox.py\", line 48, in _execfile\n",
            "    exec(code, globals, locals)\n",
            "  File \"/tmp/easy_install-qjsd9eei/halpecocotools-0.0.0/setup.py\", line 16, in <module>\n",
            "    \n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/__init__.py\", line 108, in setup\n",
            "    return distutils.core.setup(**attrs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/core.py\", line 184, in setup\n",
            "    return run_commands(dist)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/core.py\", line 200, in run_commands\n",
            "    dist.run_commands()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/dist.py\", line 970, in run_commands\n",
            "    self.run_command(cmd)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/dist.py\", line 956, in run_command\n",
            "    super().run_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/dist.py\", line 989, in run_command\n",
            "    cmd_obj.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/command/bdist_egg.py\", line 167, in run\n",
            "    cmd = self.call_command('install_lib', warn_dir=False)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/command/bdist_egg.py\", line 153, in call_command\n",
            "    self.run_command(cmdname)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py\", line 316, in run_command\n",
            "    self.distribution.run_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/dist.py\", line 956, in run_command\n",
            "    super().run_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/dist.py\", line 989, in run_command\n",
            "    cmd_obj.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/command/install_lib.py\", line 12, in run\n",
            "    self.build()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/command/install_lib.py\", line 110, in build\n",
            "    self.run_command('build_ext')\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py\", line 316, in run_command\n",
            "    self.distribution.run_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/dist.py\", line 956, in run_command\n",
            "    super().run_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/dist.py\", line 989, in run_command\n",
            "    cmd_obj.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/command/build_ext.py\", line 93, in run\n",
            "    _build_ext.run(self)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/command/build_ext.py\", line 359, in run\n",
            "    self.build_extensions()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/command/build_ext.py\", line 479, in build_extensions\n",
            "    self._build_extensions_serial()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/command/build_ext.py\", line 505, in _build_extensions_serial\n",
            "    self.build_extension(ext)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/command/build_ext.py\", line 254, in build_extension\n",
            "    _build_ext.build_extension(self, ext)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/Cython/Distutils/build_ext.py\", line 130, in build_extension\n",
            "    new_ext = cythonize(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/Cython/Build/Dependencies.py\", line 1154, in cythonize\n",
            "    cythonize_one(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/Cython/Build/Dependencies.py\", line 1300, in cythonize_one\n",
            "    result = compile_single(pyx_file, options, full_module_name=full_module_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/Cython/Compiler/Main.py\", line 615, in compile_single\n",
            "    return run_pipeline(source, options, full_module_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/Cython/Compiler/Main.py\", line 539, in run_pipeline\n",
            "    err, enddata = Pipeline.run_pipeline(pipeline, source)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/Cython/Compiler/Pipeline.py\", line 398, in run_pipeline\n",
            "    data = run(phase, data)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/Cython/Compiler/Pipeline.py\", line 375, in run\n",
            "    return phase(data)\n",
            "  File \"Cython/Compiler/Visitor.py\", line 312, in Cython.Compiler.Visitor.CythonTransform.__call__\n",
            "  File \"Cython/Compiler/Visitor.py\", line 294, in Cython.Compiler.Visitor.VisitorTransform.__call__\n",
            "  File \"Cython/Compiler/Visitor.py\", line 184, in Cython.Compiler.Visitor.TreeVisitor._visit\n",
            "  File \"Cython/Compiler/Visitor.py\", line 182, in Cython.Compiler.Visitor.TreeVisitor._visit\n",
            "  File \"Cython/Compiler/Visitor.py\", line 322, in Cython.Compiler.Visitor.CythonTransform.visit_Node\n",
            "  File \"Cython/Compiler/Visitor.py\", line 260, in Cython.Compiler.Visitor.VisitorTransform._process_children\n",
            "  File \"Cython/Compiler/Visitor.py\", line 227, in Cython.Compiler.Visitor.TreeVisitor._visitchildren\n",
            "  File \"Cython/Compiler/Visitor.py\", line 196, in Cython.Compiler.Visitor.TreeVisitor._visitchild\n",
            "  File \"Cython/Compiler/Visitor.py\", line 190, in Cython.Compiler.Visitor.TreeVisitor._visit\n",
            "  File \"Cython/Compiler/Visitor.py\", line 148, in Cython.Compiler.Visitor.TreeVisitor._raise_compiler_error\n",
            "Cython.Compiler.Errors.CompilerCrash: \n",
            "Error compiling Cython file:\n",
            "------------------------------------------------------------\n",
            "...\n",
            "# distutils: language = c\n",
            "^\n",
            "------------------------------------------------------------\n",
            "\n",
            "halpecocotools/_mask.pyx:1:0: Compiler crash in RemoveUnreachableCode\n",
            "\n",
            "ModuleNode.body = StatListNode(_mask.pyx:1:0)\n",
            "\n",
            "Compiler crash traceback from this point on:\n",
            "  File \"Cython/Compiler/Visitor.py\", line 182, in Cython.Compiler.Visitor.TreeVisitor._visit\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/Cython/Compiler/ParseTreeTransforms.py\", line 3113, in visit_StatListNode\n",
            "    if not self.current_directives['remove_unreachable']:\n",
            "TypeError: 'NoneType' object is not subscriptable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/AlphaPose/setup.py\", line 187, in <module>\n",
            "    setup(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/__init__.py\", line 108, in setup\n",
            "    return distutils.core.setup(**attrs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/core.py\", line 184, in setup\n",
            "    return run_commands(dist)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/core.py\", line 200, in run_commands\n",
            "    dist.run_commands()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/dist.py\", line 970, in run_commands\n",
            "    self.run_command(cmd)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/dist.py\", line 956, in run_command\n",
            "    super().run_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/dist.py\", line 989, in run_command\n",
            "    cmd_obj.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/command/develop.py\", line 36, in run\n",
            "    self.install_for_development()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/command/develop.py\", line 128, in install_for_development\n",
            "    self.process_distribution(None, self.dist, not self.no_deps)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/command/easy_install.py\", line 785, in process_distribution\n",
            "    distros = WorkingSet([]).resolve(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py\", line 893, in resolve\n",
            "    dist = self._resolve_dist(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py\", line 929, in _resolve_dist\n",
            "    dist = best[req.key] = env.best_match(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py\", line 1259, in best_match\n",
            "    return self.obtain(req, installer)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py\", line 1295, in obtain\n",
            "    return installer(requirement) if installer else None\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/command/easy_install.py\", line 709, in easy_install\n",
            "    return self.install_item(spec, dist.location, tmpdir, deps)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/command/easy_install.py\", line 734, in install_item\n",
            "    dists = self.install_eggs(spec, download, tmpdir)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/command/easy_install.py\", line 931, in install_eggs\n",
            "    return self.build_and_install(setup_script, setup_base)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/command/easy_install.py\", line 1203, in build_and_install\n",
            "    self.run_setup(setup_script, setup_base, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/command/easy_install.py\", line 1189, in run_setup\n",
            "    run_setup(setup_script, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/sandbox.py\", line 251, in run_setup\n",
            "    with setup_context(setup_dir):\n",
            "  File \"/usr/lib/python3.10/contextlib.py\", line 153, in __exit__\n",
            "    self.gen.throw(typ, value, traceback)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/sandbox.py\", line 192, in setup_context\n",
            "    with save_modules():\n",
            "  File \"/usr/lib/python3.10/contextlib.py\", line 153, in __exit__\n",
            "    self.gen.throw(typ, value, traceback)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/sandbox.py\", line 171, in save_modules\n",
            "    saved_exc.resume()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/sandbox.py\", line 145, in resume\n",
            "    raise exc.with_traceback(self._tb)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/sandbox.py\", line 158, in save_modules\n",
            "    yield saved\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/sandbox.py\", line 200, in setup_context\n",
            "    yield\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/sandbox.py\", line 261, in run_setup\n",
            "    _execfile(setup_script, ns)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/sandbox.py\", line 48, in _execfile\n",
            "    exec(code, globals, locals)\n",
            "  File \"/tmp/easy_install-qjsd9eei/halpecocotools-0.0.0/setup.py\", line 16, in <module>\n",
            "    \n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/__init__.py\", line 108, in setup\n",
            "    return distutils.core.setup(**attrs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/core.py\", line 184, in setup\n",
            "    return run_commands(dist)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/core.py\", line 200, in run_commands\n",
            "    dist.run_commands()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/dist.py\", line 970, in run_commands\n",
            "    self.run_command(cmd)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/dist.py\", line 956, in run_command\n",
            "    super().run_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/dist.py\", line 989, in run_command\n",
            "    cmd_obj.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/command/bdist_egg.py\", line 167, in run\n",
            "    cmd = self.call_command('install_lib', warn_dir=False)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/command/bdist_egg.py\", line 153, in call_command\n",
            "    self.run_command(cmdname)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py\", line 316, in run_command\n",
            "    self.distribution.run_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/dist.py\", line 956, in run_command\n",
            "    super().run_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/dist.py\", line 989, in run_command\n",
            "    cmd_obj.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/command/install_lib.py\", line 12, in run\n",
            "    self.build()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/command/install_lib.py\", line 110, in build\n",
            "    self.run_command('build_ext')\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py\", line 316, in run_command\n",
            "    self.distribution.run_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/dist.py\", line 956, in run_command\n",
            "    super().run_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/dist.py\", line 989, in run_command\n",
            "    cmd_obj.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/command/build_ext.py\", line 93, in run\n",
            "    _build_ext.run(self)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/command/build_ext.py\", line 359, in run\n",
            "    self.build_extensions()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/command/build_ext.py\", line 479, in build_extensions\n",
            "    self._build_extensions_serial()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/command/build_ext.py\", line 505, in _build_extensions_serial\n",
            "    self.build_extension(ext)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/command/build_ext.py\", line 254, in build_extension\n",
            "    _build_ext.build_extension(self, ext)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/Cython/Distutils/build_ext.py\", line 130, in build_extension\n",
            "    new_ext = cythonize(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/Cython/Build/Dependencies.py\", line 1154, in cythonize\n",
            "    cythonize_one(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/Cython/Build/Dependencies.py\", line 1300, in cythonize_one\n",
            "    result = compile_single(pyx_file, options, full_module_name=full_module_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/Cython/Compiler/Main.py\", line 615, in compile_single\n",
            "    return run_pipeline(source, options, full_module_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/Cython/Compiler/Main.py\", line 539, in run_pipeline\n",
            "    err, enddata = Pipeline.run_pipeline(pipeline, source)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/Cython/Compiler/Pipeline.py\", line 398, in run_pipeline\n",
            "    data = run(phase, data)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/Cython/Compiler/Pipeline.py\", line 375, in run\n",
            "    return phase(data)\n",
            "  File \"Cython/Compiler/Visitor.py\", line 312, in Cython.Compiler.Visitor.CythonTransform.__call__\n",
            "  File \"Cython/Compiler/Visitor.py\", line 294, in Cython.Compiler.Visitor.VisitorTransform.__call__\n",
            "  File \"Cython/Compiler/Visitor.py\", line 184, in Cython.Compiler.Visitor.TreeVisitor._visit\n",
            "  File \"Cython/Compiler/Visitor.py\", line 182, in Cython.Compiler.Visitor.TreeVisitor._visit\n",
            "  File \"Cython/Compiler/Visitor.py\", line 322, in Cython.Compiler.Visitor.CythonTransform.visit_Node\n",
            "  File \"Cython/Compiler/Visitor.py\", line 260, in Cython.Compiler.Visitor.VisitorTransform._process_children\n",
            "  File \"Cython/Compiler/Visitor.py\", line 227, in Cython.Compiler.Visitor.TreeVisitor._visitchildren\n",
            "  File \"Cython/Compiler/Visitor.py\", line 196, in Cython.Compiler.Visitor.TreeVisitor._visitchild\n",
            "  File \"Cython/Compiler/Visitor.py\", line 190, in Cython.Compiler.Visitor.TreeVisitor._visit\n",
            "  File \"Cython/Compiler/Visitor.py\", line 148, in Cython.Compiler.Visitor.TreeVisitor._raise_compiler_error\n",
            "setuptools.sandbox.UnpickleableException: CompilerCrash((<FileSourceDescriptor:/tmp/easy_install-qjsd9eei/halpecocotools-0.0.0/halpecocotools/_mask.pyx>, 1, 0), 'RemoveUnreachableCode', 'Compiler crash in RemoveUnreachableCode\\n\\nModuleNode.body = StatListNode(_mask.pyx:1:0)\\n\\nCompiler crash traceback from this point on:\\n  File \"Cython/Compiler/Visitor.py\", line 182, in Cython.Compiler.Visitor.TreeVisitor._visit\\n  File \"/usr/local/lib/python3.10/dist-packages/Cython/Compiler/ParseTreeTransforms.py\", line 3113, in visit_StatListNode\\n    if not self.current_directives[\\'remove_unreachable\\']:\\nTypeError: \\'NoneType\\' object is not subscriptable', TypeError(\"'NoneType' object is not subscriptable\"), <traceback object at 0x7a9d6cd9dec0>)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "os.chdir('/content/AlphaPose')\n",
        "print(os.getcwd())\n",
        "! python setup.py build develop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9tg0Z2RznSv",
        "outputId": "2f6b7dd9-3ea1-4df1-84c0-b7b8de4416d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:pydrive is deprecated and no longer maintained. We recommend that you migrate your projects to pydrive2, the maintained fork of pydrive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-09-26 07:19:10--  https://github.com/Megvii-BaseDetection/YOLOX/releases/download/0.1.0/yolox_x.pth\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/386811486/24b68daf-00bc-41f7-8d5d-92d673d84a63?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20240926%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240926T071911Z&X-Amz-Expires=300&X-Amz-Signature=b5d824addf16d1aaac752c0aa9e67028e4cb1c415716830ee17c5032148a15b8&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dyolox_x.pth&response-content-type=application%2Foctet-stream [following]\n",
            "--2024-09-26 07:19:11--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/386811486/24b68daf-00bc-41f7-8d5d-92d673d84a63?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20240926%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240926T071911Z&X-Amz-Expires=300&X-Amz-Signature=b5d824addf16d1aaac752c0aa9e67028e4cb1c415716830ee17c5032148a15b8&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dyolox_x.pth&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 793388371 (757M) [application/octet-stream]\n",
            "Saving to: ‘./detector/yolox/data/yolox_x.pth’\n",
            "\n",
            "yolox_x.pth         100%[===================>] 756.63M  37.1MB/s    in 32s     \n",
            "\n",
            "2024-09-26 07:19:43 (23.7 MB/s) - ‘./detector/yolox/data/yolox_x.pth’ saved [793388371/793388371]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install -U -q PyDrive\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "! mkdir /content/AlphaPose/detector/yolo/data\n",
        "file_id = '1D47msNOOiJKvPOXlnpyzdKA3k6E97NTC'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('/content/AlphaPose/detector/yolo/data/yolov3-spp.weights')\n",
        "\n",
        "! mkdir /content/AlphaPose/detector/tracker/data\n",
        "file_id = '1nlnuYfGNuHWZztQHXwVZSL_FvfE551pA'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('/content/AlphaPose/detector/tracker/data/JDE-1088x608-uncertainty')\n",
        "\n",
        "file_id = '1kQhnMRURFiy7NsdS8EFL-8vtqEXOgECn'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('/content/AlphaPose/pretrained_models/fast_res50_256x192.pth')\n",
        "\n",
        "!wget -P ./detector/yolox/data/ https://github.com/Megvii-BaseDetection/YOLOX/releases/download/0.1.0/yolox_x.pth\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iiJCOUjj-g-M",
        "outputId": "f4551d52-461c-4b81-eed3-9449ed1d866e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "alphapose\t    configs   examples\t   pretrained_models  setup.cfg  yolov8n.pt\n",
            "alphapose.egg-info  detector  LICENSE\t   README.md\t      setup.py\n",
            "build\t\t    docs      model_files  scripts\t      trackers\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/AlphaPose/scripts/demo_inference.py\", line 14, in <module>\n",
            "    from trackers.tracker_api import Tracker\n",
            "  File \"/content/AlphaPose/trackers/tracker_api.py\", line 24, in <module>\n",
            "    from tracking.matching import *\n",
            "  File \"/content/AlphaPose/trackers/tracking/matching.py\", line 7, in <module>\n",
            "    from cython_bbox import bbox_overlaps as bbox_ious\n",
            "ModuleNotFoundError: No module named 'cython_bbox'\n",
            "ls: cannot access 'examples/res/': No such file or directory\n",
            "ls: cannot access 'examples/res/vis': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.chdir('/content/AlphaPose')\n",
        "! ls\n",
        "! python3 scripts/demo_inference.py --cfg configs/coco/resnet/256x192_res50_lr1e-3_1x.yaml --checkpoint pretrained_models/fast_res50_256x192.pth --indir examples/demo/ --save_img\n",
        "# result json and rendered images are saved here:\n",
        "! ls examples/res/\n",
        "! ls examples/res/vis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C17tdN7MrmyL",
        "outputId": "04b845b7-954a-4eb7-b43d-83eb5b1f16f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.10/dist-packages (8.2.101)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (10.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.19.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.1.4)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.1)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.8)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "전처리"
      ],
      "metadata": {
        "id": "7LArN5SKLQJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xml.etree.ElementTree as ET\n",
        "from ultralytics import YOLO\n",
        "from alphapose.utils.config import update_config\n",
        "from alphapose.models import builder\n",
        "from alphapose.utils.transforms import get_func_heatmap_to_coord\n",
        "from PIL import Image, ImageDraw"
      ],
      "metadata": {
        "id": "awWt1c4ROM57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7RryumnqpJD"
      },
      "outputs": [],
      "source": [
        " # PIL 라이브러리에서 Image, ImageDraw 가져오기\n",
        "\n",
        "class Opt:\n",
        "    def __init__(self):\n",
        "        self.dataset = 'coco'\n",
        "        self.sp = False\n",
        "        self.save_img = False\n",
        "        self.outputpath = './results/'\n",
        "        self.pose_flow = False\n",
        "        self.vis = True\n",
        "\n",
        "class GetKeypoint:\n",
        "    NOSE = 0\n",
        "    LEFT_EYE = 1\n",
        "    RIGHT_EYE = 2\n",
        "    LEFT_EAR = 3\n",
        "    RIGHT_EAR = 4\n",
        "    LEFT_SHOULDER = 5\n",
        "    RIGHT_SHOULDER = 6\n",
        "    LEFT_ELBOW = 7\n",
        "    RIGHT_ELBOW = 8\n",
        "    LEFT_WRIST = 9\n",
        "    RIGHT_WRIST = 10\n",
        "    LEFT_HIP = 11\n",
        "    RIGHT_HIP = 12\n",
        "    LEFT_KNEE = 13\n",
        "    RIGHT_KNEE = 14\n",
        "    LEFT_ANKLE = 15\n",
        "    RIGHT_ANKLE = 16\n",
        "\n",
        "get_keypoint = GetKeypoint()\n",
        "\n",
        "\n",
        "\n",
        "# XML 파일에서 폭행이 일어나는 시간과 행동 정보 파싱\n",
        "def parse_assault_frames(xml_file):\n",
        "    tree = ET.parse(xml_file)\n",
        "    root = tree.getroot()\n",
        "    event = root.find('event')\n",
        "\n",
        "    start_time = event.find('starttime').text\n",
        "    duration = event.find('duration').text\n",
        "\n",
        "    start_frame = time_to_frames(start_time)  # starttime을 프레임으로 변환\n",
        "    end_frame = start_frame + time_to_frames(duration)  # duration을 프레임으로 변환 후 더함\n",
        "\n",
        "    return start_frame, end_frame\n",
        "\n",
        "def time_to_frames(time_str, fps=30):\n",
        "    \"\"\"시간 문자열을 프레임 수로 변환하는 함수 (fps는 30으로 가정).\"\"\"\n",
        "    time_parts = time_str.split(':')\n",
        "\n",
        "    if len(time_parts) == 3:\n",
        "        hours, minutes, seconds = map(float, time_parts)\n",
        "    elif len(time_parts) == 2:\n",
        "        hours = 0\n",
        "        minutes, seconds = map(float, time_parts)\n",
        "    else:\n",
        "        raise ValueError(f\"Unexpected time format: {time_str}\")\n",
        "\n",
        "    total_seconds = hours * 3600 + minutes * 60 + seconds\n",
        "    return int(total_seconds * fps)\n",
        "\n",
        "# XML에서 객체의 행동을 프레임 범위와 함께 파싱\n",
        "def parse_xml(xml_file):\n",
        "    tree = ET.parse(xml_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    actions = {}\n",
        "    for obj in root.findall('object'):\n",
        "        obj_name = obj.find('objectname').text\n",
        "        actions[obj_name] = {}\n",
        "\n",
        "        for action in obj.findall('action'):\n",
        "            action_name = action.find('actionname').text\n",
        "            frames = []\n",
        "            for frame in action.findall('frame'):\n",
        "                start_frame = int(frame.find('start').text)\n",
        "                end_frame = int(frame.find('end').text)\n",
        "                frames.append((start_frame, end_frame))\n",
        "            actions[obj_name][action_name] = frames\n",
        "\n",
        "    print(f\"Parsed Actions: {actions}\")  # Debug: 확인용 출력\n",
        "    return actions\n",
        "\n",
        "\n",
        "def get_label_from_frame(actions, frame_num, person_name, assault_frames, keypoints=None):\n",
        "    # 폭력 상황 프레임 확인\n",
        "    if not assault_frames or not (assault_frames[0] <= frame_num <= assault_frames[1]):\n",
        "        return 'normal'  # 폭력 상황이 아닐 경우 'normal'로 리턴\n",
        "\n",
        "    # XML에서 현재 사람에 대한 행동 정보가 있는지 확인\n",
        "    if person_name in actions:\n",
        "        for action, frames in actions[person_name].items():\n",
        "            for (start_frame, end_frame) in frames:\n",
        "                if start_frame <= frame_num <= end_frame:\n",
        "                    print(f\"Action detected for {person_name}: {action} at frame {frame_num}\")  # Debug: 액션 출력\n",
        "                    return action  # 액션 이름을 라벨로 반환\n",
        "\n",
        "    # 폭력 상황인데 라벨이 없는 경우 classify_pose로 라벨을 추정\n",
        "    if keypoints is not None:\n",
        "        return classify_pose(keypoints)  # 포즈 분석을 통해 라벨 추정\n",
        "\n",
        "    # 기본적으로 폭력 상황이면 'assault'로 리턴\n",
        "    return 'assault'\n",
        "\n",
        "\n",
        "def classify_pose(keypoints):\n",
        "    nose_y = keypoints[GetKeypoint.NOSE * 2 + 1]\n",
        "    left_knee_y = keypoints[GetKeypoint.LEFT_KNEE * 2 + 1]\n",
        "    right_knee_y = keypoints[GetKeypoint.RIGHT_KNEE * 2 + 1]\n",
        "    left_ankle_y = keypoints[GetKeypoint.LEFT_ANKLE * 2 + 1]\n",
        "    right_ankle_y = keypoints[GetKeypoint.RIGHT_ANKLE * 2 + 1]\n",
        "\n",
        "    fall_threshold = 50\n",
        "    height_threshold = 100\n",
        "\n",
        "    if max(left_knee_y, right_knee_y) < left_ankle_y + fall_threshold and max(left_knee_y, right_knee_y) < nose_y + height_threshold:\n",
        "          return 'falling'\n",
        "    elif (abs(left_knee_y - right_knee_y) > 20) and (nose_y < min(left_knee_y, right_knee_y)):\n",
        "          return 'fighting'\n",
        "    else:\n",
        "          return 'assault'\n",
        "\n",
        "\n",
        "def calculate_euclidean_distance(point1, point2):\n",
        "    \"\"\"\n",
        "    두 점 간의 유클리드 거리를 계산하는 함수\n",
        "    :param point1: 첫 번째 점 (x, y)\n",
        "    :param point2: 두 번째 점 (x, y)\n",
        "    :return: 유클리드 거리\n",
        "    \"\"\"\n",
        "    return np.linalg.norm(np.array(point1) - np.array(point2))\n",
        "\n",
        "def calculate_angle(point1, point2, point3):\n",
        "    \"\"\"\n",
        "    세 점을 기준으로 각도를 계산하는 함수\n",
        "    :param point1: 첫 번째 점 (x, y)\n",
        "    :param point2: 기준점 (x, y)\n",
        "    :param point3: 세 번째 점 (x, y)\n",
        "    :return: 각도 (degree)\n",
        "    \"\"\"\n",
        "    vector1 = np.array([point1[0] - point2[0], point1[1] - point2[1]])\n",
        "    vector2 = np.array([point3[0] - point2[0], point3[1] - point2[1]])\n",
        "    unit_vector1 = vector1 / np.linalg.norm(vector1)\n",
        "    unit_vector2 = vector2 / np.linalg.norm(vector2)\n",
        "    dot_product = np.dot(unit_vector1, unit_vector2)\n",
        "    angle = np.arccos(dot_product)  # 라디안 값으로 각도 계산\n",
        "    return np.degrees(angle)  # 각도를 도(degree)로 변환\n",
        "\n",
        "def calculate_joint_angles(keypoints):\n",
        "    \"\"\"\n",
        "    각 관절에 대한 각도를 계산하는 함수\n",
        "    :param keypoints: 신체 키포인트 리스트\n",
        "    :return: 각 관절의 각도 (왼쪽 팔꿈치, 오른쪽 팔꿈치, 왼쪽 다리, 오른쪽 다리)\n",
        "    \"\"\"\n",
        "    left_elbow_angle = calculate_angle(\n",
        "        [keypoints[GetKeypoint.LEFT_WRIST * 2], keypoints[GetKeypoint.LEFT_WRIST * 2 + 1]],\n",
        "        [keypoints[GetKeypoint.LEFT_ELBOW * 2], keypoints[GetKeypoint.LEFT_ELBOW * 2 + 1]],\n",
        "        [keypoints[GetKeypoint.LEFT_SHOULDER * 2], keypoints[GetKeypoint.LEFT_SHOULDER * 2 + 1]]\n",
        "    )\n",
        "\n",
        "    right_elbow_angle = calculate_angle(\n",
        "        [keypoints[GetKeypoint.RIGHT_WRIST * 2], keypoints[GetKeypoint.RIGHT_WRIST * 2 + 1]],\n",
        "        [keypoints[GetKeypoint.RIGHT_ELBOW * 2], keypoints[GetKeypoint.RIGHT_ELBOW * 2 + 1]],\n",
        "        [keypoints[GetKeypoint.RIGHT_SHOULDER * 2], keypoints[GetKeypoint.RIGHT_SHOULDER * 2 + 1]]\n",
        "    )\n",
        "\n",
        "    left_knee_angle = calculate_angle(\n",
        "        [keypoints[GetKeypoint.LEFT_ANKLE * 2], keypoints[GetKeypoint.LEFT_ANKLE * 2 + 1]],\n",
        "        [keypoints[GetKeypoint.LEFT_KNEE * 2], keypoints[GetKeypoint.LEFT_KNEE * 2 + 1]],\n",
        "        [keypoints[GetKeypoint.LEFT_HIP * 2], keypoints[GetKeypoint.LEFT_HIP * 2 + 1]]\n",
        "    )\n",
        "\n",
        "    right_knee_angle = calculate_angle(\n",
        "        [keypoints[GetKeypoint.RIGHT_ANKLE * 2], keypoints[GetKeypoint.RIGHT_ANKLE * 2 + 1]],\n",
        "        [keypoints[GetKeypoint.RIGHT_KNEE * 2], keypoints[GetKeypoint.RIGHT_KNEE * 2 + 1]],\n",
        "        [keypoints[GetKeypoint.RIGHT_HIP * 2], keypoints[GetKeypoint.RIGHT_HIP * 2 + 1]]\n",
        "    )\n",
        "\n",
        "    return left_elbow_angle, right_elbow_angle, left_knee_angle, right_knee_angle\n",
        "\n",
        "\n",
        "def classify_and_store_keypoints(yolo_model, pose_model, input_image, frame_num, actions, csv_file_path, keypoint_header, assault_frames, cfg, device, prev_keypoints=None):\n",
        "    \"\"\"\n",
        "    YOLO 및 AlphaPose를 사용하여 객체 감지 및 포즈 추출, 키포인트 및 라벨 저장.\n",
        "    사람이 나타나면 사람 1로 지정하고, 폭력 상황에서는 사람 1과 사람 2의 위치를 추적.\n",
        "    \"\"\"\n",
        "    results = yolo_model.predict(input_image, save=False, classes=[0])\n",
        "    human_detections = [d for d in results[0].boxes.data.cpu().numpy() if int(d[-1]) == 0]\n",
        "\n",
        "    # 사람이 감지되지 않으면 0으로 채워진 값 반환\n",
        "    if not human_detections:\n",
        "        print(f'No human detections in frame {frame_num}. Skipping this frame...')\n",
        "        return [0] * 34, [0] * 4, [0] * 4, 'normal', [0] * 34, [0] * 4, [0] * 4, 'normal'\n",
        "\n",
        "    inps = []\n",
        "    boxes = []\n",
        "    for detection in human_detections:\n",
        "        x1, y1, x2, y2 = map(int, detection[:4])\n",
        "        boxes.append([x1, y1, x2, y2])\n",
        "        inp = cv2.resize(input_image[y1:y2, x1:x2], (cfg.DATA_PRESET.IMAGE_SIZE[0], cfg.DATA_PRESET.IMAGE_SIZE[1]))\n",
        "        inps.append(inp)\n",
        "\n",
        "    inps = torch.stack([torch.from_numpy(np.array(inp)).permute(2, 0, 1).float() for inp in inps]).to(device)\n",
        "\n",
        "    # AlphaPose 모델로 스켈레톤 추출\n",
        "    with torch.no_grad():\n",
        "        hm = pose_model(inps)\n",
        "\n",
        "    keypoints = []\n",
        "    angles = []\n",
        "    distances = []  # 유클리드 거리 추가\n",
        "    labels = []\n",
        "\n",
        "    # 키포인트 및 라벨 추출\n",
        "    for i, box in enumerate(boxes):\n",
        "        preds, maxvals = get_func_heatmap_to_coord(cfg)(hm[i], box)\n",
        "        keypoints_flatten = preds.flatten().tolist()  # (17개 관절 x 2 좌표) => 34개의 값\n",
        "\n",
        "        # 관절 각도 계산\n",
        "        left_elbow_angle, right_elbow_angle, left_knee_angle, right_knee_angle = calculate_joint_angles(keypoints_flatten)\n",
        "\n",
        "        # 유클리드 거리 계산 (예: 왼쪽 어깨-왼쪽 팔꿈치)\n",
        "        left_shoulder_left_elbow_dist = calculate_euclidean_distance(\n",
        "            (keypoints_flatten[GetKeypoint.LEFT_SHOULDER * 2], keypoints_flatten[GetKeypoint.LEFT_SHOULDER * 2 + 1]),\n",
        "            (keypoints_flatten[GetKeypoint.LEFT_ELBOW * 2], keypoints_flatten[GetKeypoint.LEFT_ELBOW * 2 + 1])\n",
        "        )\n",
        "\n",
        "        right_shoulder_right_elbow_dist = calculate_euclidean_distance(\n",
        "            (keypoints_flatten[GetKeypoint.RIGHT_SHOULDER * 2], keypoints_flatten[GetKeypoint.RIGHT_SHOULDER * 2 + 1]),\n",
        "            (keypoints_flatten[GetKeypoint.RIGHT_ELBOW * 2], keypoints_flatten[GetKeypoint.RIGHT_ELBOW * 2 + 1])\n",
        "        )\n",
        "\n",
        "        # 추가된 거리 정보: 힙-무릎 거리도 추가로 계산 (총 4개의 거리)\n",
        "        left_hip_left_knee_dist = calculate_euclidean_distance(\n",
        "            (keypoints_flatten[GetKeypoint.LEFT_HIP * 2], keypoints_flatten[GetKeypoint.LEFT_HIP * 2 + 1]),\n",
        "            (keypoints_flatten[GetKeypoint.LEFT_KNEE * 2], keypoints_flatten[GetKeypoint.LEFT_KNEE * 2 + 1])\n",
        "        )\n",
        "\n",
        "        right_hip_right_knee_dist = calculate_euclidean_distance(\n",
        "            (keypoints_flatten[GetKeypoint.RIGHT_HIP * 2], keypoints_flatten[GetKeypoint.RIGHT_HIP * 2 + 1]),\n",
        "            (keypoints_flatten[GetKeypoint.RIGHT_KNEE * 2], keypoints_flatten[GetKeypoint.RIGHT_KNEE * 2 + 1])\n",
        "        )\n",
        "\n",
        "        distances.append([left_shoulder_left_elbow_dist, right_shoulder_right_elbow_dist, left_hip_left_knee_dist, right_hip_right_knee_dist])\n",
        "\n",
        "        # 키포인트와 각도를 저장\n",
        "        keypoints.append(keypoints_flatten)\n",
        "        angles.append([left_elbow_angle, right_elbow_angle, left_knee_angle, right_knee_angle])\n",
        "\n",
        "        # XML에서 라벨 가져오기\n",
        "        label = get_label_from_frame(actions, frame_num, f\"person_{i+1}\", assault_frames, keypoints_flatten)\n",
        "        labels.append(label)\n",
        "\n",
        "    # 사람 1명일 경우, 두 번째 사람의 데이터를 모두 0으로 처리\n",
        "    if len(boxes) == 1:\n",
        "        second_person_keypoints = [0] * 34\n",
        "        second_person_angles = [0] * 4\n",
        "        second_person_distances = [0] * 4\n",
        "        second_person_label = 'normal'\n",
        "    else:\n",
        "        second_person_keypoints = keypoints[1]\n",
        "        second_person_angles = angles[1]\n",
        "        second_person_distances = distances[1]\n",
        "        second_person_label = labels[1]\n",
        "\n",
        "    # 기존 리턴값 구조 유지\n",
        "    return keypoints[0], angles[0], distances[0], labels[0], second_person_keypoints, second_person_angles, second_person_distances, second_person_label\n",
        "\n",
        "\n",
        "limb_pairs = [\n",
        "    (GetKeypoint.NOSE, GetKeypoint.LEFT_EYE), (GetKeypoint.NOSE, GetKeypoint.RIGHT_EYE),\n",
        "    (GetKeypoint.LEFT_EYE, GetKeypoint.LEFT_EAR), (GetKeypoint.RIGHT_EYE, GetKeypoint.RIGHT_EAR),\n",
        "    (GetKeypoint.NOSE, GetKeypoint.LEFT_SHOULDER), (GetKeypoint.NOSE, GetKeypoint.RIGHT_SHOULDER),\n",
        "    (GetKeypoint.LEFT_SHOULDER, GetKeypoint.LEFT_ELBOW), (GetKeypoint.RIGHT_SHOULDER, GetKeypoint.RIGHT_ELBOW),\n",
        "    (GetKeypoint.LEFT_ELBOW, GetKeypoint.LEFT_WRIST), (GetKeypoint.RIGHT_ELBOW, GetKeypoint.RIGHT_WRIST),\n",
        "    (GetKeypoint.LEFT_SHOULDER, GetKeypoint.LEFT_HIP), (GetKeypoint.RIGHT_SHOULDER, GetKeypoint.RIGHT_HIP),\n",
        "    (GetKeypoint.LEFT_HIP, GetKeypoint.LEFT_KNEE), (GetKeypoint.RIGHT_HIP, GetKeypoint.RIGHT_KNEE),\n",
        "    (GetKeypoint.LEFT_KNEE, GetKeypoint.LEFT_ANKLE), (GetKeypoint.RIGHT_KNEE, GetKeypoint.RIGHT_ANKLE)\n",
        "]\n",
        "\n",
        "\n",
        "def vis_frame(orig_img, boxes, keypoints, labels):\n",
        "    vis_img = Image.fromarray(cv2.cvtColor(orig_img, cv2.COLOR_BGR2RGB))\n",
        "    draw = ImageDraw.Draw(vis_img)\n",
        "    joint_color = (255, 0, 0)\n",
        "    limb_color = (255, 0, 0)\n",
        "\n",
        "    for i, box in enumerate(boxes):\n",
        "        keypoint_coords = []\n",
        "        for j in range(0, len(keypoints[i]), 2):\n",
        "            keypoint_x, keypoint_y = int(keypoints[i][j]), int(keypoints[i][j + 1])\n",
        "            keypoint_coords.append((keypoint_x, keypoint_y))\n",
        "            if keypoint_x > 0 and keypoint_y > 0:\n",
        "                draw.ellipse([(keypoint_x - 4, keypoint_y - 4), (keypoint_x + 4, keypoint_y + 4)], fill=joint_color)\n",
        "\n",
        "        for (start, end) in limb_pairs:\n",
        "            if keypoint_coords[start][0] > 0 and keypoint_coords[start][1] > 0 and keypoint_coords[end][0] > 0 and keypoint_coords[end][1] > 0:\n",
        "                draw.line([keypoint_coords[start], keypoint_coords[end]], fill=limb_color, width=4)\n",
        "\n",
        "    return cv2.cvtColor(np.array(vis_img), cv2.COLOR_RGB2BGR)\n",
        "\n",
        "def load_existing_csv(csv_file_path):\n",
        "    \"\"\"이미 존재하는 CSV 파일이 있으면 읽어옵니다.\"\"\"\n",
        "    if os.path.exists(csv_file_path):\n",
        "        df_existing = pd.read_csv(csv_file_path)\n",
        "        processed_videos = set(df_existing['video_name'].unique())  # 이미 처리된 비디오 목록\n",
        "        return df_existing, processed_videos\n",
        "    else:\n",
        "        df_existing = pd.DataFrame(columns=['video_name', 'frame_num'])  # 비어 있는 데이터프레임 (필요한 컬럼 지정)\n",
        "        processed_videos = set()  # 빈 비디오 목록\n",
        "        return df_existing, processed_videos\n",
        "\n",
        "# 비디오 처리 후 CSV 저장하는 부분 수정:\n",
        "def save_to_csv(df_existing, all_keypoints, header, csv_file_path):\n",
        "    \"\"\"키포인트 데이터를 CSV 파일에 저장합니다.\"\"\"\n",
        "    if all_keypoints:\n",
        "        df_temp = pd.DataFrame(all_keypoints, columns=header)\n",
        "\n",
        "\n",
        "        # 기존 CSV와 합치기 전에 중복 확인 (video_name과 frame_num 기준으로 중복 제거)\n",
        "        df_combined = pd.concat([df_existing, df_temp], ignore_index=True)\n",
        "        df_combined.drop_duplicates(subset=['video_name', 'frame_num'], keep='last', inplace=True)\n",
        "\n",
        "        # 덮어쓰기 방지하며 저장\n",
        "        df_combined.to_csv(csv_file_path, index=False)\n",
        "        print(f'CSV 파일 {csv_file_path}에 저장되었습니다.')\n",
        "\n",
        "\n",
        "def check_row_data_lengths(video_filename, frame_num, keypoint_data_1, angle_data_1, distance_data_1, label_data_1, keypoint_data_2, angle_data_2, distance_data_2, label_data_2):\n",
        "    print(f\"Video filename: {video_filename}\")\n",
        "    print(f\"Frame number: {frame_num}\")\n",
        "    print(f\"Keypoint data 1 length: {len(keypoint_data_1)}\")  # 34개여야 함\n",
        "    print(f\"Angle data 1 length: {len(angle_data_1)}\")        # 4개여야 함\n",
        "    print(f\"Distance data 1 length: {len(distance_data_1)}\")  # 4개여야 함\n",
        "    print(f\"Label 1 length: {len(label_data_1)}\")             # 1개여야 함\n",
        "\n",
        "    print(f\"Keypoint data 2 length: {len(keypoint_data_2)}\")  # 34개여야 함\n",
        "    print(f\"Angle data 2 length: {len(angle_data_2)}\")        # 4개여야 함\n",
        "    print(f\"Distance data 2 length: {len(distance_data_2)}\")  # 4개여야 함\n",
        "    print(f\"Label 2 length: {len(label_data_2)}\")             # 1개여야 함\n",
        "\n",
        "\n",
        "import logging  # 로깅 추가\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def main(root_dir, output_dir, csv_base_path, frame_skip_interval=5):\n",
        "    try:\n",
        "        yolo_model = YOLO('yolov8n.pt')\n",
        "        image_files = []\n",
        "\n",
        "        # root_dir에서 모든 비디오 파일 탐색\n",
        "        for root, dirs, files in os.walk(root_dir):\n",
        "            for file in files:\n",
        "                if file.lower().endswith(('.mp4', '.avi')):\n",
        "                    image_files.append(os.path.join(root, file))  # 동영상 파일 경로 저장\n",
        "\n",
        "        image_files = sorted(image_files)  # 파일 정렬\n",
        "\n",
        "        # 출력 디렉토리가 없으면 생성\n",
        "        if not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "\n",
        "        # AlphaPose 모델 설정\n",
        "        pretrained_model_path = '/content/drive/MyDrive/논문주제/Final_project/pretrained_models/fast_res50_256x192.pth'\n",
        "        cfg_file = '/content/AlphaPose/configs/coco/resnet/256x192_res50_lr1e-3_1x.yaml'\n",
        "\n",
        "        # Config 파일 업데이트\n",
        "        cfg = update_config(cfg_file)\n",
        "\n",
        "        # checkpoint 키 추가\n",
        "        cfg['checkpoint'] = pretrained_model_path  # 체크포인트 경로 설정\n",
        "\n",
        "        # AlphaPose 모델 빌드\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        pose_model = builder.build_sppe(cfg.MODEL, preset_cfg=cfg.DATA_PRESET)\n",
        "        pose_model.load_state_dict(torch.load(cfg['checkpoint'], map_location=device))\n",
        "        pose_model = torch.nn.DataParallel(pose_model).to(device)\n",
        "        pose_model.eval()  # AlphaPose 모델을 평가 모드로 전환\n",
        "\n",
        "        # CSV 파일의 헤더 정의\n",
        "        header = [\n",
        "        'video_filename', 'frame_num',\n",
        "        'target_person_nose_x', 'target_person_nose_y',\n",
        "        'target_person_left_eye_x', 'target_person_left_eye_y',\n",
        "        'target_person_right_eye_x', 'target_person_right_eye_y',\n",
        "        'target_person_left_ear_x', 'target_person_left_ear_y',\n",
        "        'target_person_right_ear_x', 'target_person_right_ear_y',\n",
        "        'target_person_left_shoulder_x', 'target_person_left_shoulder_y',\n",
        "        'target_person_right_shoulder_x', 'target_person_right_shoulder_y',\n",
        "        'target_person_left_elbow_x', 'target_person_left_elbow_y',\n",
        "        'target_person_right_elbow_x', 'target_person_right_elbow_y',\n",
        "        'target_person_left_wrist_x', 'target_person_left_wrist_y',\n",
        "        'target_person_right_wrist_x', 'target_person_right_wrist_y',\n",
        "        'target_person_left_hip_x', 'target_person_left_hip_y',\n",
        "        'target_person_right_hip_x', 'target_person_right_hip_y',\n",
        "        'target_person_left_knee_x', 'target_person_left_knee_y',\n",
        "        'target_person_right_knee_x', 'target_person_right_knee_y',\n",
        "        'target_person_left_ankle_x', 'target_person_left_ankle_y',\n",
        "        'target_person_right_ankle_x', 'target_person_right_ankle_y',\n",
        "        # Additional joint angles (first person)\n",
        "        'left_elbow_angle_1', 'right_elbow_angle_1',\n",
        "        'left_knee_angle_1', 'right_knee_angle_1',\n",
        "        # Additional Euclidean distances (first person)\n",
        "        'left_shoulder_left_elbow_dist_1',\n",
        "        'right_shoulder_right_elbow_dist_1',\n",
        "        'left_hip_left_knee_dist_1',\n",
        "        'right_hip_right_knee_dist_1',\n",
        "        'target_person_label',\n",
        "        # Second person\n",
        "        'closest_person_nose_x', 'closest_person_nose_y',\n",
        "        'closest_person_left_eye_x', 'closest_person_left_eye_y',\n",
        "        'closest_person_right_eye_x', 'closest_person_right_eye_y',\n",
        "        'closest_person_left_ear_x', 'closest_person_left_ear_y',\n",
        "        'closest_person_right_ear_x', 'closest_person_right_ear_y',\n",
        "        'closest_person_left_shoulder_x', 'closest_person_left_shoulder_y',\n",
        "        'closest_person_right_shoulder_x', 'closest_person_right_shoulder_y',\n",
        "        'closest_person_left_elbow_x', 'closest_person_left_elbow_y',\n",
        "        'closest_person_right_elbow_x', 'closest_person_right_elbow_y',\n",
        "        'closest_person_left_wrist_x', 'closest_person_left_wrist_y',\n",
        "        'closest_person_right_wrist_x', 'closest_person_right_wrist_y',\n",
        "        'closest_person_left_hip_x', 'closest_person_left_hip_y',\n",
        "        'closest_person_right_hip_x', 'closest_person_right_hip_y',\n",
        "        'closest_person_left_knee_x', 'closest_person_left_knee_y',\n",
        "        'closest_person_right_knee_x', 'closest_person_right_knee_y',\n",
        "        'closest_person_left_ankle_x', 'closest_person_left_ankle_y',\n",
        "        'closest_person_right_ankle_x', 'closest_person_right_ankle_y',\n",
        "        # Second person joint angles\n",
        "        'left_elbow_angle_2', 'right_elbow_angle_2',\n",
        "        'left_knee_angle_2', 'right_knee_angle_2',\n",
        "        # Second person Euclidean distances\n",
        "        'left_shoulder_left_elbow_dist_2',\n",
        "        'right_shoulder_right_elbow_dist_2',\n",
        "        'left_hip_left_knee_dist_2',\n",
        "        'right_hip_right_knee_dist_2',\n",
        "        'closest_person_label'\n",
        "    ]\n",
        "\n",
        "        all_keypoints = []\n",
        "        batch_number = 1\n",
        "        batch_size = 2  # 두 개씩 처리\n",
        "\n",
        "        # 비디오 파일을 배치 단위로 처리\n",
        "        for i in range(0, len(image_files), batch_size):\n",
        "            batch_files = image_files[i:i + batch_size]\n",
        "            csv_file_path = f\"{csv_base_path}_batch_{batch_number}.csv\"\n",
        "\n",
        "            for video_file in batch_files:\n",
        "                try:\n",
        "                    logging.info(f\"Processing video file: {video_file}\")\n",
        "\n",
        "                    # 각 비디오 파일에서 XML 파일과 라벨 가져오기\n",
        "                    xml_filename = os.path.splitext(os.path.basename(video_file))[0] + \".xml\"\n",
        "                    xml_path = os.path.join(os.path.dirname(video_file), xml_filename)\n",
        "\n",
        "                    if os.path.exists(xml_path):\n",
        "                        assault_frames = parse_assault_frames(xml_path)\n",
        "                        actions = parse_xml(xml_path)\n",
        "                    else:\n",
        "                        assault_frames = None\n",
        "                        actions = None\n",
        "\n",
        "                    keypoints = process_video(video_file, yolo_model, pose_model, cfg, device, header, frame_skip_interval, assault_frames, actions)\n",
        "\n",
        "                    if keypoints:\n",
        "                        all_keypoints.extend(keypoints)\n",
        "                        logging.info(f\"Processed {len(keypoints)} keypoints from {video_file}\")\n",
        "                    else:\n",
        "                        logging.warning(f\"No keypoints found for {video_file}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    logging.error(f\"Error processing {video_file}: {e}\")\n",
        "\n",
        "            # CSV 저장\n",
        "            if all_keypoints:\n",
        "                logging.info(f\"Saving batch {batch_number} to {csv_file_path}\")\n",
        "                df_temp = pd.DataFrame(all_keypoints, columns=header)\n",
        "                df_temp.to_csv(csv_file_path, index=False)\n",
        "                logging.info(f\"Batch {batch_number} saved to {csv_file_path}\")\n",
        "            else:\n",
        "                logging.warning(f\"No keypoints to save for batch {batch_number}\")\n",
        "\n",
        "            all_keypoints = []  # 다음 배치 처리를 위해 초기화\n",
        "            batch_number += 1\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Main function error: {e}\")\n",
        "\n",
        "def process_video(video_file, yolo_model, pose_model, cfg, device, header, frame_skip_interval, assault_frames, actions):\n",
        "    \"\"\"주어진 비디오 파일에서 키포인트를 추출하여 반환\"\"\"\n",
        "    keypoints_list = []\n",
        "    cap = cv2.VideoCapture(video_file)\n",
        "    frame_num = 0\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # XML에서 제공된 폭력 상황에서만 프레임 건너뛰지 않음\n",
        "        if assault_frames and (assault_frames[0] <= frame_num <= assault_frames[1]):\n",
        "            frame_skip_interval = 1  # 폭력 상황에서는 모든 프레임 처리\n",
        "        else:\n",
        "            if frame_num % frame_skip_interval != 0:\n",
        "                frame_num += 1\n",
        "                continue\n",
        "\n",
        "        try:\n",
        "            # classify_and_store_keypoints 함수로 키포인트 추출\n",
        "            keypoint_data_1, angles_1, distances_1, label_1, keypoint_data_2, angles_2, distances_2, label_2 = classify_and_store_keypoints(\n",
        "                yolo_model, pose_model, frame, frame_num, actions, None, header, assault_frames, cfg, device\n",
        "            )\n",
        "\n",
        "            if keypoint_data_1 is not None and label_1 is not None:\n",
        "                row = (\n",
        "                    [os.path.basename(video_file), frame_num] + keypoint_data_1 + angles_1 + distances_1 +\n",
        "                    [label_1] + keypoint_data_2 + angles_2 + distances_2 + [label_2]\n",
        "                )\n",
        "\n",
        "                if len(row) == len(header):\n",
        "                    keypoints_list.append(row)\n",
        "                else:\n",
        "                    logging.warning(f\"Row length mismatch for frame {frame_num} in {video_file}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error processing frame {frame_num} in {video_file}: {e}\")\n",
        "\n",
        "        frame_num += 1\n",
        "\n",
        "    cap.release()\n",
        "    return keypoints_list\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if __name__ == \"__main__\":\n",
        "#     root_dir = '/content/drive/MyDrive/논문주제/이상행동 CCTV 영상/01.폭행(assult)'\n",
        "#     output_dir = '/content/drive/MyDrive/논문주제/이상행동 CCTV 영상/01.폭행(assult)3'\n",
        "#     csv_base_path = '/content/drive/MyDrive/논문주제/이상행동 CCTV 영상/results_batch'\n",
        "\n",
        "#     main(root_dir, output_dir, csv_base_path, frame_skip_interval=5)"
      ],
      "metadata": {
        "id": "7CxA_suRJDnT",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvxySEI5AzC9"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NKgdmmKqQAIh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "TNtnurXG8cqE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b01b3f2f-0226-4b80-92ad-38e2adf2ed63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train, Val, Test 파일들이 각각 /content/drive/MyDrive/논문주제/train.csv, /content/drive/MyDrive/논문주제/val.csv, /content/drive/MyDrive/논문주제/test.csv로 저장되었습니다.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# 파일 경로와 이름 설정\n",
        "files = [\n",
        "    '/content/drive/MyDrive/논문주제/이상행동 CCTV 영상/results_batch_batch_1.csv',\n",
        "    '/content/drive/MyDrive/논문주제/이상행동 CCTV 영상/results_batch_batch_2.csv',\n",
        "    '/content/drive/MyDrive/논문주제/이상행동 CCTV 영상/results_batch_batch_3.csv',\n",
        "    '/content/drive/MyDrive/논문주제/이상행동 CCTV 영상/results_batch_batch_4.csv',\n",
        "    '/content/drive/MyDrive/논문주제/이상행동 CCTV 영상/results_batch_batch_5.csv',\n",
        "    '/content/drive/MyDrive/논문주제/이상행동 CCTV 영상/results_batch_batch_6.csv',\n",
        "    '/content/drive/MyDrive/논문주제/이상행동 CCTV 영상/results_batch_batch_7.csv',\n",
        "    '/content/drive/MyDrive/논문주제/이상행동 CCTV 영상/results_batch_batch_8.csv',\n",
        "    '/content/drive/MyDrive/논문주제/이상행동 CCTV 영상/results_batch_batch_9.csv',\n",
        "    '/content/drive/MyDrive/논문주제/이상행동 CCTV 영상/results_batch_batch_10.csv'\n",
        "]\n",
        "\n",
        "# 폭력 구간의 전후로 프레임을 포함하여 추출하는 함수\n",
        "def extract_violence_and_nearby_frames(df, window_before=3000, window_after=500):\n",
        "    extracted_dfs = []\n",
        "\n",
        "    # video_filename을 기준으로 그룹화하여 처리\n",
        "    for video_name, group in df.groupby('video_filename'):\n",
        "        # 폭력 구간을 나타내는 인덱스 추출\n",
        "        violence_indices = group[group['frame_num'] - group['frame_num'].shift(1) == 1].index\n",
        "        extracted_indices = []\n",
        "\n",
        "        # 시작과 끝 폭력 프레임의 인덱스 범위를 찾음\n",
        "        if len(violence_indices) > 0:\n",
        "            for idx in violence_indices:\n",
        "                start_idx = max(0, idx - window_before)  # 폭력 상황 이전 2000 프레임\n",
        "                end_idx = min(len(group), idx + window_after)  # 폭력 상황 이후 프레임\n",
        "                extracted_indices.extend(range(start_idx, end_idx))\n",
        "\n",
        "        # 추출된 인덱스를 리스트로 변환하여 중복 제거 후 정렬\n",
        "        extracted_indices = sorted(list(set(extracted_indices)))\n",
        "\n",
        "        # 추출된 인덱스를 기반으로 데이터 선택\n",
        "        selected_df = group.iloc[extracted_indices]\n",
        "        extracted_dfs.append(selected_df)\n",
        "\n",
        "    # 모든 비디오에 대해 추출된 데이터프레임을 병합\n",
        "    final_df = pd.concat(extracted_dfs).reset_index(drop=True)\n",
        "    return final_df\n",
        "\n",
        "# 파일을 train, val, test로 나누는 함수 (파일을 하나로 묶어서 저장)\n",
        "def split_files_into_single_csv(files, train_csv, val_csv, test_csv):\n",
        "    # 첫 6개 파일은 train, 그 다음 2개는 val, 마지막 2개는 test\n",
        "    train_files = files[:6]\n",
        "    val_files = files[6:8]\n",
        "    test_files = files[8:]\n",
        "\n",
        "    # 각각의 파일들을 하나의 DataFrame으로 묶음\n",
        "    train_df_list = []\n",
        "    val_df_list = []\n",
        "    test_df_list = []\n",
        "\n",
        "    for file in train_files:\n",
        "        df = pd.read_csv(file)\n",
        "        extracted_df = extract_violence_and_nearby_frames(df)\n",
        "        train_df_list.append(extracted_df)\n",
        "    train_df = pd.concat(train_df_list).reset_index(drop=True)\n",
        "    train_df.to_csv(train_csv, index=False)\n",
        "\n",
        "    for file in val_files:\n",
        "        df = pd.read_csv(file)\n",
        "        extracted_df = extract_violence_and_nearby_frames(df)\n",
        "        val_df_list.append(extracted_df)\n",
        "    val_df = pd.concat(val_df_list).reset_index(drop=True)\n",
        "    val_df.to_csv(val_csv, index=False)\n",
        "\n",
        "    for file in test_files:\n",
        "        df = pd.read_csv(file)\n",
        "        extracted_df = extract_violence_and_nearby_frames(df)\n",
        "        test_df_list.append(extracted_df)\n",
        "    test_df = pd.concat(test_df_list).reset_index(drop=True)\n",
        "    test_df.to_csv(test_csv, index=False)\n",
        "\n",
        "    print(f\"Train, Val, Test 파일들이 각각 {train_csv}, {val_csv}, {test_csv}로 저장되었습니다.\")\n",
        "\n",
        "# 함수 실행\n",
        "train_csv = '/content/drive/MyDrive/논문주제/train.csv'\n",
        "val_csv = '/content/drive/MyDrive/논문주제/val.csv'\n",
        "test_csv = '/content/drive/MyDrive/논문주제/test.csv'\n",
        "\n",
        "split_files_into_single_csv(files, train_csv, val_csv, test_csv)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ],
      "metadata": {
        "id": "McpTQRdfOg_U"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 데이터 불러오기\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/논문주제/train.csv')\n",
        "val_df = pd.read_csv('/content/drive/MyDrive/논문주제/val.csv')\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/논문주제/test.csv')"
      ],
      "metadata": {
        "id": "vPFJcOxtDPux"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # ensure X_train, X_val, X_test are still pandas DataFrames before dropping columns\n",
        "# train_df = train_df.drop(columns=[\n",
        "#     'left_elbow_angle_1', 'right_elbow_angle_1', 'left_knee_angle_1', 'right_knee_angle_1',\n",
        "#     'left_shoulder_left_elbow_dist_1', 'right_shoulder_right_elbow_dist_1',\n",
        "#     'left_hip_left_knee_dist_1', 'right_hip_right_knee_dist_1',\n",
        "#     'left_elbow_angle_2', 'right_elbow_angle_2', 'left_knee_angle_2', 'right_knee_angle_2',\n",
        "#     'left_shoulder_left_elbow_dist_2', 'right_shoulder_right_elbow_dist_2',\n",
        "#     'left_hip_left_knee_dist_2', 'right_hip_right_knee_dist_2',\n",
        "#     'closest_person_label'\n",
        "# ])\n",
        "\n",
        "# val_df = val_df.drop(columns=[\n",
        "#     'left_elbow_angle_1', 'right_elbow_angle_1', 'left_knee_angle_1', 'right_knee_angle_1',\n",
        "#     'left_shoulder_left_elbow_dist_1', 'right_shoulder_right_elbow_dist_1',\n",
        "#     'left_hip_left_knee_dist_1', 'right_hip_right_knee_dist_1',\n",
        "#     'left_elbow_angle_2', 'right_elbow_angle_2', 'left_knee_angle_2', 'right_knee_angle_2',\n",
        "#     'left_shoulder_left_elbow_dist_2', 'right_shoulder_right_elbow_dist_2',\n",
        "#     'left_hip_left_knee_dist_2', 'rght_hip_right_knee_dist_2',\n",
        "#     'closest_person_label'\n",
        "# ])\n",
        "\n",
        "# test_df = test_df.drop(columns=[\n",
        "#     'left_elbow_angle_1', 'right_elbow_angle_1', 'left_knee_angle_1', 'right_knee_angle_1',\n",
        "#     'left_shoulder_left_elbow_dist_1', 'right_shoulder_right_elbow_dist_1',\n",
        "#     'left_hip_left_knee_dist_1', 'right_hip_right_knee_dist_1',\n",
        "#     'left_elbow_angle_2', 'right_elbow_angle_2', 'left_knee_angle_2', 'right_knee_angle_2',\n",
        "#     'left_shoulder_left_elbow_dist_2', 'right_shoulder_right_elbow_dist_2',\n",
        "#     'left_hip_left_knee_dist_2', 'right_hip_right_knee_dist_2',\n",
        "#     'closest_person_label'\n",
        "# ])\n",
        "\n"
      ],
      "metadata": {
        "id": "2B8YtFeRAcw1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "QP0YNUNjmyUo"
      },
      "outputs": [],
      "source": [
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# 특징(feature)과 라벨(label) 분리\n",
        "def prepare_data(df):\n",
        "    # 프레임 번호 제거 후 모든 수치형 데이터만 선택\n",
        "    X = df.drop(columns=['frame_num']).select_dtypes(include=[np.number]).values\n",
        "\n",
        "    # 프레임 간격이 1이면 폭력 상황, 5이면 비폭력 상황으로 간주\n",
        "    frame_numbers = df['frame_num'].values\n",
        "    violence_labels = []\n",
        "    for i in range(1, len(frame_numbers)):\n",
        "        frame_gap = frame_numbers[i] - frame_numbers[i - 1]\n",
        "        if frame_gap == 1:\n",
        "            violence_labels.append(1)  # 폭력 상황\n",
        "        else:\n",
        "            violence_labels.append(0)  # 비폭력 상황\n",
        "\n",
        "    # 첫 번째 프레임은 비폭력으로 간주\n",
        "    violence_labels.insert(0, 0)\n",
        "    y = np.array(violence_labels)  # reshape(-1, 1) 제거, 정수형 라벨로 유지\n",
        "    return X, y\n",
        "\n",
        "# 결측치를 채우는 함수 정의\n",
        "def fill_missing_values(df):\n",
        "    labels = df['target_person_label'].values\n",
        "    for col in ['left_elbow_angle_1', 'right_elbow_angle_1', 'left_knee_angle_1', 'right_knee_angle_1',\n",
        "                'left_elbow_angle_2', 'right_elbow_angle_2', 'left_knee_angle_2', 'right_knee_angle_2']:\n",
        "        for i in range(1, len(df)-1):\n",
        "            if labels[i] == 'assault':\n",
        "                if pd.isna(df.loc[i, col]):\n",
        "                    prev_value = df.loc[i-1, col]\n",
        "                    next_value = df.loc[i+1, col]\n",
        "                    if pd.notna(prev_value) and pd.notna(next_value):\n",
        "                        df.loc[i, col] = (prev_value + next_value) / 2\n",
        "                    elif pd.notna(prev_value):\n",
        "                        df.loc[i, col] = prev_value\n",
        "                    elif pd.notna(next_value):\n",
        "                        df.loc[i, col] = next_value\n",
        "            elif pd.isna(df.loc[i, col]):\n",
        "                df.loc[i, col] = 0\n",
        "    return df\n",
        "\n",
        "# NaN 값을 0으로 대체하는 함수\n",
        "def replace_nan_with_zero(df):\n",
        "    return df.fillna(0)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_df), len(val_df), len(test_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQkjjtEOLxb0",
        "outputId": "00f68f7c-2dd6-4532-d48f-2d2abbbe1d2b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(56846, 20754, 21775)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터프레임에 결측치 처리 적용\n",
        "train_df = replace_nan_with_zero(train_df)\n",
        "val_df = replace_nan_with_zero(val_df)\n",
        "test_df = replace_nan_with_zero(test_df)\n",
        "\n",
        "# 데이터프레임에 결측치 확인\n",
        "print(train_df.isnull().sum())\n",
        "print(val_df.isnull().sum())\n",
        "print(test_df.isnull().sum())\n",
        "\n",
        "\n",
        "# 데이터 준비 및 LSTM 모델 학습\n",
        "X_train, y_train = prepare_data(train_df)\n",
        "X_val, y_val = prepare_data(val_df)\n",
        "X_test, y_test = prepare_data(test_df)\n",
        "\n",
        "# 데이터를 숫자형으로 변환하여 처리할 수 있게 합니다\n",
        "X_train = np.array(X_train, dtype=np.float32)\n",
        "X_val = np.array(X_val, dtype=np.float32)\n",
        "X_test = np.array(X_test, dtype=np.float32)\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)  # 학습 데이터로 스케일러 학습 후 변환\n",
        "X_val = scaler.transform(X_val)  # 동일 스케일러로 validation 데이터 변환\n",
        "X_test = scaler.transform(X_test)  # 동일 스케일러로 test 데이터 변환"
      ],
      "metadata": {
        "id": "rAeYkNJRqh49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b7d1fae-9c48-4719-c625-6f0b4797938c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "video_filename                       0\n",
            "frame_num                            0\n",
            "target_person_nose_x                 0\n",
            "target_person_nose_y                 0\n",
            "target_person_left_eye_x             0\n",
            "                                    ..\n",
            "left_shoulder_left_elbow_dist_2      0\n",
            "right_shoulder_right_elbow_dist_2    0\n",
            "left_hip_left_knee_dist_2            0\n",
            "right_hip_right_knee_dist_2          0\n",
            "closest_person_label                 0\n",
            "Length: 88, dtype: int64\n",
            "video_filename                       0\n",
            "frame_num                            0\n",
            "target_person_nose_x                 0\n",
            "target_person_nose_y                 0\n",
            "target_person_left_eye_x             0\n",
            "                                    ..\n",
            "left_shoulder_left_elbow_dist_2      0\n",
            "right_shoulder_right_elbow_dist_2    0\n",
            "left_hip_left_knee_dist_2            0\n",
            "right_hip_right_knee_dist_2          0\n",
            "closest_person_label                 0\n",
            "Length: 88, dtype: int64\n",
            "video_filename                       0\n",
            "frame_num                            0\n",
            "target_person_nose_x                 0\n",
            "target_person_nose_y                 0\n",
            "target_person_left_eye_x             0\n",
            "                                    ..\n",
            "left_shoulder_left_elbow_dist_2      0\n",
            "right_shoulder_right_elbow_dist_2    0\n",
            "left_hip_left_knee_dist_2            0\n",
            "right_hip_right_knee_dist_2          0\n",
            "closest_person_label                 0\n",
            "Length: 88, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_train), len(X_val), len(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5pxdrzzL4P1",
        "outputId": "a094da51-c2bb-4d3c-b123-ee47a7f1144c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(56846, 20754, 21775)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "k2HKqQ9IVx8Y",
        "outputId": "d1e3ffc8-abcc-46bd-e78a-2325a2b2b3ed"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                video_filename  frame_num  \\\n",
              "0  12-6_cam01_assault01_place09_day_spring.mp4          0   \n",
              "1  12-6_cam01_assault01_place09_day_spring.mp4          5   \n",
              "2  12-6_cam01_assault01_place09_day_spring.mp4         10   \n",
              "3  12-6_cam01_assault01_place09_day_spring.mp4         15   \n",
              "4  12-6_cam01_assault01_place09_day_spring.mp4         20   \n",
              "\n",
              "   target_person_nose_x  target_person_nose_y  target_person_left_eye_x  \\\n",
              "0           1478.171875            898.046875               1362.542969   \n",
              "1           1476.750000            895.750000               1359.187500   \n",
              "2           1479.234375            896.109375               1362.058594   \n",
              "3           1479.109375            896.484375               1365.027344   \n",
              "4           1359.800781            922.722656               1359.800781   \n",
              "\n",
              "   target_person_left_eye_y  target_person_right_eye_x  \\\n",
              "0                934.253906                1478.171875   \n",
              "1                932.562500                1320.000000   \n",
              "2                932.800781                1479.234375   \n",
              "3                932.207031                1479.109375   \n",
              "4                929.777344                1476.203125   \n",
              "\n",
              "   target_person_right_eye_y  target_person_left_ear_x  \\\n",
              "0                 898.046875               1472.332031   \n",
              "1                 938.500000               1470.812500   \n",
              "2                 896.109375               1473.316406   \n",
              "3                 896.484375               1473.347656   \n",
              "4                 893.328125               1357.449219   \n",
              "\n",
              "   target_person_left_ear_y  ...  closest_person_right_ankle_y  \\\n",
              "0                910.894531  ...                           0.0   \n",
              "1                911.187500  ...                           0.0   \n",
              "2                909.128906  ...                           0.0   \n",
              "3                909.160156  ...                           0.0   \n",
              "4                918.019531  ...                           0.0   \n",
              "\n",
              "   left_elbow_angle_2  right_elbow_angle_2  left_knee_angle_2  \\\n",
              "0                 0.0                  0.0                0.0   \n",
              "1                 0.0                  0.0                0.0   \n",
              "2                 0.0                  0.0                0.0   \n",
              "3                 0.0                  0.0                0.0   \n",
              "4                 0.0                  0.0                0.0   \n",
              "\n",
              "   right_knee_angle_2  left_shoulder_left_elbow_dist_2  \\\n",
              "0                 0.0                              0.0   \n",
              "1                 0.0                              0.0   \n",
              "2                 0.0                              0.0   \n",
              "3                 0.0                              0.0   \n",
              "4                 0.0                              0.0   \n",
              "\n",
              "   right_shoulder_right_elbow_dist_2  left_hip_left_knee_dist_2  \\\n",
              "0                                0.0                        0.0   \n",
              "1                                0.0                        0.0   \n",
              "2                                0.0                        0.0   \n",
              "3                                0.0                        0.0   \n",
              "4                                0.0                        0.0   \n",
              "\n",
              "   right_hip_right_knee_dist_2  closest_person_label  \n",
              "0                          0.0                normal  \n",
              "1                          0.0                normal  \n",
              "2                          0.0                normal  \n",
              "3                          0.0                normal  \n",
              "4                          0.0                normal  \n",
              "\n",
              "[5 rows x 88 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d4512c46-36ec-4526-89a1-a0a0646e7849\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>video_filename</th>\n",
              "      <th>frame_num</th>\n",
              "      <th>target_person_nose_x</th>\n",
              "      <th>target_person_nose_y</th>\n",
              "      <th>target_person_left_eye_x</th>\n",
              "      <th>target_person_left_eye_y</th>\n",
              "      <th>target_person_right_eye_x</th>\n",
              "      <th>target_person_right_eye_y</th>\n",
              "      <th>target_person_left_ear_x</th>\n",
              "      <th>target_person_left_ear_y</th>\n",
              "      <th>...</th>\n",
              "      <th>closest_person_right_ankle_y</th>\n",
              "      <th>left_elbow_angle_2</th>\n",
              "      <th>right_elbow_angle_2</th>\n",
              "      <th>left_knee_angle_2</th>\n",
              "      <th>right_knee_angle_2</th>\n",
              "      <th>left_shoulder_left_elbow_dist_2</th>\n",
              "      <th>right_shoulder_right_elbow_dist_2</th>\n",
              "      <th>left_hip_left_knee_dist_2</th>\n",
              "      <th>right_hip_right_knee_dist_2</th>\n",
              "      <th>closest_person_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12-6_cam01_assault01_place09_day_spring.mp4</td>\n",
              "      <td>0</td>\n",
              "      <td>1478.171875</td>\n",
              "      <td>898.046875</td>\n",
              "      <td>1362.542969</td>\n",
              "      <td>934.253906</td>\n",
              "      <td>1478.171875</td>\n",
              "      <td>898.046875</td>\n",
              "      <td>1472.332031</td>\n",
              "      <td>910.894531</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>12-6_cam01_assault01_place09_day_spring.mp4</td>\n",
              "      <td>5</td>\n",
              "      <td>1476.750000</td>\n",
              "      <td>895.750000</td>\n",
              "      <td>1359.187500</td>\n",
              "      <td>932.562500</td>\n",
              "      <td>1320.000000</td>\n",
              "      <td>938.500000</td>\n",
              "      <td>1470.812500</td>\n",
              "      <td>911.187500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>12-6_cam01_assault01_place09_day_spring.mp4</td>\n",
              "      <td>10</td>\n",
              "      <td>1479.234375</td>\n",
              "      <td>896.109375</td>\n",
              "      <td>1362.058594</td>\n",
              "      <td>932.800781</td>\n",
              "      <td>1479.234375</td>\n",
              "      <td>896.109375</td>\n",
              "      <td>1473.316406</td>\n",
              "      <td>909.128906</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>12-6_cam01_assault01_place09_day_spring.mp4</td>\n",
              "      <td>15</td>\n",
              "      <td>1479.109375</td>\n",
              "      <td>896.484375</td>\n",
              "      <td>1365.027344</td>\n",
              "      <td>932.207031</td>\n",
              "      <td>1479.109375</td>\n",
              "      <td>896.484375</td>\n",
              "      <td>1473.347656</td>\n",
              "      <td>909.160156</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>12-6_cam01_assault01_place09_day_spring.mp4</td>\n",
              "      <td>20</td>\n",
              "      <td>1359.800781</td>\n",
              "      <td>922.722656</td>\n",
              "      <td>1359.800781</td>\n",
              "      <td>929.777344</td>\n",
              "      <td>1476.203125</td>\n",
              "      <td>893.328125</td>\n",
              "      <td>1357.449219</td>\n",
              "      <td>918.019531</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 88 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d4512c46-36ec-4526-89a1-a0a0646e7849')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d4512c46-36ec-4526-89a1-a0a0646e7849 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d4512c46-36ec-4526-89a1-a0a0646e7849');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-dcf6ce13-a171-42d0-bd6f-b0488e99e517\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dcf6ce13-a171-42d0-bd6f-b0488e99e517')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-dcf6ce13-a171-42d0-bd6f-b0488e99e517 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_df"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# # 데이터를 LSTM에서 사용할 수 있도록 3차원으로 변환 (samples, time_steps, features)\n",
        "# X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
        "# X_val = X_val.reshape(X_val.shape[0], 1, X_val.shape[1])\n",
        "# X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
        "\n",
        "# # PyTorch Tensor로 변환\n",
        "# train_data = TensorDataset(torch.Tensor(X_train), torch.LongTensor(y_train))  # LongTensor 사용\n",
        "# val_data = TensorDataset(torch.Tensor(X_val), torch.LongTensor(y_val))\n",
        "# test_data = TensorDataset(torch.Tensor(X_test), torch.LongTensor(y_test))\n",
        "\n",
        "\n",
        "# # DataLoader 생성\n",
        "# batch_size = 32\n",
        "# train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "# val_loader = DataLoader(val_data, batch_size=batch_size)\n",
        "# test_loader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n"
      ],
      "metadata": {
        "id": "8f8FNt73n9ON"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# LSTM 모델 정의\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, dropout=0.5):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)  # 최종 예측을 위한 FC layer\n",
        "        self.batch_norm = nn.BatchNorm1d(hidden_size)  # Batch Normalization 추가\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        out = out[:, -1, :]  # 마지막 타임스텝의 출력 사용\n",
        "        out = self.batch_norm(out)  # Batch Normalization 적용\n",
        "        out = self.fc(out)  # violence 여부 예측\n",
        "        return out\n",
        "\n",
        "# 입력 데이터 크기 확인 및 모델 생성\n",
        "input_size = 84  # 키포인트 및 각도, 거리 정보 크기 (정확히 전처리된 데이터에서 확인 필요)\n",
        "hidden_size = 128  # 저장된 모델과 일치하도록 설정\n",
        "output_size = 2  # Binary classification (violence vs non-violence)\n",
        "model = LSTMModel(input_size, hidden_size, output_size)\n",
        "\n",
        "# 손실 함수 및 옵티마이저\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "\n",
        "# 스케줄러\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "# 조기 종료 클래스 정의\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=3, min_delta=0.001):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.best_loss = None\n",
        "        self.counter = 0\n",
        "\n",
        "    def step(self, val_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "        elif val_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "        return self.counter >= self.patience\n",
        "\n",
        "# 모델 학습 함수\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler=None, num_epochs=20):\n",
        "    early_stopping = EarlyStopping(patience=3, min_delta=0.001)  # 조기 종료 설정\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # 학습 모드로 설정\n",
        "        train_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()  # 옵티마이저 초기화\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()  # 역전파\n",
        "            optimizer.step()  # 가중치 업데이트\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # 스케줄러가 있으면 학습률 업데이트\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "\n",
        "        val_loss = 0.0\n",
        "        model.eval()  # 평가 모드로 설정\n",
        "        with torch.no_grad():  # 평가 시 역전파 비활성화\n",
        "            for inputs, labels in val_loader:\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        # 에포크마다 손실 출력\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss/len(train_loader):.6f}, Val Loss: {val_loss/len(val_loader):.6f}')\n",
        "\n",
        "        # 조기 종료 체크\n",
        "        if early_stopping.step(val_loss / len(val_loader)):\n",
        "            print(f\"Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "# 학습을 위한 데이터 전처리 (데이터를 3차원으로 변환 필요)\n",
        "X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])  # (배치 크기, 시퀀스 길이, 특성 수)\n",
        "X_val = X_val.reshape(X_val.shape[0], 1, X_val.shape[1])\n",
        "\n",
        "# PyTorch Tensor로 변환\n",
        "train_data = TensorDataset(torch.Tensor(X_train), torch.LongTensor(y_train))\n",
        "val_data = TensorDataset(torch.Tensor(X_val), torch.LongTensor(y_val))\n",
        "\n",
        "# DataLoader 생성\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PhzedcAhVTX",
        "outputId": "50cafd75-77af-45f2-e369-458a86905664"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습\n",
        "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler=scheduler, num_epochs=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jC37W2mvNZnO",
        "outputId": "aacc35ac-a435-480f-a0f1-bf9947415daa"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Train Loss: 0.131156, Val Loss: 0.160157\n",
            "Epoch 2/20, Train Loss: 0.074304, Val Loss: 0.162443\n",
            "Epoch 3/20, Train Loss: 0.063469, Val Loss: 0.173796\n",
            "Epoch 4/20, Train Loss: 0.058754, Val Loss: 0.169835\n",
            "Early stopping at epoch 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "\n",
        "# 데이터를 3차원 텐서로 변환 (samples, time_steps, features)\n",
        "X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
        "\n",
        "# PyTorch Tensor로 변환\n",
        "test_data = TensorDataset(torch.Tensor(X_test), torch.LongTensor(y_test))  # X_test와 y_test는 테스트 데이터셋입니다.\n",
        "\n",
        "# DataLoader 생성\n",
        "batch_size = 32  # 적절한 배치 사이즈를 설정하세요\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)  # 테스트 데이터는 셔플하지 않음"
      ],
      "metadata": {
        "id": "5jkq2ESDZvr3"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "# 테스트 데이터에서 예측 수행\n",
        "def evaluate_model(model, test_loader, device):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():  # 평가 시 역전파 비활성화\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs = inputs.to(device)  # GPU로 데이터 전송\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)  # 가장 높은 확률로 예측된 클래스\n",
        "            all_preds.extend(preds.cpu().numpy())  # 예측 결과를 리스트에 추가\n",
        "            all_labels.extend(labels.cpu().numpy())  # 실제 라벨을 리스트에 추가\n",
        "\n",
        "    return np.array(all_preds), np.array(all_labels)\n",
        "\n",
        "# GPU 또는 CPU 사용\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 모델을 GPU로 전송\n",
        "model.to(device)\n",
        "\n",
        "# 예측 결과\n",
        "y_pred, y_true = evaluate_model(model, test_loader, device)\n",
        "\n",
        "# 정확도 및 분류 보고서 출력\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=[\"Non-assault\", \"Assault\"]))\n",
        "\n",
        "# 혼동 행렬 계산 및 시각화\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Non-assault\", \"Assault\"], yticklabels=[\"Non-assault\", \"Assault\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        },
        "id": "r1obvp-uy8Zr",
        "outputId": "b0c765bc-53fb-491f-bc18-24d326dfaa61"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9690\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " Non-assault       0.04      0.06      0.05       268\n",
            "     Assault       0.99      0.98      0.98     21507\n",
            "\n",
            "    accuracy                           0.97     21775\n",
            "   macro avg       0.51      0.52      0.51     21775\n",
            "weighted avg       0.98      0.97      0.97     21775\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAApoAAAIjCAYAAACjybtCAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABi20lEQVR4nO3deXxN19v38e9JyGBICCJSU0xBxdxqqqZSQdAUNZbEUEPNlEgNNbSi1EypXxVtqdKWX4vSEEMRs9RUioa0jwQ1xRgk5/nDnfPraagkzXYi5/N+Xvt1O2uvvfa1z33zXL3W2uuYzGazWQAAAEAmc7B1AAAAAMieSDQBAABgCBJNAAAAGIJEEwAAAIYg0QQAAIAhSDQBAABgCBJNAAAAGIJEEwAAAIYg0QQAAIAhSDQB/KOTJ0+qcePGcnd3l8lk0urVqzN1/DNnzshkMmnx4sWZOu7TrH79+qpfv76twwCAf41EE3gKnD59Wr169VKpUqXk4uIiNzc31a5dWzNnztTt27cNvXdwcLAOHz6s999/X59//rlq1qxp6P2epJCQEJlMJrm5uT30ezx58qRMJpNMJpM+/PDDdI9/7tw5jR07VtHR0ZkQLQA8fXLYOgAA/2zt2rV6/fXX5ezsrC5duqhSpUq6e/eutm/frmHDhuno0aNasGCBIfe+ffu2oqKiNHLkSPXr18+Qe5QoUUK3b99Wzpw5DRn/cXLkyKFbt27p+++/V9u2ba3OLV26VC4uLrpz506Gxj537pzGjRunkiVLqmrVqmm+7scff8zQ/QAgqyHRBLKwmJgYtW/fXiVKlFBkZKSKFCliOde3b1+dOnVKa9euNez+Fy9elCTly5fPsHuYTCa5uLgYNv7jODs7q3bt2vryyy9TJZrLli1TYGCgvvnmmycSy61bt5QrVy45OTk9kfsBgNGYOgeysMmTJ+vGjRtauHChVZKZokyZMho4cKDl8/379zVhwgSVLl1azs7OKlmypN555x0lJiZaXVeyZEk1b95c27dv1/PPPy8XFxeVKlVKn332maXP2LFjVaJECUnSsGHDZDKZVLJkSUkPppxT/vxXY8eOlclksmqLiIjQSy+9pHz58ilPnjzy9fXVO++8Yzn/qDWakZGRqlOnjnLnzq18+fLp1Vdf1S+//PLQ+506dUohISHKly+f3N3d1bVrV926devRX+zfdOzYUT/88IOuXr1qadu7d69Onjypjh07pup/+fJlvf322/Lz81OePHnk5uampk2b6ueff7b02bJli5577jlJUteuXS1T8CnPWb9+fVWqVEn79+9X3bp1lStXLsv38vc1msHBwXJxcUn1/AEBAcqfP7/OnTuX5mcFgCeJRBPIwr7//nuVKlVKL774Ypr69+jRQ2PGjFH16tU1ffp01atXT+Hh4Wrfvn2qvqdOnVKbNm30yiuvaOrUqcqfP79CQkJ09OhRSVKrVq00ffp0SVKHDh30+eefa8aMGemK/+jRo2revLkSExM1fvx4TZ06VS1bttSOHTv+8bqNGzcqICBAFy5c0NixYzVkyBDt3LlTtWvX1pkzZ1L1b9u2ra5fv67w8HC1bdtWixcv1rhx49IcZ6tWrWQymfTtt99a2pYtW6by5curevXqqfr/9ttvWr16tZo3b65p06Zp2LBhOnz4sOrVq2dJ+ipUqKDx48dLknr27KnPP/9cn3/+uerWrWsZ59KlS2ratKmqVq2qGTNmqEGDBg+Nb+bMmSpUqJCCg4OVlJQkSfr444/1448/avbs2fL29k7zswLAE2UGkCVdu3bNLMn86quvpql/dHS0WZK5R48eVu1vv/22WZI5MjLS0laiRAmzJPO2bdssbRcuXDA7Ozubhw4dammLiYkxSzJPmTLFaszg4GBziRIlUsXw7rvvmv/6z8r06dPNkswXL158ZNwp91i0aJGlrWrVqmZPT0/zpUuXLG0///yz2cHBwdylS5dU9+vWrZvVmK+99pq5QIECj7znX58jd+7cZrPZbG7Tpo25YcOGZrPZbE5KSjJ7eXmZx40b99Dv4M6dO+akpKRUz+Hs7GweP368pW3v3r2pni1FvXr1zJLM8+fPf+i5evXqWbVt2LDBLMn83nvvmX/77Tdznjx5zEFBQY99RgCwJSqaQBaVkJAgScqbN2+a+q9bt06SNGTIEKv2oUOHSlKqtZwVK1ZUnTp1LJ8LFSokX19f/fbbbxmO+e9S1nb+97//VXJycpquiYuLU3R0tEJCQuTh4WFpr1y5sl555RXLc/5V7969rT7XqVNHly5dsnyHadGxY0dt2bJF8fHxioyMVHx8/EOnzaUH6zodHB7885mUlKRLly5ZlgUcOHAgzfd0dnZW165d09S3cePG6tWrl8aPH69WrVrJxcVFH3/8cZrvBQC2QKIJZFFubm6SpOvXr6ep/9mzZ+Xg4KAyZcpYtXt5eSlfvnw6e/asVXvx4sVTjZE/f35duXIlgxGn1q5dO9WuXVs9evRQ4cKF1b59e61YseIfk86UOH19fVOdq1Chgv7880/dvHnTqv3vz5I/f35JStezNGvWTHnz5tVXX32lpUuX6rnnnkv1XaZITk7W9OnTVbZsWTk7O6tgwYIqVKiQDh06pGvXrqX5ns8880y6Xvz58MMP5eHhoejoaM2aNUuenp5pvhYAbIFEE8ii3Nzc5O3trSNHjqTrur+/jPMojo6OD203m80ZvkfK+sEUrq6u2rZtmzZu3KjOnTvr0KFDateunV555ZVUff+Nf/MsKZydndWqVSstWbJEq1atemQ1U5ImTpyoIUOGqG7duvriiy+0YcMGRURE6Nlnn01z5VZ68P2kx8GDB3XhwgVJ0uHDh9N1LQDYAokmkIU1b95cp0+fVlRU1GP7lihRQsnJyTp58qRV+/nz53X16lXLG+SZIX/+/FZvaKf4e9VUkhwcHNSwYUNNmzZNx44d0/vvv6/IyEht3rz5oWOnxHnixIlU544fP66CBQsqd+7c/+4BHqFjx446ePCgrl+//tAXqFJ8/fXXatCggRYuXKj27durcePGatSoUarvJK1Jf1rcvHlTXbt2VcWKFdWzZ09NnjxZe/fuzbTxAcAIJJpAFjZ8+HDlzp1bPXr00Pnz51OdP336tGbOnCnpwdSvpFRvhk+bNk2SFBgYmGlxlS5dWteuXdOhQ4csbXFxcVq1apVVv8uXL6e6NmXj8r9vuZSiSJEiqlq1qpYsWWKVuB05ckQ//vij5TmN0KBBA02YMEFz5syRl5fXI/s5OjqmqpauXLlS/+///T+rtpSE+GFJeXqFhoYqNjZWS5Ys0bRp01SyZEkFBwc/8nsEgKyADduBLKx06dJatmyZ2rVrpwoVKlj9MtDOnTu1cuVKhYSESJKqVKmi4OBgLViwQFevXlW9evW0Z88eLVmyREFBQY/cOicj2rdvr9DQUL322msaMGCAbt26pXnz5qlcuXJWL8OMHz9e27ZtU2BgoEqUKKELFy7oo48+UtGiRfXSSy89cvwpU6aoadOm8vf3V/fu3XX79m3Nnj1b7u7uGjt2bKY9x985ODho1KhRj+3XvHlzjR8/Xl27dtWLL76ow4cPa+nSpSpVqpRVv9KlSytfvnyaP3++8ubNq9y5c6tWrVry8fFJV1yRkZH66KOP9O6771q2W1q0aJHq16+v0aNHa/LkyekaDwCeFCqaQBbXsmVLHTp0SG3atNF///tf9e3bVyNGjNCZM2c0depUzZo1y9L3k08+0bhx47R3714NGjRIkZGRCgsL0/LlyzM1pgIFCmjVqlXKlSuXhg8friVLlig8PFwtWrRIFXvx4sX16aefqm/fvpo7d67q1q2ryMhIubu7P3L8Ro0aaf369SpQoIDGjBmjDz/8UC+88IJ27NiR7iTNCO+8846GDh2qDRs2aODAgTpw4IDWrl2rYsWKWfXLmTOnlixZIkdHR/Xu3VsdOnTQ1q1b03Wv69evq1u3bqpWrZpGjhxpaa9Tp44GDhyoqVOnateuXZnyXACQ2Uzm9KyWBwAAANKIiiYAAAAMQaIJAAAAQ5BoAgAAwBAkmgAAADAEiSYAAAAMQaIJAAAAQ5BoAgAAwBDZ8peB7ty3dQQAjJLM1r9AtpUrp8lm93at1s+wsW8fnGPY2FkdFU0AAAAYIltWNAEAANLFRO3NCCSaAAAAJttN22dnpO8AAAAwBBVNAAAAps4NwbcKAAAAQ1DRBAAAYI2mIahoAgAAwBBUNAEAAFijaQi+VQAAABiCiiYAAABrNA1BogkAAMDUuSH4VgEAAGAIKpoAAABMnRuCiiYAAAAMQUUTAACANZqG4FsFAACAIahoAgAAsEbTEFQ0AQAAYAgSTQAAAJODcUc6hIeH67nnnlPevHnl6empoKAgnThxwqrPnTt31LdvXxUoUEB58uRR69atdf78eas+sbGxCgwMVK5cueTp6alhw4bp/v37Vn22bNmi6tWry9nZWWXKlNHixYtTxTN37lyVLFlSLi4uqlWrlvbs2ZOu5yHRBAAAMJmMO9Jh69at6tu3r3bt2qWIiAjdu3dPjRs31s2bNy19Bg8erO+//14rV67U1q1bde7cObVq1cpyPikpSYGBgbp796527typJUuWaPHixRozZoylT0xMjAIDA9WgQQNFR0dr0KBB6tGjhzZs2GDp89VXX2nIkCF69913deDAAVWpUkUBAQG6cOFC2r9Ws9lsTtc38BS4c//xfQA8nZKz3z9ZAP5Prpy2WyfpWmfM4ztl0NWNI5WYmGjV5uzsLGdn58dee/HiRXl6emrr1q2qW7eurl27pkKFCmnZsmVq06aNJOn48eOqUKGCoqKi9MILL+iHH35Q8+bNde7cORUuXFiSNH/+fIWGhurixYtycnJSaGio1q5dqyNHjlju1b59e129elXr16+XJNWqVUvPPfec5syZI0lKTk5WsWLF1L9/f40YMSJNz05FEwAAwMCp8/DwcLm7u1sd4eHhaQrr2rVrkiQPDw9J0v79+3Xv3j01atTI0qd8+fIqXry4oqKiJElRUVHy8/OzJJmSFBAQoISEBB09etTS569jpPRJGePu3bvav3+/VR8HBwc1atTI0icteOscAADAQGFhYRoyZIhVW1qqmcnJyRo0aJBq166tSpUqSZLi4+Pl5OSkfPnyWfUtXLiw4uPjLX3+mmSmnE859099EhISdPv2bV25ckVJSUkP7XP8+PHHxp6CRBMAAMDADdvTOk3+d3379tWRI0e0fft2A6J6Mpg6BwAAyGL69eunNWvWaPPmzSpatKil3cvLS3fv3tXVq1et+p8/f15eXl6WPn9/Cz3l8+P6uLm5ydXVVQULFpSjo+ND+6SMkRYkmgAAAA4m4450MJvN6tevn1atWqXIyEj5+PhYna9Ro4Zy5sypTZs2WdpOnDih2NhY+fv7S5L8/f11+PBhq7fDIyIi5ObmpooVK1r6/HWMlD4pYzg5OalGjRpWfZKTk7Vp0yZLn7Rg6hwAACCL6Nu3r5YtW6b//ve/yps3r2VNpbu7u1xdXeXu7q7u3btryJAh8vDwkJubm/r37y9/f3+98MILkqTGjRurYsWK6ty5syZPnqz4+HiNGjVKffv2tUzh9+7dW3PmzNHw4cPVrVs3RUZGasWKFVq7dq0lliFDhig4OFg1a9bU888/rxkzZujmzZvq2rVrmp+H7Y0APFXY3gjIvmy6vdHL7xs29u3IkWnua3rEvpuLFi1SSEiIpAcbtg8dOlRffvmlEhMTFRAQoI8++shqSvvs2bPq06ePtmzZoty5cys4OFiTJk1Sjhz/qzFu2bJFgwcP1rFjx1S0aFGNHj3aco8Uc+bM0ZQpUxQfH6+qVatq1qxZqlWrVtqfh0QTwNOERBPIvmyaaDacaNjYtze9Y9jYWR1rNAEAAGAI1mgCAAAYuL2RPeNbBQAAgCGoaAIAADziJRz8O1Q0AQAAYAgqmgAAAKzRNATfKgAAAAxBRRMAAIA1moYg0QQAAGDq3BB8qwAAADAEFU0AAACmzg1BRRMAAACGoKIJAADAGk1D8K0CAADAEFQ0AQAAWKNpCCqaAAAAMAQVTQAAANZoGoJEEwAAgETTEHyrAAAAMAQVTQAAAF4GMgQVTQAAABiCiiYAAABrNA3BtwoAAABDUNEEAABgjaYhqGgCAADAEFQ0AQAAWKNpCBJNAAAAps4NQfoOAAAAQ1DRBAAAds9ERdMQVDQBAABgCCqaAADA7lHRNAYVTQAAABiCiiYAAAAFTUNQ0QQAAIAhqGgCAAC7xxpNY5BoAgAAu0eiaQymzgEAAGAIKpoAAMDuUdE0BhVNAAAAGIKKJgAAsHtUNI1BRRMAAACGoKIJAABAQdMQVDQBAABgCBJNAABg90wmk2FHem3btk0tWrSQt7e3TCaTVq9enaZYp0yZYulTsmTJVOcnTZpkNc6hQ4dUp04dubi4qFixYpo8eXKqWFauXKny5cvLxcVFfn5+WrduXbqehUQTAAAgC7l586aqVKmiuXPnPvR8XFyc1fHpp5/KZDKpdevWVv3Gjx9v1a9///6WcwkJCWrcuLFKlCih/fv3a8qUKRo7dqwWLFhg6bNz50516NBB3bt318GDBxUUFKSgoCAdOXIkzc/CGk0AAGD3stJb502bNlXTpk0fed7Ly8vq83//+181aNBApUqVsmrPmzdvqr4pli5dqrt37+rTTz+Vk5OTnn32WUVHR2vatGnq2bOnJGnmzJlq0qSJhg0bJkmaMGGCIiIiNGfOHM2fPz9Nz0JFEwAA2D0jp84TExOVkJBgdSQmJmZK3OfPn9fatWvVvXv3VOcmTZqkAgUKqFq1apoyZYru379vORcVFaW6devKycnJ0hYQEKATJ07oypUrlj6NGjWyGjMgIEBRUVFpji9LJJovv/yyrl69mqo9ISFBL7/88pMPCAAAIJOEh4fL3d3d6ggPD8+UsZcsWaK8efOqVatWVu0DBgzQ8uXLtXnzZvXq1UsTJ07U8OHDLefj4+NVuHBhq2tSPsfHx/9jn5TzaZElps63bNmiu3fvpmq/c+eOfvrpJxtEBAAA7ImRU+dhYWEaMmSIVZuzs3OmjP3pp5+qU6dOcnFxsWr/6/0qV64sJycn9erVS+Hh4Zl277SwaaJ56NAhy5+PHTtmlSEnJSVp/fr1euaZZ2wRGgAAQKZwdnY2JLn76aefdOLECX311VeP7VurVi3dv39fZ86cka+vr7y8vHT+/HmrPimfU9Z1PqrPo9Z9PoxNE82qVata1i88bIrc1dVVs2fPtkFkAADArmSdd4HSbOHChapRo4aqVKny2L7R0dFycHCQp6enJMnf318jR47UvXv3lDNnTklSRESEfH19lT9/fkufTZs2adCgQZZxIiIi5O/vn+YYbZpoxsTEyGw2q1SpUtqzZ48KFSpkOefk5CRPT085OjraMEIAAIAn68aNGzp16pTlc0xMjKKjo+Xh4aHixYtLevAey8qVKzV16tRU10dFRWn37t1q0KCB8ubNq6ioKA0ePFhvvPGGJYns2LGjxo0bp+7duys0NFRHjhzRzJkzNX36dMs4AwcOVL169TR16lQFBgZq+fLl2rdvn9UWSI9jMpvN5ox+EVnVnfuP7wPg6ZSc/f7JAvB/cuW0XVmxYMhyw8b+c3H7dPXfsmWLGjRokKo9ODhYixcvliQtWLBAgwYNUlxcnNzd3a36HThwQG+99ZaOHz+uxMRE+fj4qHPnzhoyZIjVFP6hQ4fUt29f7d27VwULFlT//v0VGhpqNdbKlSs1atQonTlzRmXLltXkyZPVrFmzND+LzRLN7777Ls19W7Zsma6xSTSB7ItEE8i+SDSzH5tNnQcFBaWpn8lkUlJSkrHBAAAAu5aVNmzPTmyWaCYnJ9vq1gAAAFZINI2RJTZsBwAAQPaTJTZsHz9+/D+eHzNmzBOKBAAA2CUKmobIEonmqlWrrD7fu3dPMTExypEjh0qXLk2iCQAA8BTKEonmwYMHU7UlJCQoJCREr732mg0iAgAA9oQ1msbIsms03dzcNG7cOI0ePdrWoQAAACADskRF81GuXbuma9eu2ToMAACQzVHRNEaWSDRnzZpl9dlsNisuLk6ff/65mjZtaqOoAAAA8G9kiUTzr7+rKUkODg4qVKiQgoODFRYWZqOoAACAvaCiaYwskWjGxMTYOgQAAGDHSDSNkWVfBgIAAMDTLUtUNCVp3759WrFihWJjY3X37l2rc99++62NogIAAHaBgqYhskRFc/ny5XrxxRf1yy+/aNWqVbp3756OHj2qyMhIubu72zo8AAAAZECWSDQnTpyo6dOn6/vvv5eTk5Nmzpyp48ePq23btipevLitwwMAANmcyWQy7LBnWSLRPH36tAIDAyVJTk5OunnzpkwmkwYPHqwFCxbYODoAAABkRJZINPPnz6/r169Lkp555hkdOXJEknT16lXdunXLlqEBAAA7QEXTGFniZaC6desqIiJCfn5+ev311zVw4EBFRkYqIiJCDRs2tHV4AAAAyIAskWjOmTNHd+7ckSSNHDlSOXPm1M6dO9W6dWuNGjXKxtEBAIDszt4rj0bJEommh4eH5c8ODg4aMWKEDaMBAAB2hzzTEFlijeaBAwd0+PBhy+f//ve/CgoK0jvvvJNqT00AAAA8HbJEotmrVy/9+uuvkqTffvtN7dq1U65cubRy5UoNHz7cxtEBAIDsjpeBjJElEs1ff/1VVatWlSStXLlS9erV07Jly7R48WJ98803tg0OAAAAGZIl1miazWYlJydLkjZu3KjmzZtLkooVK6Y///zTlqEBAAA7YO+VR6NkiYpmzZo19d577+nzzz/X1q1bLZu3x8TEqHDhwjaODgAAABmRJSqaM2bMUKdOnbR69WqNHDlSZcqUkSR9/fXXevHFF20cHbKC/fv2avGnC/XLsSO6ePGips+aq5cbNrLq89vp05oxbYr279ur+0lJKl2qtKbOmK0i3t42ihrA3y38z8eK3BihMzG/ydnFRVWqVtPAwUNV0qeUpU+PkM7av2+v1XWtX2+nUe+OkySdOH5cixYuUPSBA7p69Yq8vZ9Rm7bt1bFzlyf6LMheqGgaI0skmpUrV7Z66zzFlClT5OjoaIOIkNXcvn1Lvr6+CmrVWkMG9kt1/vfYWIV07qjXWrVWn34DlCd3Hp0+dVJOzs42iBbAoxzYt1ftOnTUs5X8dP9+kubMnK4+PXvo2/+ukWuuXJZ+rdq8rj79Blg+u7i4Wv78y7Gj8vAooPcmTZaXVxH9HH1Q740bIwdHB7Xv+MYTfR4A/yxLJJq///67TCaTihYtKknas2ePli1bpooVK6pnz542jg5ZwUt16umlOvUeeX72rOl6qW5dDX77f7sUFCte/EmEBiAd5n78idXnce+Hq2HdF3Xs2FHVqPmcpd3FxVUFCxZ66BhBrVpbfS5arJgO/RytyI0RJJrIMCqaxsgSazQ7duyozZs3S5Li4+P1yiuvaM+ePRo5cqTGjx9v4+iQ1SUnJ+unrVtUokRJ9X6zu+rX8Ven9q8rctNGW4cG4DFu3LguSXJ3d7dqX7f2ezV46QW1CWqhWdOn6vbt2/88zvXrcvvbGEC6mAw87FiWSDSPHDmi559/XpK0YsUKVapUSTt37tTSpUu1ePHif7w2MTFRCQkJVkdiYuITiBpZxeVLl3Tr1i19uvA/qv1SHc1f8KlebviKhgzsp31799g6PACPkJycrA8nTVTVatVVpmw5S3vTwOZ6f9JkLfh0ibr16Km1a77TqBGP3lM5+uAB/bjhB7Vu0/ZJhA0gHbLE1Pm9e/fk/H9r6TZu3KiWLVtKksqXL6+4uLh/vDY8PFzjxo2zahs5+l2NGjPWkFiR9SSbH2yN1aBBQ3UODpEkla9QQT9HH9DKr5ar5nPP2zA6AI8S/t54nTp1Uos+W2bV3vr1dpY/ly3nq4KFCqlX9xD9HhubaknMqZO/avCAvurZp6/8a7/0ROJG9sTUuTGyREXz2Wef1fz58/XTTz8pIiJCTZo0kSSdO3dOBQoU+Mdrw8LCdO3aNatjWGjYkwgbWUT+fPmVI0cOlSpd2qrdp1Rpxceds1FUAP7JpPfH66etW/SfTz9TYS+vf+zr51dZkvT772et2k+fPqVe3buqdZu2erNXH8NiBZBxWaKi+cEHH+i1117TlClTFBwcrCpVqkiSvvvuO8uU+qM4OztbqqEp7tw3LFRkQTmdnPRsJT+dORNj1X727BkV8X7GRlEBeBiz2awPJk5Q5KaN+s+iz/TM/70E+k9OHD8uSSpY0NPSdvrUSfXsFqIWrwap38DBhsUL+0FF0xhZItGsX7++/vzzTyUkJCh//vyW9p49eyrXX7a7gP26dfOmYmNjLZ//3x9/6Pgvv8jd3V1FvL0V3LW7hg8drBo1ntNzz9fSju0/aduWzfpk0Wc2jBrA34W/N14/rFuj6bPmKnfu3Przz4uSpDx58srFxUW/x8bqh3Vr9FKdusqXL59+/fVXTf0gXNVr1lQ5X19JD6bLe3YP0YsvvqQ3gkMsYzg4OMrDw8NmzwYgNZPZbDbbOojMRkUz+9m7Z7d6dE29GXPLV1/ThImTJEmrvv1an/5ngc6fj1fJkj7q06+/GrzcKNU1eLolZ79/suxKtUrlH9o+7r2JahnUSvFxcRoZNkynT57U7du3VdiriF5u2Eg9evVRnjx5JEnz587Wx/PmphqjiLe31v0YaWj8MFaunLarKpZ5+wfDxj71YVPDxs7qskyi+fXXX2vFihWKjY3V3bt3rc4dOHAgXWORaALZF4kmkH2RaGY/WeJloFmzZqlr164qXLiwDh48qOeff14FChTQb7/9pqZN7fd/OQAA4MkwmUyGHfYsSySaH330kRYsWKDZs2fLyclJw4cPV0REhAYMGKBr167ZOjwAAJDNmUzGHfYsSySasbGxevHFFyVJrq6uun79wS9FdO7cWV9++aUtQwMAAEAGZYlE08vLS5cvX5YkFS9eXLt27ZIkxcTEKIssIQUAANkYU+fGyBKJ5ssvv6zvvvtOktS1a1cNHjxYr7zyitq1a6fXXnvNxtEBAAAgI7LEW+fJyclKTk5WjhwPtvVcvny5du7cqbJly6pXr15ycnJK13i8dQ5kX7x1DmRftnzrvPyIDYaNfXxSgGFjZ3VZItHMbCSaQPZFoglkXySa2U+WmDr/Kz8/P/3++++2DgMAANgRBweTYYc9y3KJ5pkzZ3Tv3j1bhwEAAGAT27ZtU4sWLeTt7S2TyaTVq1dbnQ8JCUn1wlGTJk2s+ly+fFmdOnWSm5ub8uXLp+7du+vGjRtWfQ4dOqQ6derIxcVFxYoV0+TJk1PFsnLlSpUvX14uLi7y8/PTunXr0vUsWS7RBAAAeNKy0j6aN2/eVJUqVTR3buqfWk3RpEkTxcXFWY6/bwfZqVMnHT16VBEREVqzZo22bdumnj17Ws4nJCSocePGKlGihPbv368pU6Zo7NixWrBggaXPzp071aFDB3Xv3l0HDx5UUFCQgoKCdOTIkTQ/S5Zbo9msWTMtXLhQRYoUyfAYrNEEsi/WaALZly3XaFYaFWHY2EfeeyXD15pMJq1atUpBQUGWtpCQEF29ejVVpTPFL7/8oooVK2rv3r2qWbOmJGn9+vVq1qyZ/vjjD3l7e2vevHkaOXKk4uPjLS9djxgxQqtXr9bx48clSe3atdPNmze1Zs0ay9gvvPCCqlatqvnz56cp/ixX0Vy3bt2/SjIBAACyksTERCUkJFgdiYmJ/2rMLVu2yNPTU76+vurTp48uXbpkORcVFaV8+fJZkkxJatSokRwcHLR7925Ln7p161rt7BMQEKATJ07oypUrlj6NGjWyum9AQICioqLSHGeODD2dAU6ePKnNmzfrwoULSk5Otjo3ZswYG0UFAADsgZH7qoeHh2vcuHFWbe+++67Gjh2bofGaNGmiVq1aycfHR6dPn9Y777yjpk2bKioqSo6OjoqPj5enp6fVNTly5JCHh4fi4+MlSfHx8fLx8bHqU7hwYcu5/PnzKz4+3tL21z4pY6RFlkg0//Of/6hPnz4qWLCgvLy8rHbRN5lMJJoAAOCpFRYWpiFDhli1OTs7Z3i89u3bW/7s5+enypUrq3Tp0tqyZYsaNmyY4XGNkCUSzffee0/vv/++QkNDbR0KAACwQ0b+VKSzs/O/Siwfp1SpUipYsKBOnTqlhg0bysvLSxcuXLDqc//+fV2+fFleXl6SHvz89/nz5636pHx+XJ+U82mRJdZoXrlyRa+//rqtwwAAAHjq/PHHH7p06ZLlHRd/f39dvXpV+/fvt/SJjIxUcnKyatWqZemzbds2qy0lIyIi5Ovrq/z581v6bNq0yepeERER8vf3T3NsWSLRfP311/Xjjz/aOgwAAGCn/r4vZWYe6XXjxg1FR0crOjpakhQTE6Po6GjFxsbqxo0bGjZsmHbt2qUzZ85o06ZNevXVV1WmTBkFBDz4BaIKFSqoSZMmevPNN7Vnzx7t2LFD/fr1U/v27eXt7S1J6tixo5ycnNS9e3cdPXpUX331lWbOnGk1xT9w4ECtX79eU6dO1fHjxzV27Fjt27dP/fr1S/v3mhW2NwoPD9e0adMUGBgoPz8/5cyZ0+r8gAED0jUe2xsB2RfbGwHZly23N6ry7qbHd8qgn8elb93kli1b1KBBg1TtwcHBmjdvnoKCgnTw4EFdvXpV3t7eaty4sSZMmGD14s7ly5fVr18/ff/993JwcFDr1q01a9Ys5cmTx9Ln0KFD6tu3r/bu3auCBQuqf//+qZYxrly5UqNGjdKZM2dUtmxZTZ48Wc2aNUvzs2SJRPPvbz39lclk0m+//Zau8Ug0geyLRBPIvmyZaFYda1yiGT02a72g8yRliZeBYmJibB0CAACwY0a+DGTPssQazb8ym83KAkVWAAAA/EtZJtH87LPP5OfnJ1dXV7m6uqpy5cr6/PPPbR0WAACwA1npt86zkywxdT5t2jSNHj1a/fr1U+3atSVJ27dvV+/evfXnn39q8ODBNo4QAAAA6ZUlEs3Zs2dr3rx56tKli6WtZcuWevbZZzV27FgSTQAAYCjWaBojS0ydx8XF6cUXX0zV/uKLLyouLs4GEQEAAODfyhKJZpkyZbRixYpU7V999ZXKli1rg4gAAIA9YY2mMbLE1Pm4cePUrl07bdu2zbJGc8eOHdq0adNDE1AAAABkfVki0WzdurV2796tadOmafXq1ZIe/HzSnj17VK1aNdsGBwAAsj3WaBojSySaklSjRg0tXbrU1mEAAAAgk9g00XRwcHjsf0GYTCbdv89vSgIAAONQ0DSGTRPNVatWPfJcVFSUZs2apeTk5CcYEQAAsEdMnRvDponmq6++mqrtxIkTGjFihL7//nt16tRJ48ePt0FkAAAA+LeyxPZGknTu3Dm9+eab8vPz0/379xUdHa0lS5aoRIkStg4NAABkc2xvZAybJ5rXrl1TaGioypQpo6NHj2rTpk36/vvvValSJVuHBgAAgH/BplPnkydP1gcffCAvLy99+eWXD51KBwAAMBprNI1hMpvNZlvd3MHBQa6urmrUqJEcHR0f2e/bb79N17h3eEkdyLaSbfdPFgCD5cppu2TP/4Ntho0dFVrXsLGzOptWNLt06cJ/QQAAAJsjHTGGTRPNxYsX2/L2AAAAMFCW+WUgAAAAW2GG1RgkmgAAwO6RZxrD5tsbAQAAIHuiogkAAOweU+fGoKIJAAAAQ1DRBAAAdo+KpjGoaAIAAMAQVDQBAIDdo6BpDCqaAAAAMAQVTQAAYPdYo2kMEk0AAGD3yDONwdQ5AAAADEFFEwAA2D2mzo1BRRMAAACGoKIJAADsHgVNY1DRBAAAgCGoaAIAALvnQEnTEFQ0AQAAYAgqmgAAwO5R0DQGiSYAALB7bG9kDKbOAQAAYAgqmgAAwO45UNA0BBVNAAAAGIKKJgAAsHus0TQGFU0AAAAYgkQTAADYPZPJuCO9tm3bphYtWsjb21smk0mrV6+2nLt3755CQ0Pl5+en3Llzy9vbW126dNG5c+esxihZsqRMJpPVMWnSJKs+hw4dUp06deTi4qJixYpp8uTJqWJZuXKlypcvLxcXF/n5+WndunXpehYSTQAAgCzk5s2bqlKliubOnZvq3K1bt3TgwAGNHj1aBw4c0LfffqsTJ06oZcuWqfqOHz9ecXFxlqN///6WcwkJCWrcuLFKlCih/fv3a8qUKRo7dqwWLFhg6bNz50516NBB3bt318GDBxUUFKSgoCAdOXIkzc9iMpvN5nQ+f5Z3576tIwBglOTs908WgP+TK6ft1kk2/3ivYWOv6fVchq81mUxatWqVgoKCHtln7969ev7553X27FkVL15c0oOK5qBBgzRo0KCHXjNv3jyNHDlS8fHxcnJykiSNGDFCq1ev1vHjxyVJ7dq1082bN7VmzRrLdS+88IKqVq2q+fPnpyl+KpoAAMDuOZiMOxITE5WQkGB1JCYmZlrs165dk8lkUr58+azaJ02apAIFCqhatWqaMmWK7t//XyUuKipKdevWtSSZkhQQEKATJ07oypUrlj6NGjWyGjMgIEBRUVFpjo1EEwAAwEDh4eFyd3e3OsLDwzNl7Dt37ig0NFQdOnSQm5ubpX3AgAFavny5Nm/erF69emnixIkaPny45Xx8fLwKFy5sNVbK5/j4+H/sk3I+LdjeCAAA2D0jtzcKCwvTkCFDrNqcnZ3/9bj37t1T27ZtZTabNW/ePKtzf71f5cqV5eTkpF69eik8PDxT7p1WJJoAAAAGcnZ2zvTkLiXJPHv2rCIjI62qmQ9Tq1Yt3b9/X2fOnJGvr6+8vLx0/vx5qz4pn728vCz/82F9Us6nBVPnAADA7mWl7Y0eJyXJPHnypDZu3KgCBQo89pro6Gg5ODjI09NTkuTv769t27bp3r17lj4RERHy9fVV/vz5LX02bdpkNU5ERIT8/f3THCsVTQAAgCzkxo0bOnXqlOVzTEyMoqOj5eHhoSJFiqhNmzY6cOCA1qxZo6SkJMuaSQ8PDzk5OSkqKkq7d+9WgwYNlDdvXkVFRWnw4MF64403LElkx44dNW7cOHXv3l2hoaE6cuSIZs6cqenTp1vuO3DgQNWrV09Tp05VYGCgli9frn379lltgfQ4bG8E4KnC9kZA9mXL7Y1aLdxv2Njfdq+Rrv5btmxRgwYNUrUHBwdr7Nix8vHxeeh1mzdvVv369XXgwAG99dZbOn78uBITE+Xj46POnTtryJAhVlP4hw4dUt++fbV3714VLFhQ/fv3V2hoqNWYK1eu1KhRo3TmzBmVLVtWkydPVrNmzdL8LCSaAJ4qJJpA9kWimf0wdQ4AAOyegS+d2zUSTQAAYPeM3N7InvHWOQAAAAxBRRMAANg9CprGoKIJAAAAQ1DRBAAAds+BkqYhqGgCAADAEFQ0AQCA3aOeaQwqmgAAADAEFU0AAGD32EfTGCSaAADA7jmQZxqCqXMAAAAYgoomAACwe0ydG4OKJgAAAAxBRRMAANg9CprGoKIJAAAAQ1DRBAAAdo81msZIU6L53XffpXnAli1bZjgYAAAAZB9pSjSDgoLSNJjJZFJSUtK/iQcAAOCJYx9NY6Qp0UxOTjY6DgAAAJth6twYvAwEAAAAQ2ToZaCbN29q69atio2N1d27d63ODRgwIFMCAwAAeFKoZxoj3YnmwYMH1axZM926dUs3b96Uh4eH/vzzT+XKlUuenp4kmgAAAJCUganzwYMHq0WLFrpy5YpcXV21a9cunT17VjVq1NCHH35oRIwAAACGcjCZDDvsWboTzejoaA0dOlQODg5ydHRUYmKiihUrpsmTJ+udd94xIkYAAAA8hdKdaObMmVMODg8u8/T0VGxsrCTJ3d1dv//+e+ZGBwAA8ASYTMYd9izdazSrVaumvXv3qmzZsqpXr57GjBmjP//8U59//rkqVapkRIwAAAB4CqW7ojlx4kQVKVJEkvT+++8rf/786tOnjy5evKgFCxZkeoAAAABGM5lMhh32LN0VzZo1a1r+7OnpqfXr12dqQAAAAMgeMrSPJgAAQHZi54VHw6Q70fTx8fnHMvBvv/32rwICAAB40ux9GyKjpDvRHDRokNXne/fu6eDBg1q/fr2GDRuWWXEBAADgKZfuRHPgwIEPbZ87d6727dv3rwMCAAB40ihoGiPdb50/StOmTfXNN99k1nAAAAB4ymXay0Bff/21PDw8Mms4AACAJ8betyEySoY2bP/r/zLMZrPi4+N18eJFffTRR5kaHAAAAJ5e6U40X331VatE08HBQYUKFVL9+vVVvnz5TA0uo5KSzbYOAYBBCtbqb+sQABjk9sE5Nrt3pq0lhJV0J5pjx441IAwAAABkN+lO4B0dHXXhwoVU7ZcuXZKjo2OmBAUAAPAk8ROUxkh3RdNsfvi0dGJiopycnP51QAAAAE+ag33ng4ZJc6I5a9YsSQ8y/k8++UR58uSxnEtKStK2bduyzBpNAAAA2F6aE83p06dLelDRnD9/vtU0uZOTk0qWLKn58+dnfoQAAAAGo6JpjDQnmjExMZKkBg0a6Ntvv1X+/PkNCwoAAABPv3Sv0dy8ebMRcQAAANiMvb+0Y5R0v3XeunVrffDBB6naJ0+erNdffz1TggIAALBX27ZtU4sWLeTt7S2TyaTVq1dbnTebzRozZoyKFCkiV1dXNWrUSCdPnrTqc/nyZXXq1Elubm7Kly+funfvrhs3blj1OXTokOrUqSMXFxcVK1ZMkydPThXLypUrVb58ebm4uMjPz0/r1q1L17OkO9Hctm2bmjVrlqq9adOm2rZtW3qHAwAAsDkHk3FHet28eVNVqlTR3LlzH3p+8uTJmjVrlubPn6/du3crd+7cCggI0J07dyx9OnXqpKNHjyoiIkJr1qzRtm3b1LNnT8v5hIQENW7cWCVKlND+/fs1ZcoUjR07VgsWLLD02blzpzp06KDu3bvr4MGDCgoKUlBQkI4cOZLmZzGZH7Vf0SO4uroqOjpavr6+Vu3Hjx9XtWrVdPv27fQMZ4ibd/llICC74peBgOzLlr8MNGzNCcPGntLc9/GdHsFkMmnVqlUKCgqS9KCa6e3traFDh+rtt9+WJF27dk2FCxfW4sWL1b59e/3yyy+qWLGi9u7dq5o1a0qS1q9fr2bNmumPP/6Qt7e35s2bp5EjRyo+Pt6yPeWIESO0evVqHT9+XJLUrl073bx5U2vWrLHE88ILL6hq1appfgE83RVNPz8/ffXVV6naly9frooVK6Z3OAAAAJszmYw7EhMTlZCQYHUkJiZmKM6YmBjFx8erUaNGljZ3d3fVqlVLUVFRkqSoqCjly5fPkmRKUqNGjeTg4KDdu3db+tStW9dqD/SAgACdOHFCV65csfT5631S+qTcJy3S/TLQ6NGj1apVK50+fVovv/yyJGnTpk1atmyZvv766/QOBwAAYHMOBr4MFB4ernHjxlm1vfvuuxn6We/4+HhJUuHCha3aCxcubDkXHx8vT09Pq/M5cuSQh4eHVR8fH59UY6Scy58/v+Lj4//xPmmR7kSzRYsWWr16tSZOnKivv/5arq6uqlKliiIjI+Xh4ZHe4QAAALK1sLAwDRkyxKrN2dnZRtE8WelONCUpMDBQgYGBkh4sJv3yyy/19ttva//+/UpKSsrUAAEAAIyW7rWE6eDs7JxpiaWXl5ck6fz58ypSpIil/fz586pataqlz4ULF6yuu3//vi5fvmy53svLS+fPn7fqk/L5cX1SzqdFhr/Xbdu2KTg4WN7e3po6dapefvll7dq1K6PDAQAA4DF8fHzk5eWlTZs2WdoSEhK0e/du+fv7S5L8/f119epV7d+/39InMjJSycnJqlWrlqXPtm3bdO/ePUufiIgI+fr6Wn6Ux9/f3+o+KX1S7pMW6apoxsfHa/HixVq4cKESEhLUtm1bJSYmavXq1bwIBAAAnlpZab/2Gzdu6NSpU5bPMTExio6OloeHh4oXL65BgwbpvffeU9myZeXj46PRo0fL29vb8mZ6hQoV1KRJE7355puaP3++7t27p379+ql9+/by9vaWJHXs2FHjxo1T9+7dFRoaqiNHjmjmzJmWnxyXpIEDB6pevXqaOnWqAgMDtXz5cu3bt89qC6THSXNFs0WLFvL19dWhQ4c0Y8YMnTt3TrNnz07zjQAAAPB4+/btU7Vq1VStWjVJ0pAhQ1StWjWNGTNGkjR8+HD1799fPXv21HPPPacbN25o/fr1cnFxsYyxdOlSlS9fXg0bNlSzZs300ksvWSWI7u7u+vHHHxUTE6MaNWpo6NChGjNmjNVemy+++KKWLVumBQsWqEqVKvr666+1evVqVapUKc3PkuZ9NHPkyKEBAwaoT58+Klu2rKU9Z86c+vnnn7NURZN9NIHsi300gezLlvtojl5/8vGdMmhCk7KP75RNpbmiuX37dl2/fl01atRQrVq1NGfOHP35559GxgYAAICnWJoTzRdeeEH/+c9/FBcXp169emn58uXy9vZWcnKyIiIidP36dSPjBAAAMIyRG7bbs3S/dZ47d25169ZN27dv1+HDhzV06FBNmjRJnp6eatmypRExAgAAGCor/dZ5dvKvto3y9fXV5MmT9ccff+jLL7/MrJgAAACQDWRow/a/c3R0VFBQkOW1egAAgKeJkT9Bac+M3AgfAAAAdixTKpoAAABPMwqaxqCiCQAAAENQ0QQAAHbP3t8ONwoVTQAAABiCiiYAALB7JlHSNAKJJgAAsHtMnRuDqXMAAAAYgoomAACwe1Q0jUFFEwAAAIagogkAAOyeiR3bDUFFEwAAAIagogkAAOweazSNQUUTAAAAhqCiCQAA7B5LNI1BogkAAOyeA5mmIZg6BwAAgCGoaAIAALvHy0DGoKIJAAAAQ1DRBAAAdo8lmsagogkAAABDUNEEAAB2z0GUNI1ARRMAAACGoKIJAADsHms0jUGiCQAA7B7bGxmDqXMAAAAYgoomAACwe/wEpTGoaAIAAMAQVDQBAIDdo6BpDCqaAAAAMAQVTQAAYPdYo2kMKpoAAAAwBBVNAABg9yhoGoNEEwAA2D2meI3B9woAAABDUNEEAAB2z8TcuSGoaAIAAMAQVDQBAIDdo55pDCqaAAAAMASJJgAAsHsOJpNhR3qULFlSJpMp1dG3b19JUv369VOd6927t9UYsbGxCgwMVK5cueTp6alhw4bp/v37Vn22bNmi6tWry9nZWWXKlNHixYv/1ff3KEydAwAAZBF79+5VUlKS5fORI0f0yiuv6PXXX7e0vfnmmxo/frzlc65cuSx/TkpKUmBgoLy8vLRz507FxcWpS5cuypkzpyZOnChJiomJUWBgoHr37q2lS5dq06ZN6tGjh4oUKaKAgIBMfR4STQAAYPeyyhrNQoUKWX2eNGmSSpcurXr16lnacuXKJS8vr4de/+OPP+rYsWPauHGjChcurKpVq2rChAkKDQ3V2LFj5eTkpPnz58vHx0dTp06VJFWoUEHbt2/X9OnTMz3RZOocAADYPZPJuCMxMVEJCQlWR2Ji4mNjunv3rr744gt169bNavulpUuXqmDBgqpUqZLCwsJ069Yty7moqCj5+fmpcOHClraAgAAlJCTo6NGjlj6NGjWyuldAQICioqL+7deYCokmAACAgcLDw+Xu7m51hIeHP/a61atX6+rVqwoJCbG0dezYUV988YU2b96ssLAwff7553rjjTcs5+Pj462STEmWz/Hx8f/YJyEhQbdv387oYz4UU+cAAMDuGblhe1hYmIYMGWLV5uzs/NjrFi5cqKZNm8rb29vS1rNnT8uf/fz8VKRIETVs2FCnT59W6dKlMy/oTEKiCQAAYCBnZ+c0JZZ/dfbsWW3cuFHffvvtP/arVauWJOnUqVMqXbq0vLy8tGfPHqs+58+flyTLuk4vLy9L21/7uLm5ydXVNV1xPg5T5wAAwO45GHhkxKJFi+Tp6anAwMB/7BcdHS1JKlKkiCTJ399fhw8f1oULFyx9IiIi5ObmpooVK1r6bNq0yWqciIgI+fv7ZzDaRyPRBAAAyEKSk5O1aNEiBQcHK0eO/00+nz59WhMmTND+/ft15swZfffdd+rSpYvq1q2rypUrS5IaN26sihUrqnPnzvr555+1YcMGjRo1Sn379rVUVXv37q3ffvtNw4cP1/Hjx/XRRx9pxYoVGjx4cKY/C1PnAADA7hm5RjO9Nm7cqNjYWHXr1s2q3cnJSRs3btSMGTN08+ZNFStWTK1bt9aoUaMsfRwdHbVmzRr16dNH/v7+yp07t4KDg6323fTx8dHatWs1ePBgzZw5U0WLFtUnn3yS6VsbSZLJbDabM31UG7t5N9s9EoD/U7BWf1uHAMAgtw/Osdm9V0SfM2zstlW9H98pm6KiCQAA7F7WqWdmL6zRBAAAgCGoaAIAALuXldZoZickmgAAwO4xxWsMvlcAAAAYgoomAACwe0ydG4OKJgAAAAxBRRMAANg96pnGoKIJAAAAQ1DRBAAAdo8lmsagogkAAABDUNEEAAB2z4FVmoYg0QQAAHaPqXNjMHUOAAAAQ1DRBAAAds/E1LkhqGgCAADAEFQ0AQCA3WONpjGoaAIAAMAQNk80Y2NjZTabU7WbzWbFxsbaICIAAGBvHGQy7LBnNk80fXx8dPHixVTtly9flo+Pjw0iAgAAQGaw+RpNs9ks00MWRty4cUMuLi42iAgAANgb1mgaw2aJ5pAhQyRJJpNJo0ePVq5cuSznkpKStHv3blWtWtVG0QEAAHtComkMmyWaBw8elPSgonn48GE5OTlZzjk5OalKlSp6++23bRUeAAAA/iWbJZqbN2+WJHXt2lUzZ86Um5ubrUIBAAB2jg3bjWHzNZqLFi2ydQgAAAAwgE0SzVatWqW577fffmtgJAAAAJIDBU1D2CTRdHd3t8VtAQAA8ATZJNFkuhwAAGQlrNE0hs03bAcAAED2ZPOXgXx8fB66YXuK33777QlGAwAA7BH7aBrD5onmoEGDrD7fu3dPBw8e1Pr16zVs2DDbBAUAAOwKU+fGsHmiOXDgwIe2z507V/v27XvC0QAAACCzZNk1mk2bNtU333xj6zAAAIAdcDAZd9izLJtofv311/Lw8LB1GAAAAMggm0+dV6tWzeplILPZrPj4eF28eFEfffSRDSMDAAD2gjWaxrB5ohkUFGT12cHBQYUKFVL9+vVVvnx52wQFAACAf83miea7775r6xCQxS36ZIFmz5ymDm900bDQd3Tt2lXNnztbu6J2KD4uTvnze6j+yw3Vp99A5c2bV5J09eoVjRwxTCd/PaFrV6/Kw6OA6jV4Wf0GDlGePHls/ERA9vR2t8YKermKypUsrNuJ97T75980cuZ/dfLsBUufbq1qq13Tmqpavqjc8rjKq84wXbtx22qc/G65NC30dTWrW0nJZrNWb4rW25O/1s3bdy19GvlX0OjezVShdBHduXtPOw6cVujUbxUbdzlVXP5VSunHTwbq6Ok4vdB+knFfAJ5qbG9kjCy1RvPOnTtKSEiwOmDfjh45rG++/kply/la2i5euKCLFy9o0NDhWrHqe419L1w7d/yk8e+OtPRxMDmofoOGmjH7I61as15j3wvXnl1Rmjie/7ABjFKnehnN/2qb6nX5UM37zFGOHI5aM6+fcrk4WfrkcsmpiJ3HNOXTHx85zqKJwapQuoia95mj1gPm66XqZTR3dEfL+RLeBbRyek9t2furarWfpJZvzVWBfLm1fOqbqcZyz+OqTyZ01uY9v2buwwJIE5tXNG/evKnQ0FCtWLFCly5dSnU+KSnJBlEhK7h166ZGjnhbo9+doE8WzLO0lylbTh9On235XKxYcfXtP1ijwobp/v37ypEjh9zc3fV6uw6WPt7ez+j19h302aJPn+gzAPbk1X7W6+p7vvuFfo+cpGoVi2nHgdOSpDnLtkiS6tQo+9AxfH0KK6D2s6rdabIOHIuVJA35YKVWz+6jsOmrFHfxmqpXLCZHBweNnbtGZrNZkjTjs01aOb2ncuRw0P37yZbxZo9qr6/W71NSklktGlTO7EdGNkJB0xg2r2gOHz5ckZGRmjdvnpydnfXJJ59o3Lhx8vb21meffWbr8GBDk94fr5fq1Fct/xcf2/fGjevKnSePcuR4+H87XbxwXpEbI1S95nOZHSaAR3DL4yJJunLtVpqvqVXZR1cSblmSTEmK3H1CyclmPVephCTpwLHflWxOVpdXX5CDg0lueVzUMfB5Re4+YZVkdm75gnyeKaD3P/4hk54I2ZmDyWTYYc9sXtH8/vvv9dlnn6l+/frq2rWr6tSpozJlyqhEiRJaunSpOnXq9I/XJyYmKjEx0artvslJzs7ORoYNg234Ya2OHzumz5d//di+V65c0X8+nqdWbdqmOhc2fIi2bo7UnTt3VLd+A40Z954R4QL4G5PJpClvt9HOg6d17HRcmq8rXMBNFy9ft2pLSkrW5YRbKlzQTZJ09twlNX9rrr74oJvmjGyvHDkctevn3xTU738zH6WLF9KEAS3VqNsMJSUlC4Bt2LyiefnyZZUqVUqS5ObmpsuXHyzkfumll7Rt27bHXh8eHi53d3er48PJ4YbGDGPFx8dpyqSJem/Sh4/9D4YbN25oYN9eKlWqtHr16Zfq/NDhYVr61beaPusj/fH775o2hRcBgCdhRlhbPVumiLqMWJTpYxcukFcfje6opd/v1ktvTFGj7tN1916Sln3YXZLk4GDSkokhem/+Op2KvfCY0YAHTAYe9szmFc1SpUopJiZGxYsXV/ny5bVixQo9//zz+v7775UvX77HXh8WFqYhQ4ZYtd03OT2iN54Gvxw9qsuXL6lTu1aWtqSkJB3Yv08rvlyqXfsPydHRUTdv3lC/3j2UK1duTZ05Rzlz5kw1VsGChVSwYCH5lColN3d3dQ/upB69+qhQIc8n+UiAXZke+rqa1amkRt1n6P9duJqua89fSlAhj7xWbY6ODvJwy6Xzfz54QbRXu7pKuHFbI2f+19Kn28glOrXhPT3vV1InYs6rxrMlVMW3qKaHvi7pQfLp4OCg63tnqvlbc7V1Ly8HAU+CzRPNrl276ueff1a9evU0YsQItWjRQnPmzNG9e/c0bdq0x17v7Oycqup1867ZqHDxBDz/wgta8e13Vm1jR7+jkj6lFNKthxwdHXXjxg317dVdTk5Omj77ozQtlUhOfjB9du/u3cf0BJBR00NfV8uXq6jxmzN19lzqFzwfZ/ehGOV3y6VqFYrp4C+/S5LqP1dODg4m7T1yVpKUy8VJycnW/84n/d/fbwcHkxJu3lGNNu9bne/Zto7qP1dOHYct1Jn/l/64YAfsvfRoEJtPnQ8ePFgDBgyQJDVq1EjHjx/XsmXLdPDgQQ0cONDG0cEWcufOozJly1kdrq6ucs+XT2XKltONGzf0Vq/uun37tsaMf183b97Qn39e1J9/XrTsUrB921b9d9U3OnXyV537f3/op21bNHHCWFWtVl3ezxS18RMC2dOMsLZqH/icgt9ZrBs376hwgbwqXCCvXJz/N9tQuEBeVS73jEoXLyhJqlTWW5XLPaP8brkkSSdizmvDjqOaO7qjaj5bQv5VSmn6iLZaueGA4i5ekyT98NNR1Xi2uMJ6NlHp4oVUtXxRfTz2DZ09d0nRx/+Q2WzWsdNxVsfFyzd05+59HTsdp1t3+I9NZF1jx46VyWSyOv76AzZ37txR3759VaBAAeXJk0etW7fW+fPnrcaIjY1VYGCgcuXKJU9PTw0b9mBXlr/asmWLqlevLmdnZ5UpU0aLFy825HlsXtH8uxIlSsjd3T1N0+awT8d/Oaojh36WJL3arLHVuTXrN8r7maJydnHWqm9WauqUSbp3964Ke3np5YaN1bV76n32AGSOXm3rSpIiPhlk1f7mmM/1xfe7JUk92tTRqN7NLOc2fjo4VZ+u7yzR9BFtte7j/kpOfrBh+9DJKy3XbN37q0LeWaLBwY00JPgV3bpzV7sPxahl3490J/GekY+IbCwr/QTls88+q40bN1o+/3VHlcGDB2vt2rVauXKl3N3d1a9fP7Vq1Uo7duyQ9GCpWWBgoLy8vLRz507FxcWpS5cuypkzpyZOnChJiomJUWBgoHr37q2lS5dq06ZN6tGjh4oUKaKAgIBMfRaTOWUTMhv54IMPVLJkSbVr106S1LZtW33zzTfy8vLSunXrVKVKlXSPydQ5kH0VrNXf1iEAMMjtg3Nsdu/dp68ZNnbVoi6pdsh52NI/6UFFc/Xq1YqOjk517tq1aypUqJCWLVumNm3aSJKOHz+uChUqKCoqSi+88IJ++OEHNW/eXOfOnVPhwoUlSfPnz1doaKguXrwoJycnhYaGau3atTpy5Ihl7Pbt2+vq1atav359Jj55Fpg6nz9/vooVKyZJioiIUEREhH744Qc1bdpUw4YNs3F0AADAHphMxh0P2yEnPPzRO+ScPHlS3t7eKlWqlDp16qTY2Af7yu7fv1/37t1To0aNLH3Lly+v4sWLKyoqSpIUFRUlPz8/S5IpSQEBAUpISNDRo0ctff46RkqflDEyk82nzuPj4y2J5po1a9S2bVs1btxYJUuWVK1atWwcHQAAsAdGTpw/bIecR73EWqtWLS1evFi+vr6Ki4vTuHHjVKdOHR05ckTx8fFycnJKtbywcOHCio+Pl/Qgr/prkplyPuXcP/VJSEjQ7du35erqmuFn/TubJ5r58+fX77//rmLFimn9+vV6770HG2qbzWZ+fhIAADz1HjVN/jBNmza1/Lly5cqqVauWSpQooRUrVmRqAvik2HzqvFWrVurYsaNeeeUVXbp0yfIFHzx4UGXKlLFxdAAAwC5k0R3b8+XLp3LlyunUqVPy8vLS3bt3dfXqVas+58+fl5eXlyTJy8sr1VvoKZ8f18fNzS3Tk1mbJ5rTp09Xv379VLFiRUVERChPnjySpLi4OL311ls2jg4AAMB2bty4odOnT6tIkSKqUaOGcubMqU2bNlnOnzhxQrGxsfL395ck+fv76/Dhw7pw4X+/ihURESE3NzdVrFjR0uevY6T0SRkjM9n8rXMj8NY5kH3x1jmQfdnyrfN9MQmGjV3Txy3Nfd9++221aNFCJUqU0Llz5/Tuu+8qOjpax44dU6FChdSnTx+tW7dOixcvlpubm/r3f/Bv4s6dOyU92N6oatWq8vb21uTJkxUfH6/OnTurR48eVtsbVapUSX379lW3bt0UGRmpAQMGaO3atZm+vZHNK5pLlizR2rVrLZ+HDx+ufPny6cUXX9TZs2dtGBkAAMCT9ccff6hDhw7y9fVV27ZtVaBAAe3atUuFChWS9GAmuHnz5mrdurXq1q0rLy8vffvtt5brHR0dtWbNGjk6Osrf319vvPGGunTpovHjx1v6+Pj4aO3atYqIiFCVKlU0depUffLJJ5meZEpZoKLp6+urefPm6eWXX7a8bj99+nStWbNGOXLksPry0oqKJpB9UdEEsi9bVjT3nzGuolmjZNormtmNzd86//333y0v/axevVqtW7dWz549Vbt2bdWvX9+2wQEAACDDbD51nidPHl26dEmS9OOPP+qVV16RJLm4uOj27du2DA0AANiJLPrS+VPP5hXNV155RT169FC1atX066+/qlmzB7+Be/ToUZUoUcLG0QEAALtg7xmhQWxe0Zw7d678/f118eJFffPNNypQoICkBz+z1KFDBxtHBwAAgIyy+ctAf3f9+nV9+eWX+uSTT7R///4M/ToQLwMB2RcvAwHZly1fBjp49rphY1crkdewsbM6m1c0U2zbtk3BwcEqUqSIPvzwQ7388svatWuXrcMCAABABtl0jWZ8fLwWL16shQsXKiEhQW3btlViYqJWr15t2b0eAADAaCbWaBrCZhXNFi1ayNfXV4cOHdKMGTN07tw5zZ4921bhAAAAIJPZrKL5ww8/aMCAAerTp4/Kli1rqzAAAAB46dwgNqtobt++XdevX1eNGjVUq1YtzZkzR3/++aetwgEAAEAms1mi+cILL+g///mP4uLi1KtXLy1fvlze3t5KTk5WRESErl837u0vAAAAK+zYbgibv3WeO3dudevWTdu3b9fhw4c1dOhQTZo0SZ6enmrZsqWtwwMAAHbAZOD/s2c2TzT/ytfXV5MnT9Yff/yhL7/80tbhAAAA4F+w+U9QPoyjo6OCgoIUFBRk61AAAIAdYHsjY2SpiiYAAACyjyxZ0QQAAHiSKGgag4omAAAADEFFEwAAgJKmIahoAgAAwBBUNAEAgN2z9/0ujUJFEwAAAIagogkAAOwe+2gag0QTAADYPfJMYzB1DgAAAENQ0QQAAKCkaQgqmgAAADAEFU0AAGD32N7IGFQ0AQAAYAgqmgAAwO6xvZExqGgCAADAEFQ0AQCA3aOgaQwSTQAAADJNQzB1DgAAAENQ0QQAAHaP7Y2MQUUTAAAAhqCiCQAA7B7bGxmDiiYAAAAMQUUTAADYPQqaxqCiCQAAAENQ0QQAAKCkaQgSTQAAYPfY3sgYTJ0DAADAEFQ0AQCA3WN7I2NQ0QQAAMgiwsPD9dxzzylv3rzy9PRUUFCQTpw4YdWnfv36MplMVkfv3r2t+sTGxiowMFC5cuWSp6enhg0bpvv371v12bJli6pXry5nZ2eVKVNGixcvzvTnIdEEAAB2z2TgkR5bt25V3759tWvXLkVEROjevXtq3Lixbt68adXvzTffVFxcnOWYPHmy5VxSUpICAwN19+5d7dy5U0uWLNHixYs1ZswYS5+YmBgFBgaqQYMGio6O1qBBg9SjRw9t2LAhnRH/M5PZbDZn6ohZwM272e6RAPyfgrX62zoEAAa5fXCOze595s87ho1dsqBLhq+9ePGiPD09tXXrVtWtW1fSg4pm1apVNWPGjIde88MPP6h58+Y6d+6cChcuLEmaP3++QkNDdfHiRTk5OSk0NFRr167VkSNHLNe1b99eV69e1fr16zMc799R0QQAADCwpJmYmKiEhASrIzExMU1hXbt2TZLk4eFh1b506VIVLFhQlSpVUlhYmG7dumU5FxUVJT8/P0uSKUkBAQFKSEjQ0aNHLX0aNWpkNWZAQICioqLSFFdakWgCAAAYKDw8XO7u7lZHeHj4Y69LTk7WoEGDVLt2bVWqVMnS3rFjR33xxRfavHmzwsLC9Pnnn+uNN96wnI+Pj7dKMiVZPsfHx/9jn4SEBN2+fTvDz/p3vHUOAADsnpH7aIaFhWnIkCFWbc7Ozo+9rm/fvjpy5Ii2b99u1d6zZ0/Ln/38/FSkSBE1bNhQp0+fVunSpTMn6ExCogkAAOyekdsbOTs7pymx/Kt+/fppzZo12rZtm4oWLfqPfWvVqiVJOnXqlEqXLi0vLy/t2bPHqs/58+clSV5eXpb/mdL21z5ubm5ydXVNV6z/hKlzAACALMJsNqtfv35atWqVIiMj5ePj89hroqOjJUlFihSRJPn7++vw4cO6cOGCpU9ERITc3NxUsWJFS59NmzZZjRMRESF/f/9MepIHSDQBAIDdyyrbG/Xt21dffPGFli1bprx58yo+Pl7x8fGWdZOnT5/WhAkTtH//fp05c0bfffedunTporp166py5cqSpMaNG6tixYrq3Lmzfv75Z23YsEGjRo1S3759LZXV3r1767ffftPw4cN1/PhxffTRR1qxYoUGDx6csS/wEdjeCMBThe2NgOzLltsb/X45bW+BZ0Qxj7RPm5seMYe/aNEihYSE6Pfff9cbb7yhI0eO6ObNmypWrJhee+01jRo1Sm5ubpb+Z8+eVZ8+fbRlyxblzp1bwcHBmjRpknLk+N+qyS1btmjw4ME6duyYihYtqtGjRyskJCTDz/nQ5yHRBPA0IdEEsi9bJpp/XDEu0SyaP33rM7MTps4BAABgCN46BwAAMHB7I3tGRRMAAACGoKIJAADsnpH7aNozEk0AAGD3yDONwdQ5AAAADEFFEwAA2D2mzo1BRRMAAACGoKIJAADsnolVmoagogkAAABDUNEEAACgoGkIKpoAAAAwBBVNAABg9yhoGoNEEwAA2D22NzIGU+cAAAAwBBVNAABg99jeyBhUNAEAAGAIKpoAAAAUNA1BRRMAAACGoKIJAADsHgVNY1DRBAAAgCGoaAIAALvHPprGINEEAAB2j+2NjMHUOQAAAAxBRRMAANg9ps6NQUUTAAAAhiDRBAAAgCFINAEAAGAI1mgCAAC7xxpNY1DRBAAAgCGoaAIAALvHPprGINEEAAB2j6lzYzB1DgAAAENQ0QQAAHaPgqYxqGgCAADAEFQ0AQAAKGkagoomAAAADEFFEwAA2D22NzIGFU0AAAAYgoomAACwe+yjaQwqmgAAADAEFU0AAGD3KGgag0QTAACATNMQTJ0DAADAEFQ0AQCA3WN7I2NQ0QQAAIAhqGgCAAC7x/ZGxqCiCQAAAEOYzGaz2dZBABmVmJio8PBwhYWFydnZ2dbhAMhE/P0Gnn4kmniqJSQkyN3dXdeuXZObm5utwwGQifj7DTz9mDoHAACAIUg0AQAAYAgSTQAAABiCRBNPNWdnZ7377ru8KABkQ/z9Bp5+vAwEAAAAQ1DRBAAAgCFINAEAAGAIEk0AAAAYgkQTyAQmk0mrV6+2dRgA/iYkJERBQUG2DgOwWySaSJOQkBCZTCZNmjTJqn316tUymUw2iiprOnPmjEwmk6Kjo20dCvDEREVFydHRUYGBgbYO5R/Vr19fgwYNsnUYgN0g0USaubi46IMPPtCVK1dsHQqALGbhwoXq37+/tm3bpnPnztk6HABZBIkm0qxRo0by8vJSeHj4I/t88803evbZZ+Xs7KySJUtq6tSpVudLliypiRMnqlu3bsqbN6+KFy+uBQsW/ON9k5KS1L17d/n4+MjV1VW+vr6aOXOmVZ8tW7bo+eefV+7cuZUvXz7Vrl1bZ8+elST9/PPPatCggfLmzSs3NzfVqFFD+/btkyRdunRJHTp00DPPPKNcuXLJz89PX375ZaqYZ8yYYdVWtWpVjR079qHx+vj4SJKqVasmk8mk+vXr/+PzAU+7Gzdu6KuvvlKfPn0UGBioxYsXW85duXJFnTp1UqFCheTq6qqyZctq0aJFkqS7d++qX79+KlKkiFxcXFSiRAmrf1+mTZsmPz8/5c6dW8WKFdNbb72lGzduWM6PHTtWVatWtYplxowZKlmy5EPjDAkJ0datWzVz5kyZTCaZTCadOXMms74GAA9Book0c3R01MSJEzV79mz98ccfqc7v379fbdu2Vfv27XX48GGNHTtWo0ePtvr/dCRp6tSpqlmzpg4ePKi33npLffr00YkTJx553+TkZBUtWlQrV67UsWPHNGbMGL3zzjtasWKFJOn+/fsKCgpSvXr1dOjQIUVFRalnz56WKf1OnTqpaNGi2rt3r/bv368RI0YoZ86ckqQ7d+6oRo0aWrt2rY4cOaKePXuqc+fO2rNnT4a/p5RrN27cqLi4OH377bcZHgt4GqxYsULly5eXr6+v3njjDX366adK2aJ59OjROnbsmH744Qf98ssvmjdvngoWLChJmjVrlr777jutWLFCJ06c0NKlS62SRAcHB82aNUtHjx7VkiVLFBkZqeHDh2c4zpkzZ8rf319vvvmm4uLiFBcXp2LFiv2rZwfwz3LYOgA8XV577TVVrVpV7777rhYuXGh1btq0aWrYsKFGjx4tSSpXrpyOHTumKVOmKCQkxNKvWbNmeuuttyRJoaGhmj59ujZv3ixfX9+H3jNnzpwaN26c5bOPj4+ioqK0YsUKtW3bVgkJCbp27ZqaN2+u0qVLS5IqVKhg6R8bG6thw4apfPnykqSyZctazj3zzDN6++23LZ/79++vDRs2aMWKFXr++ecz8hWpUKFCkqQCBQrIy8srQ2MAT5OFCxfqjTfekCQ1adJE165d09atW1W/fn3FxsaqWrVqqlmzpiRZJZKxsbEqW7asXnrpJZlMJpUoUcJq3L+upSxZsqTee+899e7dWx999FGG4nR3d5eTk5Ny5crF303gCaGiiXT74IMPtGTJEv3yyy9W7b/88otq165t1Va7dm2dPHlSSUlJlrbKlStb/mwymeTl5aULFy5Ikpo2bao8efIoT548evbZZy395s6dqxo1aqhQoULKkyePFixYoNjYWEmSh4eHQkJCFBAQoBYtWmjmzJmKi4uzXDtkyBD16NFDjRo10qRJk3T69GnLuaSkJE2YMEF+fn7y8PBQnjx5tGHDBsvYAP7ZiRMntGfPHnXo0EGSlCNHDrVr187yH6J9+vTR8uXLVbVqVQ0fPlw7d+60XBsSEqLo6Gj5+vpqwIAB+vHHH63G3rhxoxo2bKhnnnlGefPmVefOnXXp0iXdunXryT0ggH+FRBPpVrduXQUEBCgsLCxD16dMW6cwmUxKTk6WJH3yySeKjo5WdHS01q1bJ0lavny53n77bXXv3l0//vijoqOj1bVrV929e9cyxqJFixQVFaUXX3xRX331lcqVK6ddu3ZJerCO6+jRowoMDFRkZKQqVqyoVatWSZKmTJmimTNnKjQ0VJs3b1Z0dLQCAgKsxnZwcNDff6n13r17GXp2ILtZuHCh7t+/L29vb+XIkUM5cuTQvHnz9M033+jatWtq2rSpzp49q8GDB+vcuXNq2LChZRahevXqiomJ0YQJE3T79m21bdtWbdq0kfRg94bmzZurcuXK+uabb7R//37NnTtXkix/P/m7CWR9TJ0jQyZNmqSqVataTXdXqFBBO3bssOq3Y8cOlStXTo6Ojmka95lnnknVtmPHDr344ouW6XZJVlXJFNWqVVO1atUUFhYmf39/LVu2TC+88IKkB9P45cqV0+DBg9WhQwctWrRIr732mnbs2KFXX33VMu2XnJysX3/9VRUrVrSMW6hQIasKaUJCgmJiYh75DE5OTpJkVcUFsqP79+/rs88+09SpU9W4cWOrc0FBQfryyy/Vu3dvFSpUSMHBwQoODladOnU0bNgwffjhh5IkNzc3tWvXTu3atVObNm3UpEkTXb58Wfv371dycrKmTp0qB4cHNZGUddkpChUqpPj4eJnNZsua7MdtK+bk5MTfTeAJItFEhvj5+alTp06aNWuWpW3o0KF67rnnNGHCBLVr105RUVGaM2dOhtdTpShbtqw+++wzbdiwQT4+Pvr888+1d+9ey9vdMTExWrBggVq2bClvb2+dOHFCJ0+eVJcuXXT79m0NGzZMbdq0kY+Pj/744w/t3btXrVu3toz99ddfa+fOncqfP7+mTZum8+fPWyWaL7/8shYvXqwWLVooX758GjNmzD8mzp6ennJ1ddX69etVtGhRubi4yN3d/V99B0BWtGbNGl25ckXdu3dP9X/jrVu31sKFC3Xu3DnVqFFDzz77rBITE7VmzRrLGupp06apSJEiqlatmhwcHLRy5Up5eXkpX758KlOmjO7du6fZs2erRYsW2rFjh+bPn291j/r16+vixYuaPHmy2rRpo/Xr1+uHH36Qm5vbI2MuWbKkdu/erTNnzihPnjzy8PCwJLIAMh9/u5Bh48ePt0x5Sw+mwVasWKHly5erUqVKGjNmjMaPH2/1IlBG9OrVS61atVK7du1Uq1YtXbp0yaq6mStXLh0/flytW7dWuXLl1LNnT/Xt21e9evWSo6OjLl26pC5duqhcuXJq27atmjZtanm5aNSoUapevboCAgJUv359eXl5pfoVkbCwMNWrV0/NmzdXYGCggoKCLC8dPUyOHDk0a9Ysffzxx/L29tarr776r54fyKoWLlyoRo0aPfQ/pFq3bq19+/YpR44cCgsLU+XKlVW3bl05Ojpq+fLlkqS8efNq8uTJqlmzpp577jmdOXNG69atk4ODg6pUqaJp06bpgw8+UKVKlbR06dJUW6tVqFBBH330kebOnasqVapoz549Vi/3Pczbb78tR0dHVaxYUYUKFWI9NmAwk/nvC1wAAACATEBFEwAAAIYg0QQAAIAhSDQBAABgCBJNAAAAGIJEEwAAAIYg0QQAAIAhSDQBAABgCBJNAAAAGIJEE0CWFRISYvVLTfXr19egQYOeeBxbtmyRyWTS1atXn/i9AeBpRqIJIN1CQkJkMplkMpnk5OSkMmXKaPz48bp//76h9/322281YcKENPUlOQQA28th6wAAPJ2aNGmiRYsWKTExUevWrVPfvn2VM2dOhYWFWfW7e/eunJycMuWeHh4emTIOAODJoKIJIEOcnZ3l5eWlEiVKqE+fPmrUqJG+++47y3T3+++/L29vb/n6+kqSfv/9d7Vt21b58uWTh4eHXn31VZ05c8YyXlJSkoYMGaJ8+fKpQIECGj58uMxms9U9/z51npiYqNDQUBUrVkzOzs4qU6aMFi5cqDNnzqhBgwaSpPz588tkMikkJESSlJycrPDwcPn4+MjV1VVVqlTR119/bXWfdevWqVy5cnJ1dVWDBg2s4gQApB2JJoBM4erqqrt370qSNm3apBMnTigiIkJr1qzRvXv3FBAQoLx58+qnn37Sjh07lCdPHjVp0sRyzdSpU7V48WJ9+umn2r59uy5fvqxVq1b94z27dOmiL7/8UrNmzdIvv/yijz/+WHny5FGxYsX0zTffSJJOnDihuLg4zZw5U5IUHh6uzz77TPPnz9fRo0c1ePBgvfHGG9q6daukBwlxq1at1KJFC0VHR6tHjx4aMWKEUV8bAGRrTJ0D+FfMZrM2bdqkDRs2qH///rp48aJy586tTz75xDJl/sUXXyg5OVmffPKJTCaTJGnRokXKly+ftmzZosaNG2vGjBkKCwtTq1atJEnz58/Xhg0bHnnfX3/9VStWrFBERIQaNWokSSpVqpTlfMo0u6enp/LlyyfpQQV04sSJ2rhxo/z9/S3XbN++XR9//LHq1aunefPmqXTp0po6daokydfXV4cPH9YHH3yQid8aANgHEk0AGbJmzRrlyZNH9+7dU3Jysjp27KixY8eqb9++8vPzs1qX+fPPP+vUqVPKmzev1Rh37tzR6dOnde3aNcXFxalWrVqWczly5FDNmjVTTZ+niI6OlqOjo+rVq5fmmE+dOqVbt27plVdesWq/e/euqlWrJkn65ZdfrOKQZElKAQDpQ6IJIEMaNGigefPmycnJSd7e3sqR43//nOTOnduq740bN1SjRg0tXbo01TiFChXK0P1dXV3Tfc2NGzckSWvXrtUzzzxjdc7Z2TlDcQAAHo1EE0CG5M6dW2XKlElT3+rVq+urr76Sp6en3NzcHtqnSJEi2r17t+rWrStJun//vvbv36/q1as/tL+fn5+Sk5O1detWy9T5X6VUVJOSkixtFStWlLOzs2JjYx9ZCa1QoYK+++47q7Zdu3Y9/iEBAKnwMhAAw3Xq1EkFCxbUq6++qp9++kkxMTHasmWLBgwYoD/++EOSNHDgQE2aNEmrV6/W8ePH9dZbb/3jHpglS5ZUcHCwunXrptWrV1vGXLFihSSpRIkSMplMWrNmjS5evKgbN24ob968evvttzV48GAtWbJEp0+f1oEDBzR79mwtWbJEktS7d2+dPHlSw4YN04kTJ7Rs2TItXrzY6K8IALIlEk0AhsuVK5e2bdum4sWLq1WrVqpQoYK6d++uO3fuWCqcQ4cOVefOnRUcHCx/f3/lzZtXr7322j+OO2/ePLVp00ZvvfWWypcvrzfffFM3b96UJD3zzDMaN26cRowYocKFC6tfv36SpAkTJmj06NEKDw9XhQoV1KRJE61du1Y+Pj6SpOLFi+ubb77R6tWrVaVKFc2fP18TJ0408NsBgOzLZH7USnsAAADgX6CiCQAAAEOQaAIAAMAQJJoAAAAwBIkmAAAADEGiCQAAAEOQaAIAAMAQJJoAAAAwBIkmAAAADEGiCQAAAEOQaAIAAMAQJJoAAAAwxP8H7J4cs387ROsAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 저장\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/논문주제/violence_detection_model.pth')"
      ],
      "metadata": {
        "id": "gXDsSdA0KcY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from alphapose.utils.config import update_config\n",
        "from alphapose.models import builder\n",
        "from alphapose.utils.transforms import get_func_heatmap_to_coord\n",
        "from scipy.spatial.distance import cdist\n",
        "import os\n",
        "from ultralytics import YOLO\n",
        "\n",
        "class GetKeypoint:\n",
        "    NOSE = 0\n",
        "    LEFT_EYE = 1\n",
        "    RIGHT_EYE = 2\n",
        "    LEFT_EAR = 3\n",
        "    RIGHT_EAR = 4\n",
        "    LEFT_SHOULDER = 5\n",
        "    RIGHT_SHOULDER = 6\n",
        "    LEFT_ELBOW = 7\n",
        "    RIGHT_ELBOW = 8\n",
        "    LEFT_WRIST = 9\n",
        "    RIGHT_WRIST = 10\n",
        "    LEFT_HIP = 11\n",
        "    RIGHT_HIP = 12\n",
        "    LEFT_KNEE = 13\n",
        "    RIGHT_KNEE = 14\n",
        "    LEFT_ANKLE = 15\n",
        "    RIGHT_ANKLE = 16\n",
        "\n",
        "get_keypoint = GetKeypoint()\n",
        "\n",
        "# 각 키포인트에 고유의 색상을 지정\n",
        "keypoint_colors = [\n",
        "    (255, 0, 0),  # NOSE\n",
        "    (255, 165, 0),  # LEFT_EYE\n",
        "    (255, 255, 0),  # RIGHT_EYE\n",
        "    (0, 255, 0),  # LEFT_EAR\n",
        "    (0, 255, 255),  # RIGHT_EAR\n",
        "    (0, 0, 255),  # LEFT_SHOULDER\n",
        "    (75, 0, 130),  # RIGHT_SHOULDER\n",
        "    (238, 130, 238),  # LEFT_ELBOW\n",
        "    (255, 105, 180),  # RIGHT_ELBOW\n",
        "    (255, 20, 147),  # LEFT_WRIST\n",
        "    (128, 0, 128),  # RIGHT_WRIST\n",
        "    (0, 128, 0),  # LEFT_HIP\n",
        "    (0, 128, 255),  # RIGHT_HIP\n",
        "    (128, 0, 0),  # LEFT_KNEE\n",
        "    (128, 0, 128),  # RIGHT_KNEE\n",
        "    (255, 69, 0),  # LEFT_ANKLE\n",
        "    (255, 140, 0)   # RIGHT_ANKLE\n",
        "]\n",
        "\n",
        "# 관절을 연결하는 라인 정의\n",
        "limb_pairs = [\n",
        "    (get_keypoint.NOSE, get_keypoint.LEFT_EYE), (get_keypoint.NOSE, get_keypoint.RIGHT_EYE),\n",
        "    (get_keypoint.LEFT_EYE, get_keypoint.LEFT_EAR), (get_keypoint.RIGHT_EYE, get_keypoint.RIGHT_EAR),\n",
        "    (get_keypoint.NOSE, get_keypoint.LEFT_SHOULDER), (get_keypoint.NOSE, get_keypoint.RIGHT_SHOULDER),\n",
        "    (get_keypoint.LEFT_SHOULDER, get_keypoint.LEFT_ELBOW), (get_keypoint.RIGHT_SHOULDER, get_keypoint.RIGHT_ELBOW),\n",
        "    (get_keypoint.LEFT_ELBOW, get_keypoint.LEFT_WRIST), (get_keypoint.RIGHT_ELBOW, get_keypoint.RIGHT_WRIST),\n",
        "    (get_keypoint.LEFT_SHOULDER, get_keypoint.LEFT_HIP), (get_keypoint.RIGHT_SHOULDER, get_keypoint.RIGHT_HIP),\n",
        "    (get_keypoint.LEFT_HIP, get_keypoint.LEFT_KNEE), (get_keypoint.RIGHT_HIP, get_keypoint.RIGHT_KNEE),\n",
        "    (get_keypoint.LEFT_KNEE, get_keypoint.LEFT_ANKLE), (get_keypoint.RIGHT_KNEE, get_keypoint.RIGHT_ANKLE)\n",
        "]\n",
        "# AlphaPose 모델 및 YOLO 설정\n",
        "def load_models():\n",
        "    # AlphaPose 설정\n",
        "    pretrained_model_path = '/content/drive/MyDrive/논문주제/Final_project/pretrained_models/fast_res50_256x192.pth'\n",
        "    cfg_file = '/content/AlphaPose/configs/coco/resnet/256x192_res50_lr1e-3_1x.yaml'\n",
        "\n",
        "    cfg = update_config(cfg_file)\n",
        "    cfg['checkpoint'] = pretrained_model_path\n",
        "\n",
        "    # AlphaPose 모델 빌드\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    pose_model = builder.build_sppe(cfg.MODEL, preset_cfg=cfg.DATA_PRESET)\n",
        "    pose_model.load_state_dict(torch.load(cfg['checkpoint'], map_location=device))\n",
        "    pose_model = torch.nn.DataParallel(pose_model).to(device)\n",
        "    pose_model.eval()\n",
        "\n",
        "    return pose_model, cfg, device\n",
        "\n",
        "# 관절과 관절을 연결하는 라인(임의로 설정한 링크들)\n",
        "limb_pairs = [\n",
        "    (get_keypoint.LEFT_SHOULDER, get_keypoint.LEFT_ELBOW),\n",
        "    (get_keypoint.LEFT_ELBOW, get_keypoint.LEFT_WRIST),\n",
        "    (get_keypoint.RIGHT_SHOULDER, get_keypoint.RIGHT_ELBOW),\n",
        "    (get_keypoint.RIGHT_ELBOW, get_keypoint.RIGHT_WRIST),\n",
        "    (get_keypoint.LEFT_HIP, get_keypoint.LEFT_KNEE),\n",
        "    (get_keypoint.LEFT_KNEE, get_keypoint.LEFT_ANKLE),\n",
        "    (get_keypoint.RIGHT_HIP, get_keypoint.RIGHT_KNEE),\n",
        "    (get_keypoint.RIGHT_KNEE, get_keypoint.RIGHT_ANKLE),\n",
        "    (get_keypoint.LEFT_SHOULDER, get_keypoint.RIGHT_SHOULDER),\n",
        "    (get_keypoint.LEFT_HIP, get_keypoint.RIGHT_HIP),\n",
        "]\n",
        "\n",
        "# 키포인트 시각화 함수\n",
        "# 키포인트 및 스켈레톤을 그리는 함수\n",
        "def draw_keypoints_and_skeleton(frame, keypoints_list, boxes):\n",
        "    vis_img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "    draw = ImageDraw.Draw(vis_img)\n",
        "\n",
        "    # 키포인트마다 색상 다르게 그리기\n",
        "    for i, keypoints in enumerate(keypoints_list):\n",
        "        for j in range(0, len(keypoints), 2):\n",
        "            keypoint_x, keypoint_y = int(keypoints[j]), int(keypoints[j + 1])\n",
        "            if keypoint_x > 0 and keypoint_y > 0:\n",
        "                draw.ellipse([(keypoint_x - 3, keypoint_y - 3), (keypoint_x + 3, keypoint_y + 3)], fill=keypoint_colors[j // 2])\n",
        "\n",
        "        # 관절 연결 그리기\n",
        "        for (start, end) in limb_pairs:\n",
        "            start_x, start_y = keypoints[start * 2], keypoints[start * 2 + 1]\n",
        "            end_x, end_y = keypoints[end * 2], keypoints[end * 2 + 1]\n",
        "            if start_x > 0 and start_y > 0 and end_x > 0 and end_y > 0:\n",
        "                draw.line([(start_x, start_y), (end_x, end_y)], fill=(255, 255, 0), width=2)\n",
        "\n",
        "    return cv2.cvtColor(np.array(vis_img), cv2.COLOR_RGB2BGR)\n",
        "\n",
        "# 프레임 처리 함수\n",
        "def process_video_frame(frame, yolo_model, pose_model, cfg, device):\n",
        "    results = yolo_model.predict(frame, save=False, classes=[0])\n",
        "    human_detections = [d for d in results[0].boxes.data.cpu().numpy() if int(d[-1]) == 0]\n",
        "\n",
        "    if not human_detections:\n",
        "\n",
        "# 두 사람의 키포인트와 박스를 찾아서 리턴\n",
        "def find_closest_two_persons(keypoints_list, boxes):\n",
        "    centers = []\n",
        "    for keypoints in keypoints_list:\n",
        "        if len(keypoints) == 34:\n",
        "            left_hip_x, left_hip_y = keypoints[get_keypoint.LEFT_HIP * 2], keypoints[get_keypoint.LEFT_HIP * 2 + 1]\n",
        "            right_hip_x, right_hip_y = keypoints[get_keypoint.RIGHT_HIP * 2], keypoints[get_keypoint.RIGHT_HIP * 2 + 1]\n",
        "            if left_hip_x > 0 and right_hip_x > 0 and left_hip_y > 0 and right_hip_y > 0:\n",
        "                center_x = (left_hip_x + right_hip_x) / 2\n",
        "                center_y = (left_hip_y + right_hip_y) / 2\n",
        "                centers.append([center_x, center_y])\n",
        "            else:\n",
        "                centers.append([0, 0])\n",
        "        else:\n",
        "            centers.append([0, 0])\n",
        "\n",
        "    if len(centers) == 0:\n",
        "        return None, None, None, None\n",
        "\n",
        "    else: len(centers) >= 2:\n",
        "        distances = cdist(centers, centers, metric='euclidean')\n",
        "        np.fill_diagonal(distances, np.inf)\n",
        "        person_1_idx, person_2_idx = np.unravel_index(np.argmin(distances), distances.shape)\n",
        "        return keypoints_list[person_1_idx], keypoints_list[person_2_idx], boxes[person_1_idx], boxes[person_2_idx]\n",
        "    else:\n",
        "        return None, None, None, None\n",
        "\n",
        "def pad_or_truncate_keypoints(keypoints, angles, distances, expected_size=84):\n",
        "    combined_data = keypoints + angles + distances\n",
        "    current_size = len(combined_data)\n",
        "    if current_size > expected_size:\n",
        "        combined_data = combined_data[:expected_size]\n",
        "    elif current_size < expected_size:\n",
        "        combined_data += [0] * (expected_size - current_size)\n",
        "    return combined_data\n",
        "\n",
        "def extract_keypoints_and_boxes(yolo_model, pose_model, frame, cfg, device):\n",
        "    results = yolo_model.predict(frame, save=False, classes=[0])\n",
        "    human_detections = [d for d in results[0].boxes.data.cpu().numpy() if int(d[-1]) == 0]\n",
        "\n",
        "    keypoints_list = []\n",
        "    boxes = []\n",
        "    if not human_detections:\n",
        "        return keypoints_list, boxes\n",
        "\n",
        "    inps = []\n",
        "    for detection in human_detections:\n",
        "        x1, y1, x2, y2 = map(int, detection[:4])\n",
        "        boxes.append([x1, y1, x2, y2])\n",
        "        inp = cv2.resize(frame[y1:y2, x1:x2], (cfg.DATA_PRESET.IMAGE_SIZE[0], cfg.DATA_PRESET.IMAGE_SIZE[1]))\n",
        "        inps.append(inp)\n",
        "\n",
        "    inps = torch.stack([torch.from_numpy(np.array(inp)).permute(2, 0, 1).float() for inp in inps]).to(device)\n",
        "    with torch.no_grad():\n",
        "        hm = pose_model(inps)\n",
        "\n",
        "    for i, box in enumerate(boxes):\n",
        "        preds, _ = get_func_heatmap_to_coord(cfg)(hm[i], box)\n",
        "        keypoints_flatten = preds.flatten().tolist()\n",
        "        keypoints_list.append(keypoints_flatten)\n",
        "\n",
        "    return keypoints_list, boxes\n",
        "\n",
        "# 폭력 상황 시 \"Violence!\" 표시하는 함수\n",
        "def draw_keypoints_on_frame(frame, keypoints_1, keypoints_2, frame_num, save_dir, violence_detected, person_count, box_1=None, box_2=None):\n",
        "    vis_img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "    draw = ImageDraw.Draw(vis_img)\n",
        "\n",
        "    try:\n",
        "        font = ImageFont.truetype(\"arial.ttf\", 40)\n",
        "    except IOError:\n",
        "        font = ImageFont.load_default()\n",
        "\n",
        "    joint_color = (255, 0, 0)\n",
        "    limb_color = (255, 255, 0)\n",
        "\n",
        "    if person_count == 1:\n",
        "        print(f\"Frame {frame_num}: 한 명 감지됨. 이미지 저장만 진행합니다.\")\n",
        "    else:\n",
        "        if violence_detected:\n",
        "            draw.text((10, 10), \"Violence!\", fill=(255, 0, 0), font=font)\n",
        "            if box_1:\n",
        "                draw.rectangle(box_1, outline=\"red\", width=5)\n",
        "            if box_2:\n",
        "                draw.rectangle(box_2, outline=\"red\", width=5)\n",
        "            for keypoints in [keypoints_1, keypoints_2]:\n",
        "                if keypoints:\n",
        "                    for i in range(0, len(keypoints), 2):\n",
        "                        keypoint_x, keypoint_y = int(keypoints[i]), int(keypoints[i + 1])\n",
        "                        if keypoint_x > 0 and keypoint_y > 0:\n",
        "                            draw.ellipse([(keypoint_x - 4, keypoint_y - 4), (keypoint_x + 4, keypoint_y + 4)], fill=joint_color)\n",
        "\n",
        "    # 디렉터리가 존재하는지 확인하고 없으면 생성\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    save_path = f\"{save_dir}/frame_{frame_num}.png\"\n",
        "    vis_img.save(save_path)\n",
        "    print(f\"Frame {frame_num} saved to {save_path}\")\n",
        "\n",
        "# LSTM 모델 정의\n",
        "class LSTMModel(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, dropout=0.5):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm = torch.nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
        "        self.fc = torch.nn.Linear(hidden_size, output_size)\n",
        "        self.batch_norm = torch.nn.BatchNorm1d(hidden_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        out = out[:, -1, :]\n",
        "        out = self.batch_norm(out)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "# LSTM 모델 로드 함수\n",
        "def load_model(model_path, input_size, hidden_size, output_size, num_layers=1, dropout=0.5, device='cpu'):\n",
        "    model = LSTMModel(input_size, hidden_size, output_size, num_layers, dropout)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "# 비디오 프레임 예측 및 처리\n",
        "def predict_violence_in_video(video_path, model, yolo_model, pose_model, cfg, device, save_dir, frame_skip=30):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frame_count = 0\n",
        "    violence_frames = 0\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        if frame_count % frame_skip == 0 or violence_frames > 0:\n",
        "            try:\n",
        "                keypoints_list, boxes = extract_keypoints_and_boxes(yolo_model, pose_model, frame, cfg, device)\n",
        "                person_count = len(keypoints_list)\n",
        "\n",
        "                violence_detected = False\n",
        "                box_1, box_2 = None, None\n",
        "\n",
        "                if person_count >= 2:\n",
        "                    keypoint_data_1, keypoint_data_2, box_1, box_2 = find_closest_two_persons(keypoints_list, boxes)\n",
        "\n",
        "                    if keypoint_data_1 and keypoint_data_2:\n",
        "                        lstm_input_1 = pad_or_truncate_keypoints(keypoint_data_1, [], [])\n",
        "                        lstm_input = np.array(lstm_input_1).astype(np.float32)\n",
        "                        lstm_input = np.expand_dims(lstm_input, axis=0)\n",
        "                        lstm_input = torch.tensor(lstm_input, dtype=torch.float32).to(device)\n",
        "                        lstm_input = lstm_input.unsqueeze(1)\n",
        "\n",
        "                        with torch.no_grad():\n",
        "                            outputs = model(lstm_input)\n",
        "                            _, predicted = torch.max(outputs, 1)\n",
        "                            violence_detected = predicted.item() == 1\n",
        "\n",
        "                        if violence_detected:\n",
        "                            violence_frames = 5\n",
        "                        else:\n",
        "                            violence_frames = max(0, violence_frames - 1)\n",
        "\n",
        "                draw_keypoints_on_frame(frame, keypoint_data_1, keypoint_data_2, frame_count, save_dir, violence_detected, person_count, box_1, box_2)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing frame {frame_count}: {e}\")\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "# 모델 로드 및 비디오 처리\n",
        "input_size = 84\n",
        "hidden_size = 128\n",
        "output_size = 2\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model_path = '/content/drive/MyDrive/논문주제/violence_detection_model.pth'\n",
        "video_path = '/content/drive/MyDrive/논문주제/이상행동 CCTV 영상/01.폭행(assult)/14-4/14-4_cam01_assault02_place06_night_summer.mp4'\n",
        "save_dir = '/content/drive/MyDrive/논문주제/ending'\n",
        "if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "\n",
        "yolo_model = YOLO('yolov8n.pt')\n",
        "pose_model, cfg, device = load_models()\n",
        "model = load_model(model_path, input_size, hidden_size, output_size, device=device)\n",
        "\n",
        "predict_violence_in_video(video_path, model, yolo_model, pose_model, cfg, device, save_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ebAf2Y6lQpPv",
        "outputId": "7c189101-74ec-4fb6-8b00-880b25861fc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0: 384x640 (no detections), 7.9ms\n",
            "Speed: 2.3ms preprocess, 7.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Error processing frame 0: local variable 'keypoint_data_1' referenced before assignment\n",
            "\n",
            "0: 384x640 (no detections), 11.2ms\n",
            "Speed: 3.7ms preprocess, 11.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Error processing frame 30: local variable 'keypoint_data_1' referenced before assignment\n",
            "\n",
            "0: 384x640 (no detections), 11.3ms\n",
            "Speed: 3.7ms preprocess, 11.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Error processing frame 60: local variable 'keypoint_data_1' referenced before assignment\n",
            "\n",
            "0: 384x640 (no detections), 11.0ms\n",
            "Speed: 3.5ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Error processing frame 90: local variable 'keypoint_data_1' referenced before assignment\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 3.2ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Error processing frame 120: local variable 'keypoint_data_1' referenced before assignment\n",
            "\n",
            "0: 384x640 1 person, 10.3ms\n",
            "Speed: 3.7ms preprocess, 10.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Error processing frame 150: local variable 'keypoint_data_1' referenced before assignment\n",
            "\n",
            "0: 384x640 1 person, 8.6ms\n",
            "Speed: 3.5ms preprocess, 8.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Error processing frame 180: local variable 'keypoint_data_1' referenced before assignment\n",
            "\n",
            "0: 384x640 2 persons, 11.4ms\n",
            "Speed: 3.5ms preprocess, 11.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 210 saved to /content/drive/MyDrive/논문주제/ending/frame_210.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.8ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 211 saved to /content/drive/MyDrive/논문주제/ending/frame_211.png\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 212 saved to /content/drive/MyDrive/논문주제/ending/frame_212.png\n",
            "\n",
            "0: 384x640 2 persons, 7.1ms\n",
            "Speed: 2.7ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 213 saved to /content/drive/MyDrive/논문주제/ending/frame_213.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.8ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 214 saved to /content/drive/MyDrive/논문주제/ending/frame_214.png\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.7ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 215 saved to /content/drive/MyDrive/논문주제/ending/frame_215.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 3.0ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 216 saved to /content/drive/MyDrive/논문주제/ending/frame_216.png\n",
            "\n",
            "0: 384x640 2 persons, 10.8ms\n",
            "Speed: 3.4ms preprocess, 10.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 217 saved to /content/drive/MyDrive/논문주제/ending/frame_217.png\n",
            "\n",
            "0: 384x640 2 persons, 8.0ms\n",
            "Speed: 2.8ms preprocess, 8.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 218 saved to /content/drive/MyDrive/논문주제/ending/frame_218.png\n",
            "\n",
            "0: 384x640 2 persons, 9.9ms\n",
            "Speed: 3.5ms preprocess, 9.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 219 saved to /content/drive/MyDrive/논문주제/ending/frame_219.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 220 saved to /content/drive/MyDrive/논문주제/ending/frame_220.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 221 saved to /content/drive/MyDrive/논문주제/ending/frame_221.png\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 222 saved to /content/drive/MyDrive/논문주제/ending/frame_222.png\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 3.0ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 223 saved to /content/drive/MyDrive/논문주제/ending/frame_223.png\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.7ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 224 saved to /content/drive/MyDrive/논문주제/ending/frame_224.png\n",
            "\n",
            "0: 384x640 2 persons, 7.1ms\n",
            "Speed: 3.0ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 225 saved to /content/drive/MyDrive/논문주제/ending/frame_225.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.8ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 226 saved to /content/drive/MyDrive/논문주제/ending/frame_226.png\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.9ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 227 saved to /content/drive/MyDrive/논문주제/ending/frame_227.png\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.9ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 228 saved to /content/drive/MyDrive/논문주제/ending/frame_228.png\n",
            "\n",
            "0: 384x640 2 persons, 8.1ms\n",
            "Speed: 2.3ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 229 saved to /content/drive/MyDrive/논문주제/ending/frame_229.png\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 230 saved to /content/drive/MyDrive/논문주제/ending/frame_230.png\n",
            "\n",
            "0: 384x640 2 persons, 7.1ms\n",
            "Speed: 2.6ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 231 saved to /content/drive/MyDrive/논문주제/ending/frame_231.png\n",
            "\n",
            "0: 384x640 2 persons, 7.1ms\n",
            "Speed: 2.8ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 232 saved to /content/drive/MyDrive/논문주제/ending/frame_232.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.8ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 233 saved to /content/drive/MyDrive/논문주제/ending/frame_233.png\n",
            "\n",
            "0: 384x640 2 persons, 7.1ms\n",
            "Speed: 2.8ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 234 saved to /content/drive/MyDrive/논문주제/ending/frame_234.png\n",
            "\n",
            "0: 384x640 2 persons, 7.1ms\n",
            "Speed: 2.8ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 235 saved to /content/drive/MyDrive/논문주제/ending/frame_235.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 236 saved to /content/drive/MyDrive/논문주제/ending/frame_236.png\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.5ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 237 saved to /content/drive/MyDrive/논문주제/ending/frame_237.png\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.9ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 238 saved to /content/drive/MyDrive/논문주제/ending/frame_238.png\n",
            "\n",
            "0: 384x640 2 persons, 11.1ms\n",
            "Speed: 3.5ms preprocess, 11.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 239 saved to /content/drive/MyDrive/논문주제/ending/frame_239.png\n",
            "\n",
            "0: 384x640 2 persons, 7.9ms\n",
            "Speed: 2.5ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 240 saved to /content/drive/MyDrive/논문주제/ending/frame_240.png\n",
            "\n",
            "0: 384x640 2 persons, 8.0ms\n",
            "Speed: 2.8ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 241 saved to /content/drive/MyDrive/논문주제/ending/frame_241.png\n",
            "\n",
            "0: 384x640 2 persons, 8.1ms\n",
            "Speed: 2.7ms preprocess, 8.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 242 saved to /content/drive/MyDrive/논문주제/ending/frame_242.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 243 saved to /content/drive/MyDrive/논문주제/ending/frame_243.png\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.7ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 244 saved to /content/drive/MyDrive/논문주제/ending/frame_244.png\n",
            "\n",
            "0: 384x640 2 persons, 7.1ms\n",
            "Speed: 2.9ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 245 saved to /content/drive/MyDrive/논문주제/ending/frame_245.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.8ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 246 saved to /content/drive/MyDrive/논문주제/ending/frame_246.png\n",
            "\n",
            "0: 384x640 2 persons, 8.0ms\n",
            "Speed: 2.9ms preprocess, 8.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 247 saved to /content/drive/MyDrive/논문주제/ending/frame_247.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.6ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 248 saved to /content/drive/MyDrive/논문주제/ending/frame_248.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.5ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 249 saved to /content/drive/MyDrive/논문주제/ending/frame_249.png\n",
            "\n",
            "0: 384x640 2 persons, 8.6ms\n",
            "Speed: 2.4ms preprocess, 8.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 250 saved to /content/drive/MyDrive/논문주제/ending/frame_250.png\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.8ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 251 saved to /content/drive/MyDrive/논문주제/ending/frame_251.png\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.9ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 252 saved to /content/drive/MyDrive/논문주제/ending/frame_252.png\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.6ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 253 saved to /content/drive/MyDrive/논문주제/ending/frame_253.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 254 saved to /content/drive/MyDrive/논문주제/ending/frame_254.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 2.7ms preprocess, 7.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 255 saved to /content/drive/MyDrive/논문주제/ending/frame_255.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.6ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 256 saved to /content/drive/MyDrive/논문주제/ending/frame_256.png\n",
            "\n",
            "0: 384x640 2 persons, 11.3ms\n",
            "Speed: 3.5ms preprocess, 11.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 257 saved to /content/drive/MyDrive/논문주제/ending/frame_257.png\n",
            "\n",
            "0: 384x640 2 persons, 11.0ms\n",
            "Speed: 3.4ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 258 saved to /content/drive/MyDrive/논문주제/ending/frame_258.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.9ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 259 saved to /content/drive/MyDrive/논문주제/ending/frame_259.png\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.9ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 260 saved to /content/drive/MyDrive/논문주제/ending/frame_260.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.5ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 261 saved to /content/drive/MyDrive/논문주제/ending/frame_261.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 262 saved to /content/drive/MyDrive/논문주제/ending/frame_262.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 3.0ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 263 saved to /content/drive/MyDrive/논문주제/ending/frame_263.png\n",
            "\n",
            "0: 384x640 2 persons, 11.0ms\n",
            "Speed: 3.6ms preprocess, 11.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 264 saved to /content/drive/MyDrive/논문주제/ending/frame_264.png\n",
            "\n",
            "0: 384x640 2 persons, 11.1ms\n",
            "Speed: 3.5ms preprocess, 11.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 265 saved to /content/drive/MyDrive/논문주제/ending/frame_265.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.8ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 266 saved to /content/drive/MyDrive/논문주제/ending/frame_266.png\n",
            "\n",
            "0: 384x640 2 persons, 8.3ms\n",
            "Speed: 3.5ms preprocess, 8.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 267 saved to /content/drive/MyDrive/논문주제/ending/frame_267.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 2.9ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 268 saved to /content/drive/MyDrive/논문주제/ending/frame_268.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 269 saved to /content/drive/MyDrive/논문주제/ending/frame_269.png\n",
            "\n",
            "0: 384x640 2 persons, 10.4ms\n",
            "Speed: 3.4ms preprocess, 10.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 270 saved to /content/drive/MyDrive/논문주제/ending/frame_270.png\n",
            "\n",
            "0: 384x640 2 persons, 8.3ms\n",
            "Speed: 2.7ms preprocess, 8.3ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 271 saved to /content/drive/MyDrive/논문주제/ending/frame_271.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.7ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 272 saved to /content/drive/MyDrive/논문주제/ending/frame_272.png\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 273 saved to /content/drive/MyDrive/논문주제/ending/frame_273.png\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.9ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 274 saved to /content/drive/MyDrive/논문주제/ending/frame_274.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 2.9ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 275 saved to /content/drive/MyDrive/논문주제/ending/frame_275.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.7ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 276 saved to /content/drive/MyDrive/논문주제/ending/frame_276.png\n",
            "\n",
            "0: 384x640 2 persons, 7.1ms\n",
            "Speed: 2.6ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 277 saved to /content/drive/MyDrive/논문주제/ending/frame_277.png\n",
            "\n",
            "0: 384x640 2 persons, 11.5ms\n",
            "Speed: 3.8ms preprocess, 11.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 278 saved to /content/drive/MyDrive/논문주제/ending/frame_278.png\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 279 saved to /content/drive/MyDrive/논문주제/ending/frame_279.png\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.8ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 280 saved to /content/drive/MyDrive/논문주제/ending/frame_280.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.8ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 281 saved to /content/drive/MyDrive/논문주제/ending/frame_281.png\n",
            "\n",
            "0: 384x640 2 persons, 7.1ms\n",
            "Speed: 2.5ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 282 saved to /content/drive/MyDrive/논문주제/ending/frame_282.png\n",
            "\n",
            "0: 384x640 2 persons, 9.2ms\n",
            "Speed: 3.5ms preprocess, 9.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 283 saved to /content/drive/MyDrive/논문주제/ending/frame_283.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.5ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 284 saved to /content/drive/MyDrive/논문주제/ending/frame_284.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 3.0ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 285 saved to /content/drive/MyDrive/논문주제/ending/frame_285.png\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.5ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 286 saved to /content/drive/MyDrive/논문주제/ending/frame_286.png\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.8ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 287 saved to /content/drive/MyDrive/논문주제/ending/frame_287.png\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 288 saved to /content/drive/MyDrive/논문주제/ending/frame_288.png\n",
            "\n",
            "0: 384x640 2 persons, 11.0ms\n",
            "Speed: 3.4ms preprocess, 11.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 289 saved to /content/drive/MyDrive/논문주제/ending/frame_289.png\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.7ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 290 saved to /content/drive/MyDrive/논문주제/ending/frame_290.png\n",
            "\n",
            "0: 384x640 2 persons, 8.5ms\n",
            "Speed: 2.4ms preprocess, 8.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 291 saved to /content/drive/MyDrive/논문주제/ending/frame_291.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.8ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 292 saved to /content/drive/MyDrive/논문주제/ending/frame_292.png\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 293 saved to /content/drive/MyDrive/논문주제/ending/frame_293.png\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 3.0ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 294 saved to /content/drive/MyDrive/논문주제/ending/frame_294.png\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.8ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 295 saved to /content/drive/MyDrive/논문주제/ending/frame_295.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.6ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 296 saved to /content/drive/MyDrive/논문주제/ending/frame_296.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 3.0ms preprocess, 7.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 297 saved to /content/drive/MyDrive/논문주제/ending/frame_297.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 3.0ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 298 saved to /content/drive/MyDrive/논문주제/ending/frame_298.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.6ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 299 saved to /content/drive/MyDrive/논문주제/ending/frame_299.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 300 saved to /content/drive/MyDrive/논문주제/ending/frame_300.png\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 301 saved to /content/drive/MyDrive/논문주제/ending/frame_301.png\n",
            "\n",
            "0: 384x640 2 persons, 7.1ms\n",
            "Speed: 2.7ms preprocess, 7.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 302 saved to /content/drive/MyDrive/논문주제/ending/frame_302.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 2.6ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 303 saved to /content/drive/MyDrive/논문주제/ending/frame_303.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 304 saved to /content/drive/MyDrive/논문주제/ending/frame_304.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.8ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 305 saved to /content/drive/MyDrive/논문주제/ending/frame_305.png\n",
            "\n",
            "0: 384x640 2 persons, 7.1ms\n",
            "Speed: 2.9ms preprocess, 7.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 306 saved to /content/drive/MyDrive/논문주제/ending/frame_306.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 3.1ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 307 saved to /content/drive/MyDrive/논문주제/ending/frame_307.png\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.9ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 308 saved to /content/drive/MyDrive/논문주제/ending/frame_308.png\n",
            "\n",
            "0: 384x640 2 persons, 11.7ms\n",
            "Speed: 3.4ms preprocess, 11.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 309 saved to /content/drive/MyDrive/논문주제/ending/frame_309.png\n",
            "\n",
            "0: 384x640 2 persons, 10.7ms\n",
            "Speed: 3.6ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 310 saved to /content/drive/MyDrive/논문주제/ending/frame_310.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.6ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 311 saved to /content/drive/MyDrive/논문주제/ending/frame_311.png\n",
            "\n",
            "0: 384x640 2 persons, 8.2ms\n",
            "Speed: 2.6ms preprocess, 8.2ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 312 saved to /content/drive/MyDrive/논문주제/ending/frame_312.png\n",
            "\n",
            "0: 384x640 2 persons, 7.1ms\n",
            "Speed: 2.8ms preprocess, 7.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 313 saved to /content/drive/MyDrive/논문주제/ending/frame_313.png\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.8ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 314 saved to /content/drive/MyDrive/논문주제/ending/frame_314.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.8ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 315 saved to /content/drive/MyDrive/논문주제/ending/frame_315.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.9ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 316 saved to /content/drive/MyDrive/논문주제/ending/frame_316.png\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.5ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 317 saved to /content/drive/MyDrive/논문주제/ending/frame_317.png\n",
            "\n",
            "0: 384x640 2 persons, 7.0ms\n",
            "Speed: 2.8ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 318 saved to /content/drive/MyDrive/논문주제/ending/frame_318.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.6ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 319 saved to /content/drive/MyDrive/논문주제/ending/frame_319.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.9ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 320 saved to /content/drive/MyDrive/논문주제/ending/frame_320.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 3.1ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 321 saved to /content/drive/MyDrive/논문주제/ending/frame_321.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.9ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 322 saved to /content/drive/MyDrive/논문주제/ending/frame_322.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 3.0ms preprocess, 7.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 323 saved to /content/drive/MyDrive/논문주제/ending/frame_323.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 3.1ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 324 saved to /content/drive/MyDrive/논문주제/ending/frame_324.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.6ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 325 saved to /content/drive/MyDrive/논문주제/ending/frame_325.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.8ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 326 saved to /content/drive/MyDrive/논문주제/ending/frame_326.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 3.3ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 327 saved to /content/drive/MyDrive/논문주제/ending/frame_327.png\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.5ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 328 saved to /content/drive/MyDrive/논문주제/ending/frame_328.png\n",
            "\n",
            "0: 384x640 2 persons, 7.1ms\n",
            "Speed: 2.8ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 329 saved to /content/drive/MyDrive/논문주제/ending/frame_329.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 3.0ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 330 saved to /content/drive/MyDrive/논문주제/ending/frame_330.png\n",
            "\n",
            "0: 384x640 2 persons, 10.1ms\n",
            "Speed: 3.5ms preprocess, 10.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 331 saved to /content/drive/MyDrive/논문주제/ending/frame_331.png\n",
            "\n",
            "0: 384x640 2 persons, 8.5ms\n",
            "Speed: 2.6ms preprocess, 8.5ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 332 saved to /content/drive/MyDrive/논문주제/ending/frame_332.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 333 saved to /content/drive/MyDrive/논문주제/ending/frame_333.png\n",
            "\n",
            "0: 384x640 2 persons, 11.0ms\n",
            "Speed: 3.6ms preprocess, 11.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 334 saved to /content/drive/MyDrive/논문주제/ending/frame_334.png\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.9ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 335 saved to /content/drive/MyDrive/논문주제/ending/frame_335.png\n",
            "\n",
            "0: 384x640 2 persons, 7.1ms\n",
            "Speed: 2.7ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 336 saved to /content/drive/MyDrive/논문주제/ending/frame_336.png\n",
            "\n",
            "0: 384x640 2 persons, 7.1ms\n",
            "Speed: 2.8ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 337 saved to /content/drive/MyDrive/논문주제/ending/frame_337.png\n",
            "\n",
            "0: 384x640 2 persons, 11.3ms\n",
            "Speed: 3.4ms preprocess, 11.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 338 saved to /content/drive/MyDrive/논문주제/ending/frame_338.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 3.0ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 339 saved to /content/drive/MyDrive/논문주제/ending/frame_339.png\n",
            "\n",
            "0: 384x640 2 persons, 12.2ms\n",
            "Speed: 3.3ms preprocess, 12.2ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 340 saved to /content/drive/MyDrive/논문주제/ending/frame_340.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 341 saved to /content/drive/MyDrive/논문주제/ending/frame_341.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 342 saved to /content/drive/MyDrive/논문주제/ending/frame_342.png\n",
            "\n",
            "0: 384x640 2 persons, 7.0ms\n",
            "Speed: 2.8ms preprocess, 7.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 343 saved to /content/drive/MyDrive/논문주제/ending/frame_343.png\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 3.0ms preprocess, 7.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 344 saved to /content/drive/MyDrive/논문주제/ending/frame_344.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.6ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 345 saved to /content/drive/MyDrive/논문주제/ending/frame_345.png\n",
            "\n",
            "0: 384x640 2 persons, 7.1ms\n",
            "Speed: 2.7ms preprocess, 7.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 346 saved to /content/drive/MyDrive/논문주제/ending/frame_346.png\n",
            "\n",
            "0: 384x640 2 persons, 8.6ms\n",
            "Speed: 3.3ms preprocess, 8.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 347 saved to /content/drive/MyDrive/논문주제/ending/frame_347.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.9ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 348 saved to /content/drive/MyDrive/논문주제/ending/frame_348.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 349 saved to /content/drive/MyDrive/논문주제/ending/frame_349.png\n",
            "\n",
            "0: 384x640 2 persons, 7.9ms\n",
            "Speed: 2.8ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 350 saved to /content/drive/MyDrive/논문주제/ending/frame_350.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.7ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 351 saved to /content/drive/MyDrive/논문주제/ending/frame_351.png\n",
            "\n",
            "0: 384x640 2 persons, 9.1ms\n",
            "Speed: 2.4ms preprocess, 9.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 352 saved to /content/drive/MyDrive/논문주제/ending/frame_352.png\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 3.0ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 353 saved to /content/drive/MyDrive/논문주제/ending/frame_353.png\n",
            "\n",
            "0: 384x640 2 persons, 8.1ms\n",
            "Speed: 3.0ms preprocess, 8.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 354 saved to /content/drive/MyDrive/논문주제/ending/frame_354.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 2.8ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 355 saved to /content/drive/MyDrive/논문주제/ending/frame_355.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 356 saved to /content/drive/MyDrive/논문주제/ending/frame_356.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.9ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 357 saved to /content/drive/MyDrive/논문주제/ending/frame_357.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.5ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 358 saved to /content/drive/MyDrive/논문주제/ending/frame_358.png\n",
            "\n",
            "0: 384x640 3 persons, 7.7ms\n",
            "Speed: 2.9ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 359 saved to /content/drive/MyDrive/논문주제/ending/frame_359.png\n",
            "\n",
            "0: 384x640 2 persons, 11.2ms\n",
            "Speed: 3.6ms preprocess, 11.2ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 360 saved to /content/drive/MyDrive/논문주제/ending/frame_360.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 3.1ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 361 saved to /content/drive/MyDrive/논문주제/ending/frame_361.png\n",
            "\n",
            "0: 384x640 3 persons, 7.7ms\n",
            "Speed: 2.5ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 362 saved to /content/drive/MyDrive/논문주제/ending/frame_362.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.7ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 363 saved to /content/drive/MyDrive/논문주제/ending/frame_363.png\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 2.9ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 364: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 364 saved to /content/drive/MyDrive/논문주제/ending/frame_364.png\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 2.7ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 365: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 365 saved to /content/drive/MyDrive/논문주제/ending/frame_365.png\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 366: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 366 saved to /content/drive/MyDrive/논문주제/ending/frame_366.png\n",
            "\n",
            "0: 384x640 1 person, 7.8ms\n",
            "Speed: 2.7ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 367: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 367 saved to /content/drive/MyDrive/논문주제/ending/frame_367.png\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 2.9ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 368: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 368 saved to /content/drive/MyDrive/논문주제/ending/frame_368.png\n",
            "\n",
            "0: 384x640 1 person, 7.7ms\n",
            "Speed: 2.7ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 369: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 369 saved to /content/drive/MyDrive/논문주제/ending/frame_369.png\n",
            "\n",
            "0: 384x640 1 person, 7.2ms\n",
            "Speed: 3.0ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 370: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 370 saved to /content/drive/MyDrive/논문주제/ending/frame_370.png\n",
            "\n",
            "0: 384x640 1 person, 9.2ms\n",
            "Speed: 3.6ms preprocess, 9.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 371: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 371 saved to /content/drive/MyDrive/논문주제/ending/frame_371.png\n",
            "\n",
            "0: 384x640 1 person, 8.5ms\n",
            "Speed: 2.6ms preprocess, 8.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 372: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 372 saved to /content/drive/MyDrive/논문주제/ending/frame_372.png\n",
            "\n",
            "0: 384x640 1 person, 7.9ms\n",
            "Speed: 2.9ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 373: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 373 saved to /content/drive/MyDrive/논문주제/ending/frame_373.png\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 2.8ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 374: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 374 saved to /content/drive/MyDrive/논문주제/ending/frame_374.png\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 375: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 375 saved to /content/drive/MyDrive/논문주제/ending/frame_375.png\n",
            "\n",
            "0: 384x640 1 person, 7.9ms\n",
            "Speed: 2.8ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 376: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 376 saved to /content/drive/MyDrive/논문주제/ending/frame_376.png\n",
            "\n",
            "0: 384x640 1 person, 7.1ms\n",
            "Speed: 2.9ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 377: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 377 saved to /content/drive/MyDrive/논문주제/ending/frame_377.png\n",
            "\n",
            "0: 384x640 1 person, 7.8ms\n",
            "Speed: 2.8ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 378: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 378 saved to /content/drive/MyDrive/논문주제/ending/frame_378.png\n",
            "\n",
            "0: 384x640 1 person, 7.8ms\n",
            "Speed: 2.9ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 379: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 379 saved to /content/drive/MyDrive/논문주제/ending/frame_379.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 380 saved to /content/drive/MyDrive/논문주제/ending/frame_380.png\n",
            "\n",
            "0: 384x640 2 persons, 11.4ms\n",
            "Speed: 3.4ms preprocess, 11.4ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 381 saved to /content/drive/MyDrive/논문주제/ending/frame_381.png\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 3.1ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 382 saved to /content/drive/MyDrive/논문주제/ending/frame_382.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.5ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 383 saved to /content/drive/MyDrive/논문주제/ending/frame_383.png\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.9ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 384 saved to /content/drive/MyDrive/논문주제/ending/frame_384.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.6ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 385 saved to /content/drive/MyDrive/논문주제/ending/frame_385.png\n",
            "\n",
            "0: 384x640 2 persons, 9.5ms\n",
            "Speed: 3.5ms preprocess, 9.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 386 saved to /content/drive/MyDrive/논문주제/ending/frame_386.png\n",
            "\n",
            "0: 384x640 3 persons, 7.7ms\n",
            "Speed: 3.0ms preprocess, 7.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 387 saved to /content/drive/MyDrive/논문주제/ending/frame_387.png\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 3.1ms preprocess, 7.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 388 saved to /content/drive/MyDrive/논문주제/ending/frame_388.png\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.9ms preprocess, 7.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 389 saved to /content/drive/MyDrive/논문주제/ending/frame_389.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.9ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 390 saved to /content/drive/MyDrive/논문주제/ending/frame_390.png\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.8ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 391 saved to /content/drive/MyDrive/논문주제/ending/frame_391.png\n",
            "\n",
            "0: 384x640 2 persons, 10.1ms\n",
            "Speed: 2.6ms preprocess, 10.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 392 saved to /content/drive/MyDrive/논문주제/ending/frame_392.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 393 saved to /content/drive/MyDrive/논문주제/ending/frame_393.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 394 saved to /content/drive/MyDrive/논문주제/ending/frame_394.png\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 395 saved to /content/drive/MyDrive/논문주제/ending/frame_395.png\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.7ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 396 saved to /content/drive/MyDrive/논문주제/ending/frame_396.png\n",
            "\n",
            "0: 384x640 2 persons, 11.8ms\n",
            "Speed: 2.9ms preprocess, 11.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 397 saved to /content/drive/MyDrive/논문주제/ending/frame_397.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 398 saved to /content/drive/MyDrive/논문주제/ending/frame_398.png\n",
            "\n",
            "0: 384x640 2 persons, 10.3ms\n",
            "Speed: 3.5ms preprocess, 10.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 399 saved to /content/drive/MyDrive/논문주제/ending/frame_399.png\n",
            "\n",
            "0: 384x640 2 persons, 11.0ms\n",
            "Speed: 3.5ms preprocess, 11.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 400 saved to /content/drive/MyDrive/논문주제/ending/frame_400.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 401 saved to /content/drive/MyDrive/논문주제/ending/frame_401.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 3.1ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 402 saved to /content/drive/MyDrive/논문주제/ending/frame_402.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 3.0ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 403 saved to /content/drive/MyDrive/논문주제/ending/frame_403.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.6ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 404 saved to /content/drive/MyDrive/논문주제/ending/frame_404.png\n",
            "\n",
            "0: 384x640 2 persons, 11.6ms\n",
            "Speed: 3.6ms preprocess, 11.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 405 saved to /content/drive/MyDrive/논문주제/ending/frame_405.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 3.2ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 406 saved to /content/drive/MyDrive/논문주제/ending/frame_406.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.5ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 407 saved to /content/drive/MyDrive/논문주제/ending/frame_407.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 2.8ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 408 saved to /content/drive/MyDrive/논문주제/ending/frame_408.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.8ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 409 saved to /content/drive/MyDrive/논문주제/ending/frame_409.png\n",
            "\n",
            "0: 384x640 2 persons, 7.1ms\n",
            "Speed: 3.0ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 410 saved to /content/drive/MyDrive/논문주제/ending/frame_410.png\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 3.4ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 411 saved to /content/drive/MyDrive/논문주제/ending/frame_411.png\n",
            "\n",
            "0: 384x640 2 persons, 8.6ms\n",
            "Speed: 2.5ms preprocess, 8.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 412 saved to /content/drive/MyDrive/논문주제/ending/frame_412.png\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 3.0ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 413 saved to /content/drive/MyDrive/논문주제/ending/frame_413.png\n",
            "\n",
            "0: 384x640 2 persons, 11.2ms\n",
            "Speed: 3.5ms preprocess, 11.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 414 saved to /content/drive/MyDrive/논문주제/ending/frame_414.png\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.9ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 415 saved to /content/drive/MyDrive/논문주제/ending/frame_415.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 416 saved to /content/drive/MyDrive/논문주제/ending/frame_416.png\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.7ms preprocess, 7.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 417 saved to /content/drive/MyDrive/논문주제/ending/frame_417.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 3.2ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 418 saved to /content/drive/MyDrive/논문주제/ending/frame_418.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 3.0ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 419 saved to /content/drive/MyDrive/논문주제/ending/frame_419.png\n",
            "\n",
            "0: 384x640 2 persons, 7.1ms\n",
            "Speed: 2.6ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 420 saved to /content/drive/MyDrive/논문주제/ending/frame_420.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.8ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 421 saved to /content/drive/MyDrive/논문주제/ending/frame_421.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.8ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 422 saved to /content/drive/MyDrive/논문주제/ending/frame_422.png\n",
            "\n",
            "0: 384x640 2 persons, 8.5ms\n",
            "Speed: 2.6ms preprocess, 8.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 423 saved to /content/drive/MyDrive/논문주제/ending/frame_423.png\n",
            "\n",
            "0: 384x640 2 persons, 8.1ms\n",
            "Speed: 2.8ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 424 saved to /content/drive/MyDrive/논문주제/ending/frame_424.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 425 saved to /content/drive/MyDrive/논문주제/ending/frame_425.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 3.1ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 426 saved to /content/drive/MyDrive/논문주제/ending/frame_426.png\n",
            "\n",
            "0: 384x640 2 persons, 8.6ms\n",
            "Speed: 2.8ms preprocess, 8.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 427 saved to /content/drive/MyDrive/논문주제/ending/frame_427.png\n",
            "\n",
            "0: 384x640 2 persons, 6.9ms\n",
            "Speed: 2.5ms preprocess, 6.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 428 saved to /content/drive/MyDrive/논문주제/ending/frame_428.png\n",
            "\n",
            "0: 384x640 2 persons, 7.9ms\n",
            "Speed: 2.8ms preprocess, 7.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 429 saved to /content/drive/MyDrive/논문주제/ending/frame_429.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.7ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 430 saved to /content/drive/MyDrive/논문주제/ending/frame_430.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 431 saved to /content/drive/MyDrive/논문주제/ending/frame_431.png\n",
            "\n",
            "0: 384x640 2 persons, 11.0ms\n",
            "Speed: 2.7ms preprocess, 11.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 432 saved to /content/drive/MyDrive/논문주제/ending/frame_432.png\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 3.0ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 433 saved to /content/drive/MyDrive/논문주제/ending/frame_433.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 3.3ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 434 saved to /content/drive/MyDrive/논문주제/ending/frame_434.png\n",
            "\n",
            "0: 384x640 2 persons, 12.1ms\n",
            "Speed: 3.4ms preprocess, 12.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 435 saved to /content/drive/MyDrive/논문주제/ending/frame_435.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.7ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 436 saved to /content/drive/MyDrive/논문주제/ending/frame_436.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.9ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 437 saved to /content/drive/MyDrive/논문주제/ending/frame_437.png\n",
            "\n",
            "0: 384x640 2 persons, 7.1ms\n",
            "Speed: 2.7ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 438 saved to /content/drive/MyDrive/논문주제/ending/frame_438.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.9ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 439 saved to /content/drive/MyDrive/논문주제/ending/frame_439.png\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 3.3ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 440 saved to /content/drive/MyDrive/논문주제/ending/frame_440.png\n",
            "\n",
            "0: 384x640 2 persons, 11.1ms\n",
            "Speed: 3.6ms preprocess, 11.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 441 saved to /content/drive/MyDrive/논문주제/ending/frame_441.png\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.5ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 442 saved to /content/drive/MyDrive/논문주제/ending/frame_442.png\n",
            "\n",
            "0: 384x640 2 persons, 7.9ms\n",
            "Speed: 3.1ms preprocess, 7.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 443 saved to /content/drive/MyDrive/논문주제/ending/frame_443.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 444 saved to /content/drive/MyDrive/논문주제/ending/frame_444.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 3.0ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 445 saved to /content/drive/MyDrive/논문주제/ending/frame_445.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.9ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 446 saved to /content/drive/MyDrive/논문주제/ending/frame_446.png\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.9ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 447 saved to /content/drive/MyDrive/논문주제/ending/frame_447.png\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 448 saved to /content/drive/MyDrive/논문주제/ending/frame_448.png\n",
            "\n",
            "0: 384x640 2 persons, 11.2ms\n",
            "Speed: 3.4ms preprocess, 11.2ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 449 saved to /content/drive/MyDrive/논문주제/ending/frame_449.png\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.9ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 450 saved to /content/drive/MyDrive/논문주제/ending/frame_450.png\n",
            "\n",
            "0: 384x640 2 persons, 8.8ms\n",
            "Speed: 3.0ms preprocess, 8.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 451 saved to /content/drive/MyDrive/논문주제/ending/frame_451.png\n",
            "\n",
            "0: 384x640 2 persons, 9.3ms\n",
            "Speed: 2.7ms preprocess, 9.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 452 saved to /content/drive/MyDrive/논문주제/ending/frame_452.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.9ms preprocess, 7.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 453 saved to /content/drive/MyDrive/논문주제/ending/frame_453.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.9ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 454 saved to /content/drive/MyDrive/논문주제/ending/frame_454.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.8ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 455 saved to /content/drive/MyDrive/논문주제/ending/frame_455.png\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 3.2ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 456 saved to /content/drive/MyDrive/논문주제/ending/frame_456.png\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.5ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 457 saved to /content/drive/MyDrive/논문주제/ending/frame_457.png\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.8ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 458 saved to /content/drive/MyDrive/논문주제/ending/frame_458.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 3.1ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 459 saved to /content/drive/MyDrive/논문주제/ending/frame_459.png\n",
            "\n",
            "0: 384x640 2 persons, 11.2ms\n",
            "Speed: 3.3ms preprocess, 11.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 460 saved to /content/drive/MyDrive/논문주제/ending/frame_460.png\n",
            "\n",
            "0: 384x640 2 persons, 8.2ms\n",
            "Speed: 2.8ms preprocess, 8.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 461 saved to /content/drive/MyDrive/논문주제/ending/frame_461.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 3.4ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 462 saved to /content/drive/MyDrive/논문주제/ending/frame_462.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.6ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 463 saved to /content/drive/MyDrive/논문주제/ending/frame_463.png\n",
            "\n",
            "0: 384x640 2 persons, 8.2ms\n",
            "Speed: 3.1ms preprocess, 8.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 464 saved to /content/drive/MyDrive/논문주제/ending/frame_464.png\n",
            "\n",
            "0: 384x640 2 persons, 10.3ms\n",
            "Speed: 3.5ms preprocess, 10.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 465 saved to /content/drive/MyDrive/논문주제/ending/frame_465.png\n",
            "\n",
            "0: 384x640 2 persons, 8.0ms\n",
            "Speed: 2.6ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 466 saved to /content/drive/MyDrive/논문주제/ending/frame_466.png\n",
            "\n",
            "0: 384x640 2 persons, 11.1ms\n",
            "Speed: 3.6ms preprocess, 11.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 467 saved to /content/drive/MyDrive/논문주제/ending/frame_467.png\n",
            "\n",
            "0: 384x640 2 persons, 11.3ms\n",
            "Speed: 3.3ms preprocess, 11.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 468 saved to /content/drive/MyDrive/논문주제/ending/frame_468.png\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.7ms preprocess, 7.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 469 saved to /content/drive/MyDrive/논문주제/ending/frame_469.png\n",
            "\n",
            "0: 384x640 2 persons, 7.1ms\n",
            "Speed: 3.0ms preprocess, 7.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 470 saved to /content/drive/MyDrive/논문주제/ending/frame_470.png\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.6ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 471 saved to /content/drive/MyDrive/논문주제/ending/frame_471.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 2.6ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 472 saved to /content/drive/MyDrive/논문주제/ending/frame_472.png\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 473 saved to /content/drive/MyDrive/논문주제/ending/frame_473.png\n",
            "\n",
            "0: 384x640 2 persons, 8.5ms\n",
            "Speed: 3.1ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 474 saved to /content/drive/MyDrive/논문주제/ending/frame_474.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 3.2ms preprocess, 7.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 475 saved to /content/drive/MyDrive/논문주제/ending/frame_475.png\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 3.2ms preprocess, 7.3ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 476 saved to /content/drive/MyDrive/논문주제/ending/frame_476.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.7ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 477 saved to /content/drive/MyDrive/논문주제/ending/frame_477.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 478 saved to /content/drive/MyDrive/논문주제/ending/frame_478.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.9ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 479 saved to /content/drive/MyDrive/논문주제/ending/frame_479.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.8ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 480 saved to /content/drive/MyDrive/논문주제/ending/frame_480.png\n",
            "\n",
            "0: 384x640 2 persons, 7.9ms\n",
            "Speed: 2.6ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 481 saved to /content/drive/MyDrive/논문주제/ending/frame_481.png\n",
            "\n",
            "0: 384x640 2 persons, 8.1ms\n",
            "Speed: 3.5ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 482 saved to /content/drive/MyDrive/논문주제/ending/frame_482.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 3.1ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 483 saved to /content/drive/MyDrive/논문주제/ending/frame_483.png\n",
            "\n",
            "0: 384x640 2 persons, 8.0ms\n",
            "Speed: 2.8ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 484 saved to /content/drive/MyDrive/논문주제/ending/frame_484.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.7ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 485 saved to /content/drive/MyDrive/논문주제/ending/frame_485.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 486 saved to /content/drive/MyDrive/논문주제/ending/frame_486.png\n",
            "\n",
            "0: 384x640 2 persons, 10.8ms\n",
            "Speed: 3.3ms preprocess, 10.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 487 saved to /content/drive/MyDrive/논문주제/ending/frame_487.png\n",
            "\n",
            "0: 384x640 2 persons, 8.3ms\n",
            "Speed: 3.0ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 488 saved to /content/drive/MyDrive/논문주제/ending/frame_488.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 2.8ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 489 saved to /content/drive/MyDrive/논문주제/ending/frame_489.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 3.0ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 490 saved to /content/drive/MyDrive/논문주제/ending/frame_490.png\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.7ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 491 saved to /content/drive/MyDrive/논문주제/ending/frame_491.png\n",
            "\n",
            "0: 384x640 1 person, 8.2ms\n",
            "Speed: 2.6ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 492: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 492 saved to /content/drive/MyDrive/논문주제/ending/frame_492.png\n",
            "\n",
            "0: 384x640 1 person, 11.2ms\n",
            "Speed: 3.4ms preprocess, 11.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 493: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 493 saved to /content/drive/MyDrive/논문주제/ending/frame_493.png\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 2.5ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 494: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 494 saved to /content/drive/MyDrive/논문주제/ending/frame_494.png\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 2.5ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 495: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 495 saved to /content/drive/MyDrive/논문주제/ending/frame_495.png\n",
            "\n",
            "0: 384x640 1 person, 11.3ms\n",
            "Speed: 3.5ms preprocess, 11.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 496: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 496 saved to /content/drive/MyDrive/논문주제/ending/frame_496.png\n",
            "\n",
            "0: 384x640 1 person, 7.9ms\n",
            "Speed: 2.8ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 497: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 497 saved to /content/drive/MyDrive/논문주제/ending/frame_497.png\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 3.2ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 498: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 498 saved to /content/drive/MyDrive/논문주제/ending/frame_498.png\n",
            "\n",
            "0: 384x640 1 person, 9.7ms\n",
            "Speed: 3.6ms preprocess, 9.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 499: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 499 saved to /content/drive/MyDrive/논문주제/ending/frame_499.png\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 2.9ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 500: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 500 saved to /content/drive/MyDrive/논문주제/ending/frame_500.png\n",
            "\n",
            "0: 384x640 1 person, 10.4ms\n",
            "Speed: 3.8ms preprocess, 10.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 501: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 501 saved to /content/drive/MyDrive/논문주제/ending/frame_501.png\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 3.2ms preprocess, 7.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 502: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 502 saved to /content/drive/MyDrive/논문주제/ending/frame_502.png\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 2.8ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 503: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 503 saved to /content/drive/MyDrive/논문주제/ending/frame_503.png\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 2.7ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 504: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 504 saved to /content/drive/MyDrive/논문주제/ending/frame_504.png\n",
            "\n",
            "0: 384x640 1 person, 7.8ms\n",
            "Speed: 2.6ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 505: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 505 saved to /content/drive/MyDrive/논문주제/ending/frame_505.png\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 2.7ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 506: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 506 saved to /content/drive/MyDrive/논문주제/ending/frame_506.png\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 3.2ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 507: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 507 saved to /content/drive/MyDrive/논문주제/ending/frame_507.png\n",
            "\n",
            "0: 384x640 1 person, 7.0ms\n",
            "Speed: 2.7ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 508: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 508 saved to /content/drive/MyDrive/논문주제/ending/frame_508.png\n",
            "\n",
            "0: 384x640 1 person, 7.1ms\n",
            "Speed: 2.6ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 509: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 509 saved to /content/drive/MyDrive/논문주제/ending/frame_509.png\n",
            "\n",
            "0: 384x640 1 person, 7.0ms\n",
            "Speed: 3.0ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 510: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 510 saved to /content/drive/MyDrive/논문주제/ending/frame_510.png\n",
            "\n",
            "0: 384x640 1 person, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 511: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 511 saved to /content/drive/MyDrive/논문주제/ending/frame_511.png\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 512: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 512 saved to /content/drive/MyDrive/논문주제/ending/frame_512.png\n",
            "\n",
            "0: 384x640 1 person, 7.8ms\n",
            "Speed: 2.5ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 513: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 513 saved to /content/drive/MyDrive/논문주제/ending/frame_513.png\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 514: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 514 saved to /content/drive/MyDrive/논문주제/ending/frame_514.png\n",
            "\n",
            "0: 384x640 1 person, 8.0ms\n",
            "Speed: 3.4ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 515: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 515 saved to /content/drive/MyDrive/논문주제/ending/frame_515.png\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 2.9ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 516: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 516 saved to /content/drive/MyDrive/논문주제/ending/frame_516.png\n",
            "\n",
            "0: 384x640 1 person, 7.9ms\n",
            "Speed: 2.7ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 517: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 517 saved to /content/drive/MyDrive/논문주제/ending/frame_517.png\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 2.5ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 518: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 518 saved to /content/drive/MyDrive/논문주제/ending/frame_518.png\n",
            "\n",
            "0: 384x640 1 person, 7.9ms\n",
            "Speed: 2.5ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 519: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 519 saved to /content/drive/MyDrive/논문주제/ending/frame_519.png\n",
            "\n",
            "0: 384x640 1 person, 8.4ms\n",
            "Speed: 2.9ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 520: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 520 saved to /content/drive/MyDrive/논문주제/ending/frame_520.png\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 2.9ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 521: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 521 saved to /content/drive/MyDrive/논문주제/ending/frame_521.png\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 2.9ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 522: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 522 saved to /content/drive/MyDrive/논문주제/ending/frame_522.png\n",
            "\n",
            "0: 384x640 1 person, 7.2ms\n",
            "Speed: 2.8ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 523: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 523 saved to /content/drive/MyDrive/논문주제/ending/frame_523.png\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 3.3ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 524: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 524 saved to /content/drive/MyDrive/논문주제/ending/frame_524.png\n",
            "\n",
            "0: 384x640 1 person, 9.8ms\n",
            "Speed: 3.3ms preprocess, 9.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 525: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 525 saved to /content/drive/MyDrive/논문주제/ending/frame_525.png\n",
            "\n",
            "0: 384x640 1 person, 7.9ms\n",
            "Speed: 2.8ms preprocess, 7.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 526: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 526 saved to /content/drive/MyDrive/논문주제/ending/frame_526.png\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 2.9ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 527: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 527 saved to /content/drive/MyDrive/논문주제/ending/frame_527.png\n",
            "\n",
            "0: 384x640 1 person, 7.9ms\n",
            "Speed: 2.5ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 528: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 528 saved to /content/drive/MyDrive/논문주제/ending/frame_528.png\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 2.5ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 529: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 529 saved to /content/drive/MyDrive/논문주제/ending/frame_529.png\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 2.8ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 530: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 530 saved to /content/drive/MyDrive/논문주제/ending/frame_530.png\n",
            "\n",
            "0: 384x640 1 person, 8.6ms\n",
            "Speed: 2.9ms preprocess, 8.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 531: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 531 saved to /content/drive/MyDrive/논문주제/ending/frame_531.png\n",
            "\n",
            "0: 384x640 1 person, 7.9ms\n",
            "Speed: 2.5ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 532: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 532 saved to /content/drive/MyDrive/논문주제/ending/frame_532.png\n",
            "\n",
            "0: 384x640 1 person, 8.1ms\n",
            "Speed: 2.7ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 533: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 533 saved to /content/drive/MyDrive/논문주제/ending/frame_533.png\n",
            "\n",
            "0: 384x640 1 person, 8.8ms\n",
            "Speed: 2.8ms preprocess, 8.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 534: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 534 saved to /content/drive/MyDrive/논문주제/ending/frame_534.png\n",
            "\n",
            "0: 384x640 1 person, 8.0ms\n",
            "Speed: 2.6ms preprocess, 8.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 535: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 535 saved to /content/drive/MyDrive/논문주제/ending/frame_535.png\n",
            "\n",
            "0: 384x640 1 person, 7.1ms\n",
            "Speed: 2.7ms preprocess, 7.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 536: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 536 saved to /content/drive/MyDrive/논문주제/ending/frame_536.png\n",
            "\n",
            "0: 384x640 1 person, 12.1ms\n",
            "Speed: 3.5ms preprocess, 12.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 537: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 537 saved to /content/drive/MyDrive/논문주제/ending/frame_537.png\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 3.2ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 538: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 538 saved to /content/drive/MyDrive/논문주제/ending/frame_538.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 2.8ms preprocess, 7.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 539 saved to /content/drive/MyDrive/논문주제/ending/frame_539.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 3.0ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 540 saved to /content/drive/MyDrive/논문주제/ending/frame_540.png\n",
            "\n",
            "0: 384x640 2 persons, 7.0ms\n",
            "Speed: 2.6ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 541 saved to /content/drive/MyDrive/논문주제/ending/frame_541.png\n",
            "\n",
            "0: 384x640 2 persons, 7.9ms\n",
            "Speed: 2.8ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 542 saved to /content/drive/MyDrive/논문주제/ending/frame_542.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.7ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 543 saved to /content/drive/MyDrive/논문주제/ending/frame_543.png\n",
            "\n",
            "0: 384x640 2 persons, 8.4ms\n",
            "Speed: 3.0ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 544 saved to /content/drive/MyDrive/논문주제/ending/frame_544.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 3.2ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 545 saved to /content/drive/MyDrive/논문주제/ending/frame_545.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 2.7ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 546 saved to /content/drive/MyDrive/논문주제/ending/frame_546.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 3.3ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 547 saved to /content/drive/MyDrive/논문주제/ending/frame_547.png\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 2.6ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 548: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 548 saved to /content/drive/MyDrive/논문주제/ending/frame_548.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.7ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 549 saved to /content/drive/MyDrive/논문주제/ending/frame_549.png\n",
            "\n",
            "0: 384x640 1 person, 7.0ms\n",
            "Speed: 2.5ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 550: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 550 saved to /content/drive/MyDrive/논문주제/ending/frame_550.png\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 2.6ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 551: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 551 saved to /content/drive/MyDrive/논문주제/ending/frame_551.png\n",
            "\n",
            "0: 384x640 1 person, 7.8ms\n",
            "Speed: 2.7ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 552: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 552 saved to /content/drive/MyDrive/논문주제/ending/frame_552.png\n",
            "\n",
            "0: 384x640 1 person, 7.2ms\n",
            "Speed: 2.6ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 553: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 553 saved to /content/drive/MyDrive/논문주제/ending/frame_553.png\n",
            "\n",
            "0: 384x640 2 persons, 8.1ms\n",
            "Speed: 2.4ms preprocess, 8.1ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 554 saved to /content/drive/MyDrive/논문주제/ending/frame_554.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 3.2ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 555 saved to /content/drive/MyDrive/논문주제/ending/frame_555.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 556 saved to /content/drive/MyDrive/논문주제/ending/frame_556.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 3.1ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 557 saved to /content/drive/MyDrive/논문주제/ending/frame_557.png\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.6ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 558 saved to /content/drive/MyDrive/논문주제/ending/frame_558.png\n",
            "\n",
            "0: 384x640 2 persons, 10.9ms\n",
            "Speed: 3.5ms preprocess, 10.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 559 saved to /content/drive/MyDrive/논문주제/ending/frame_559.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.8ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 560 saved to /content/drive/MyDrive/논문주제/ending/frame_560.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 2.8ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 561 saved to /content/drive/MyDrive/논문주제/ending/frame_561.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 2.6ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 562 saved to /content/drive/MyDrive/논문주제/ending/frame_562.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 3.4ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 563 saved to /content/drive/MyDrive/논문주제/ending/frame_563.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 564 saved to /content/drive/MyDrive/논문주제/ending/frame_564.png\n",
            "\n",
            "0: 384x640 2 persons, 7.1ms\n",
            "Speed: 2.9ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 565 saved to /content/drive/MyDrive/논문주제/ending/frame_565.png\n",
            "\n",
            "0: 384x640 2 persons, 10.8ms\n",
            "Speed: 3.3ms preprocess, 10.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 566 saved to /content/drive/MyDrive/논문주제/ending/frame_566.png\n",
            "\n",
            "0: 384x640 2 persons, 7.1ms\n",
            "Speed: 3.1ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 567 saved to /content/drive/MyDrive/논문주제/ending/frame_567.png\n",
            "\n",
            "0: 384x640 2 persons, 10.0ms\n",
            "Speed: 3.6ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 568 saved to /content/drive/MyDrive/논문주제/ending/frame_568.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.5ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 569 saved to /content/drive/MyDrive/논문주제/ending/frame_569.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 3.3ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 570 saved to /content/drive/MyDrive/논문주제/ending/frame_570.png\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 2.9ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 571: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 571 saved to /content/drive/MyDrive/논문주제/ending/frame_571.png\n",
            "\n",
            "0: 384x640 1 person, 9.9ms\n",
            "Speed: 3.5ms preprocess, 9.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 572: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 572 saved to /content/drive/MyDrive/논문주제/ending/frame_572.png\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 573: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 573 saved to /content/drive/MyDrive/논문주제/ending/frame_573.png\n",
            "\n",
            "0: 384x640 1 person, 7.0ms\n",
            "Speed: 2.6ms preprocess, 7.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 574: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 574 saved to /content/drive/MyDrive/논문주제/ending/frame_574.png\n",
            "\n",
            "0: 384x640 1 person, 8.9ms\n",
            "Speed: 2.7ms preprocess, 8.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 575: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 575 saved to /content/drive/MyDrive/논문주제/ending/frame_575.png\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 2.7ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 576: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 576 saved to /content/drive/MyDrive/논문주제/ending/frame_576.png\n",
            "\n",
            "0: 384x640 1 person, 7.1ms\n",
            "Speed: 2.6ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 577: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 577 saved to /content/drive/MyDrive/논문주제/ending/frame_577.png\n",
            "\n",
            "0: 384x640 1 person, 8.1ms\n",
            "Speed: 3.5ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 578: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 578 saved to /content/drive/MyDrive/논문주제/ending/frame_578.png\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 579: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 579 saved to /content/drive/MyDrive/논문주제/ending/frame_579.png\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 3.4ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 580: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 580 saved to /content/drive/MyDrive/논문주제/ending/frame_580.png\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 2.7ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 581: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 581 saved to /content/drive/MyDrive/논문주제/ending/frame_581.png\n",
            "\n",
            "0: 384x640 1 person, 7.8ms\n",
            "Speed: 2.7ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 582: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 582 saved to /content/drive/MyDrive/논문주제/ending/frame_582.png\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 2.9ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 583: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 583 saved to /content/drive/MyDrive/논문주제/ending/frame_583.png\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 2.5ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 584: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 584 saved to /content/drive/MyDrive/논문주제/ending/frame_584.png\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 2.9ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 585: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 585 saved to /content/drive/MyDrive/논문주제/ending/frame_585.png\n",
            "\n",
            "0: 384x640 1 person, 10.9ms\n",
            "Speed: 3.3ms preprocess, 10.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 586: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 586 saved to /content/drive/MyDrive/논문주제/ending/frame_586.png\n",
            "\n",
            "0: 384x640 1 person, 7.8ms\n",
            "Speed: 2.9ms preprocess, 7.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 587: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 587 saved to /content/drive/MyDrive/논문주제/ending/frame_587.png\n",
            "\n",
            "0: 384x640 1 person, 11.3ms\n",
            "Speed: 3.5ms preprocess, 11.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 588: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 588 saved to /content/drive/MyDrive/논문주제/ending/frame_588.png\n",
            "\n",
            "0: 384x640 1 person, 7.8ms\n",
            "Speed: 2.7ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 589: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 589 saved to /content/drive/MyDrive/논문주제/ending/frame_589.png\n",
            "\n",
            "0: 384x640 1 person, 7.2ms\n",
            "Speed: 2.8ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 590: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 590 saved to /content/drive/MyDrive/논문주제/ending/frame_590.png\n",
            "\n",
            "0: 384x640 1 person, 7.8ms\n",
            "Speed: 2.4ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 591: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 591 saved to /content/drive/MyDrive/논문주제/ending/frame_591.png\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 2.9ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 592: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 592 saved to /content/drive/MyDrive/논문주제/ending/frame_592.png\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 2.5ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 593: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 593 saved to /content/drive/MyDrive/논문주제/ending/frame_593.png\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 2.7ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 594: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 594 saved to /content/drive/MyDrive/논문주제/ending/frame_594.png\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 595: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 595 saved to /content/drive/MyDrive/논문주제/ending/frame_595.png\n",
            "\n",
            "0: 384x640 1 person, 12.9ms\n",
            "Speed: 3.5ms preprocess, 12.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 596: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 596 saved to /content/drive/MyDrive/논문주제/ending/frame_596.png\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 2.6ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 597: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 597 saved to /content/drive/MyDrive/논문주제/ending/frame_597.png\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 2.9ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 598: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 598 saved to /content/drive/MyDrive/논문주제/ending/frame_598.png\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 2.6ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 599: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 599 saved to /content/drive/MyDrive/논문주제/ending/frame_599.png\n",
            "\n",
            "0: 384x640 1 person, 10.8ms\n",
            "Speed: 3.5ms preprocess, 10.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 600: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 600 saved to /content/drive/MyDrive/논문주제/ending/frame_600.png\n",
            "\n",
            "0: 384x640 1 person, 9.7ms\n",
            "Speed: 3.5ms preprocess, 9.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 601: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 601 saved to /content/drive/MyDrive/논문주제/ending/frame_601.png\n",
            "\n",
            "0: 384x640 1 person, 8.3ms\n",
            "Speed: 2.5ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 602: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 602 saved to /content/drive/MyDrive/논문주제/ending/frame_602.png\n",
            "\n",
            "0: 384x640 1 person, 7.8ms\n",
            "Speed: 2.9ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 603: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 603 saved to /content/drive/MyDrive/논문주제/ending/frame_603.png\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 2.7ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 604: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 604 saved to /content/drive/MyDrive/논문주제/ending/frame_604.png\n",
            "\n",
            "0: 384x640 1 person, 11.0ms\n",
            "Speed: 3.6ms preprocess, 11.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 605: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 605 saved to /content/drive/MyDrive/논문주제/ending/frame_605.png\n",
            "\n",
            "0: 384x640 1 person, 7.7ms\n",
            "Speed: 2.7ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 606: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 606 saved to /content/drive/MyDrive/논문주제/ending/frame_606.png\n",
            "\n",
            "0: 384x640 1 person, 7.9ms\n",
            "Speed: 2.5ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 607: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 607 saved to /content/drive/MyDrive/논문주제/ending/frame_607.png\n",
            "\n",
            "0: 384x640 1 person, 7.7ms\n",
            "Speed: 3.0ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 608: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 608 saved to /content/drive/MyDrive/논문주제/ending/frame_608.png\n",
            "\n",
            "0: 384x640 1 person, 7.2ms\n",
            "Speed: 2.4ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 609: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 609 saved to /content/drive/MyDrive/논문주제/ending/frame_609.png\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 2.9ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 610: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 610 saved to /content/drive/MyDrive/논문주제/ending/frame_610.png\n",
            "\n",
            "0: 384x640 1 person, 7.7ms\n",
            "Speed: 3.3ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 611: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 611 saved to /content/drive/MyDrive/논문주제/ending/frame_611.png\n",
            "\n",
            "0: 384x640 1 person, 10.8ms\n",
            "Speed: 3.5ms preprocess, 10.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 612: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 612 saved to /content/drive/MyDrive/논문주제/ending/frame_612.png\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 2.5ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 613: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 613 saved to /content/drive/MyDrive/논문주제/ending/frame_613.png\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 3.0ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 614: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 614 saved to /content/drive/MyDrive/논문주제/ending/frame_614.png\n",
            "\n",
            "0: 384x640 1 person, 7.7ms\n",
            "Speed: 2.4ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 615: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 615 saved to /content/drive/MyDrive/논문주제/ending/frame_615.png\n",
            "\n",
            "0: 384x640 1 person, 12.0ms\n",
            "Speed: 3.3ms preprocess, 12.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 616: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 616 saved to /content/drive/MyDrive/논문주제/ending/frame_616.png\n",
            "\n",
            "0: 384x640 1 person, 7.8ms\n",
            "Speed: 2.8ms preprocess, 7.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 617: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 617 saved to /content/drive/MyDrive/논문주제/ending/frame_617.png\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 2.5ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 618: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 618 saved to /content/drive/MyDrive/논문주제/ending/frame_618.png\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 2.9ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 619: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 619 saved to /content/drive/MyDrive/논문주제/ending/frame_619.png\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 3.0ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 620: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 620 saved to /content/drive/MyDrive/논문주제/ending/frame_620.png\n",
            "\n",
            "0: 384x640 1 person, 7.2ms\n",
            "Speed: 3.2ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 621: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 621 saved to /content/drive/MyDrive/논문주제/ending/frame_621.png\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 2.4ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 622: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 622 saved to /content/drive/MyDrive/논문주제/ending/frame_622.png\n",
            "\n",
            "0: 384x640 1 person, 7.7ms\n",
            "Speed: 2.6ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 623: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 623 saved to /content/drive/MyDrive/논문주제/ending/frame_623.png\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 2.6ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 624: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 624 saved to /content/drive/MyDrive/논문주제/ending/frame_624.png\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 2.9ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 625: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 625 saved to /content/drive/MyDrive/논문주제/ending/frame_625.png\n",
            "\n",
            "0: 384x640 1 person, 7.7ms\n",
            "Speed: 2.7ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 626: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 626 saved to /content/drive/MyDrive/논문주제/ending/frame_626.png\n",
            "\n",
            "0: 384x640 1 person, 7.1ms\n",
            "Speed: 3.1ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 627: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 627 saved to /content/drive/MyDrive/논문주제/ending/frame_627.png\n",
            "\n",
            "0: 384x640 1 person, 8.3ms\n",
            "Speed: 2.5ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 628: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 628 saved to /content/drive/MyDrive/논문주제/ending/frame_628.png\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 2.4ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 629: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 629 saved to /content/drive/MyDrive/논문주제/ending/frame_629.png\n",
            "\n",
            "0: 384x640 1 person, 8.1ms\n",
            "Speed: 2.9ms preprocess, 8.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 630: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 630 saved to /content/drive/MyDrive/논문주제/ending/frame_630.png\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 2.7ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 631: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 631 saved to /content/drive/MyDrive/논문주제/ending/frame_631.png\n",
            "\n",
            "0: 384x640 1 person, 7.7ms\n",
            "Speed: 2.7ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 632: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 632 saved to /content/drive/MyDrive/논문주제/ending/frame_632.png\n",
            "\n",
            "0: 384x640 1 person, 7.9ms\n",
            "Speed: 2.6ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 633: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 633 saved to /content/drive/MyDrive/논문주제/ending/frame_633.png\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 2.4ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 634: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 634 saved to /content/drive/MyDrive/논문주제/ending/frame_634.png\n",
            "\n",
            "0: 384x640 1 person, 7.2ms\n",
            "Speed: 2.5ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 635: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 635 saved to /content/drive/MyDrive/논문주제/ending/frame_635.png\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 2.7ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 636: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 636 saved to /content/drive/MyDrive/논문주제/ending/frame_636.png\n",
            "\n",
            "0: 384x640 1 person, 11.0ms\n",
            "Speed: 3.3ms preprocess, 11.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 637: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 637 saved to /content/drive/MyDrive/논문주제/ending/frame_637.png\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 2.4ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 638: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 638 saved to /content/drive/MyDrive/논문주제/ending/frame_638.png\n",
            "\n",
            "0: 384x640 1 person, 7.2ms\n",
            "Speed: 3.1ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 639: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 639 saved to /content/drive/MyDrive/논문주제/ending/frame_639.png\n",
            "\n",
            "0: 384x640 1 person, 8.0ms\n",
            "Speed: 2.7ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 640: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 640 saved to /content/drive/MyDrive/논문주제/ending/frame_640.png\n",
            "\n",
            "0: 384x640 1 person, 10.9ms\n",
            "Speed: 3.5ms preprocess, 10.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 641: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 641 saved to /content/drive/MyDrive/논문주제/ending/frame_641.png\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 2.5ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 642: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 642 saved to /content/drive/MyDrive/논문주제/ending/frame_642.png\n",
            "\n",
            "0: 384x640 1 person, 7.2ms\n",
            "Speed: 2.6ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 643: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 643 saved to /content/drive/MyDrive/논문주제/ending/frame_643.png\n",
            "\n",
            "0: 384x640 1 person, 7.8ms\n",
            "Speed: 3.0ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 644: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 644 saved to /content/drive/MyDrive/논문주제/ending/frame_644.png\n",
            "\n",
            "0: 384x640 1 person, 7.2ms\n",
            "Speed: 3.6ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 645: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 645 saved to /content/drive/MyDrive/논문주제/ending/frame_645.png\n",
            "\n",
            "0: 384x640 1 person, 7.8ms\n",
            "Speed: 2.7ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 646: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 646 saved to /content/drive/MyDrive/논문주제/ending/frame_646.png\n",
            "\n",
            "0: 384x640 1 person, 7.7ms\n",
            "Speed: 3.0ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 647: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 647 saved to /content/drive/MyDrive/논문주제/ending/frame_647.png\n",
            "\n",
            "0: 384x640 1 person, 11.6ms\n",
            "Speed: 3.4ms preprocess, 11.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 648: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 648 saved to /content/drive/MyDrive/논문주제/ending/frame_648.png\n",
            "\n",
            "0: 384x640 1 person, 11.0ms\n",
            "Speed: 3.4ms preprocess, 11.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 649: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 649 saved to /content/drive/MyDrive/논문주제/ending/frame_649.png\n",
            "\n",
            "0: 384x640 1 person, 7.8ms\n",
            "Speed: 2.7ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 650: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 650 saved to /content/drive/MyDrive/논문주제/ending/frame_650.png\n",
            "\n",
            "0: 384x640 1 person, 10.9ms\n",
            "Speed: 3.4ms preprocess, 10.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 651: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 651 saved to /content/drive/MyDrive/논문주제/ending/frame_651.png\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 3.3ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 652: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 652 saved to /content/drive/MyDrive/논문주제/ending/frame_652.png\n",
            "\n",
            "0: 384x640 1 person, 10.6ms\n",
            "Speed: 3.4ms preprocess, 10.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 653: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 653 saved to /content/drive/MyDrive/논문주제/ending/frame_653.png\n",
            "\n",
            "0: 384x640 1 person, 7.8ms\n",
            "Speed: 3.0ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 654: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 654 saved to /content/drive/MyDrive/논문주제/ending/frame_654.png\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 2.9ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 655: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 655 saved to /content/drive/MyDrive/논문주제/ending/frame_655.png\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 2.6ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 656: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 656 saved to /content/drive/MyDrive/논문주제/ending/frame_656.png\n",
            "\n",
            "0: 384x640 1 person, 8.0ms\n",
            "Speed: 2.6ms preprocess, 8.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 657: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 657 saved to /content/drive/MyDrive/논문주제/ending/frame_657.png\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 2.5ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 658: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 658 saved to /content/drive/MyDrive/논문주제/ending/frame_658.png\n",
            "\n",
            "0: 384x640 1 person, 7.7ms\n",
            "Speed: 2.4ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 659: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 659 saved to /content/drive/MyDrive/논문주제/ending/frame_659.png\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 3.4ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 660: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 660 saved to /content/drive/MyDrive/논문주제/ending/frame_660.png\n",
            "\n",
            "0: 384x640 1 person, 8.2ms\n",
            "Speed: 2.5ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 661: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 661 saved to /content/drive/MyDrive/논문주제/ending/frame_661.png\n",
            "\n",
            "0: 384x640 1 person, 8.2ms\n",
            "Speed: 2.8ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 662: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 662 saved to /content/drive/MyDrive/논문주제/ending/frame_662.png\n",
            "\n",
            "0: 384x640 1 person, 11.2ms\n",
            "Speed: 3.6ms preprocess, 11.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 663: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 663 saved to /content/drive/MyDrive/논문주제/ending/frame_663.png\n",
            "\n",
            "0: 384x640 1 person, 7.7ms\n",
            "Speed: 2.7ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 664: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 664 saved to /content/drive/MyDrive/논문주제/ending/frame_664.png\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 665: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 665 saved to /content/drive/MyDrive/논문주제/ending/frame_665.png\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 666: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 666 saved to /content/drive/MyDrive/논문주제/ending/frame_666.png\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 2.4ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 667: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 667 saved to /content/drive/MyDrive/논문주제/ending/frame_667.png\n",
            "\n",
            "0: 384x640 1 person, 7.7ms\n",
            "Speed: 3.0ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 668: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 668 saved to /content/drive/MyDrive/논문주제/ending/frame_668.png\n",
            "\n",
            "0: 384x640 1 person, 7.7ms\n",
            "Speed: 2.5ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 669: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 669 saved to /content/drive/MyDrive/논문주제/ending/frame_669.png\n",
            "\n",
            "0: 384x640 1 person, 8.3ms\n",
            "Speed: 2.9ms preprocess, 8.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 670: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 670 saved to /content/drive/MyDrive/논문주제/ending/frame_670.png\n",
            "\n",
            "0: 384x640 1 person, 8.0ms\n",
            "Speed: 2.8ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 671: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 671 saved to /content/drive/MyDrive/논문주제/ending/frame_671.png\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 2.5ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 672: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 672 saved to /content/drive/MyDrive/논문주제/ending/frame_672.png\n",
            "\n",
            "0: 384x640 1 person, 8.6ms\n",
            "Speed: 2.6ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 673: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 673 saved to /content/drive/MyDrive/논문주제/ending/frame_673.png\n",
            "\n",
            "0: 384x640 1 person, 11.4ms\n",
            "Speed: 3.6ms preprocess, 11.4ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 674: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 674 saved to /content/drive/MyDrive/논문주제/ending/frame_674.png\n",
            "\n",
            "0: 384x640 1 person, 9.1ms\n",
            "Speed: 2.8ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 675: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 675 saved to /content/drive/MyDrive/논문주제/ending/frame_675.png\n",
            "\n",
            "0: 384x640 1 person, 8.4ms\n",
            "Speed: 3.0ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 676: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 676 saved to /content/drive/MyDrive/논문주제/ending/frame_676.png\n",
            "\n",
            "0: 384x640 1 person, 11.0ms\n",
            "Speed: 2.7ms preprocess, 11.0ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 677: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 677 saved to /content/drive/MyDrive/논문주제/ending/frame_677.png\n",
            "\n",
            "0: 384x640 1 person, 10.9ms\n",
            "Speed: 3.4ms preprocess, 10.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 678: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 678 saved to /content/drive/MyDrive/논문주제/ending/frame_678.png\n",
            "\n",
            "0: 384x640 1 person, 8.4ms\n",
            "Speed: 2.6ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 679: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 679 saved to /content/drive/MyDrive/논문주제/ending/frame_679.png\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 3.0ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 680: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 680 saved to /content/drive/MyDrive/논문주제/ending/frame_680.png\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 2.4ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 681: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 681 saved to /content/drive/MyDrive/논문주제/ending/frame_681.png\n",
            "\n",
            "0: 384x640 1 person, 7.7ms\n",
            "Speed: 3.3ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 682: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 682 saved to /content/drive/MyDrive/논문주제/ending/frame_682.png\n",
            "\n",
            "0: 384x640 1 person, 11.0ms\n",
            "Speed: 3.5ms preprocess, 11.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 683: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 683 saved to /content/drive/MyDrive/논문주제/ending/frame_683.png\n",
            "\n",
            "0: 384x640 1 person, 11.4ms\n",
            "Speed: 3.4ms preprocess, 11.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 684: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 684 saved to /content/drive/MyDrive/논문주제/ending/frame_684.png\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 3.2ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 685: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 685 saved to /content/drive/MyDrive/논문주제/ending/frame_685.png\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 2.5ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 686: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 686 saved to /content/drive/MyDrive/논문주제/ending/frame_686.png\n",
            "\n",
            "0: 384x640 1 person, 10.0ms\n",
            "Speed: 3.4ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 687: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 687 saved to /content/drive/MyDrive/논문주제/ending/frame_687.png\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 2.8ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 688: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 688 saved to /content/drive/MyDrive/논문주제/ending/frame_688.png\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 2.7ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 689: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 689 saved to /content/drive/MyDrive/논문주제/ending/frame_689.png\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 3.3ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 690: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 690 saved to /content/drive/MyDrive/논문주제/ending/frame_690.png\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 2.9ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 691: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 691 saved to /content/drive/MyDrive/논문주제/ending/frame_691.png\n",
            "\n",
            "0: 384x640 1 person, 8.2ms\n",
            "Speed: 2.9ms preprocess, 8.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 692: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 692 saved to /content/drive/MyDrive/논문주제/ending/frame_692.png\n",
            "\n",
            "0: 384x640 1 person, 7.1ms\n",
            "Speed: 2.5ms preprocess, 7.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 693: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 693 saved to /content/drive/MyDrive/논문주제/ending/frame_693.png\n",
            "\n",
            "0: 384x640 1 person, 7.8ms\n",
            "Speed: 2.8ms preprocess, 7.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 694: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 694 saved to /content/drive/MyDrive/논문주제/ending/frame_694.png\n",
            "\n",
            "0: 384x640 1 person, 11.0ms\n",
            "Speed: 3.5ms preprocess, 11.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 695: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 695 saved to /content/drive/MyDrive/논문주제/ending/frame_695.png\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 3.1ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 696: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 696 saved to /content/drive/MyDrive/논문주제/ending/frame_696.png\n",
            "\n",
            "0: 384x640 1 person, 7.9ms\n",
            "Speed: 2.5ms preprocess, 7.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 697: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 697 saved to /content/drive/MyDrive/논문주제/ending/frame_697.png\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 2.9ms preprocess, 7.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 698: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 698 saved to /content/drive/MyDrive/논문주제/ending/frame_698.png\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 2.4ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 699: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 699 saved to /content/drive/MyDrive/논문주제/ending/frame_699.png\n",
            "\n",
            "0: 384x640 1 person, 7.7ms\n",
            "Speed: 2.7ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 700: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 700 saved to /content/drive/MyDrive/논문주제/ending/frame_700.png\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 3.1ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 701: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 701 saved to /content/drive/MyDrive/논문주제/ending/frame_701.png\n",
            "\n",
            "0: 384x640 1 person, 11.4ms\n",
            "Speed: 3.5ms preprocess, 11.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 702: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 702 saved to /content/drive/MyDrive/논문주제/ending/frame_702.png\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 3.6ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 703: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 703 saved to /content/drive/MyDrive/논문주제/ending/frame_703.png\n",
            "\n",
            "0: 384x640 1 person, 7.8ms\n",
            "Speed: 2.7ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 704: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 704 saved to /content/drive/MyDrive/논문주제/ending/frame_704.png\n",
            "\n",
            "0: 384x640 1 person, 7.9ms\n",
            "Speed: 2.6ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 705: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 705 saved to /content/drive/MyDrive/논문주제/ending/frame_705.png\n",
            "\n",
            "0: 384x640 1 person, 11.1ms\n",
            "Speed: 3.6ms preprocess, 11.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 706: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 706 saved to /content/drive/MyDrive/논문주제/ending/frame_706.png\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 3.2ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 707: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 707 saved to /content/drive/MyDrive/논문주제/ending/frame_707.png\n",
            "\n",
            "0: 384x640 2 persons, 11.0ms\n",
            "Speed: 3.5ms preprocess, 11.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 708 saved to /content/drive/MyDrive/논문주제/ending/frame_708.png\n",
            "\n",
            "0: 384x640 1 person, 8.4ms\n",
            "Speed: 3.3ms preprocess, 8.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 709: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 709 saved to /content/drive/MyDrive/논문주제/ending/frame_709.png\n",
            "\n",
            "0: 384x640 2 persons, 7.0ms\n",
            "Speed: 3.1ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 710 saved to /content/drive/MyDrive/논문주제/ending/frame_710.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.9ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 711 saved to /content/drive/MyDrive/논문주제/ending/frame_711.png\n",
            "\n",
            "0: 384x640 2 persons, 8.2ms\n",
            "Speed: 2.5ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 712 saved to /content/drive/MyDrive/논문주제/ending/frame_712.png\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 713: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 713 saved to /content/drive/MyDrive/논문주제/ending/frame_713.png\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 2.4ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 714: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 714 saved to /content/drive/MyDrive/논문주제/ending/frame_714.png\n",
            "\n",
            "0: 384x640 1 person, 7.2ms\n",
            "Speed: 3.2ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 715: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 715 saved to /content/drive/MyDrive/논문주제/ending/frame_715.png\n",
            "\n",
            "0: 384x640 1 person, 7.7ms\n",
            "Speed: 2.5ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 716: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 716 saved to /content/drive/MyDrive/논문주제/ending/frame_716.png\n",
            "\n",
            "0: 384x640 1 person, 9.1ms\n",
            "Speed: 3.2ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 717: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 717 saved to /content/drive/MyDrive/논문주제/ending/frame_717.png\n",
            "\n",
            "0: 384x640 2 persons, 7.9ms\n",
            "Speed: 2.7ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 718 saved to /content/drive/MyDrive/논문주제/ending/frame_718.png\n",
            "\n",
            "0: 384x640 1 person, 7.1ms\n",
            "Speed: 3.5ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 719: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 719 saved to /content/drive/MyDrive/논문주제/ending/frame_719.png\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 3.4ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 720: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 720 saved to /content/drive/MyDrive/논문주제/ending/frame_720.png\n",
            "\n",
            "0: 384x640 1 person, 8.2ms\n",
            "Speed: 2.6ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 721: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 721 saved to /content/drive/MyDrive/논문주제/ending/frame_721.png\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 2.4ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 722: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 722 saved to /content/drive/MyDrive/논문주제/ending/frame_722.png\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 2.9ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 723: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 723 saved to /content/drive/MyDrive/논문주제/ending/frame_723.png\n",
            "\n",
            "0: 384x640 1 person, 8.3ms\n",
            "Speed: 3.3ms preprocess, 8.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 724: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 724 saved to /content/drive/MyDrive/논문주제/ending/frame_724.png\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 2.9ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 725: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 725 saved to /content/drive/MyDrive/논문주제/ending/frame_725.png\n",
            "\n",
            "0: 384x640 1 person, 8.0ms\n",
            "Speed: 2.8ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 726: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 726 saved to /content/drive/MyDrive/논문주제/ending/frame_726.png\n",
            "\n",
            "0: 384x640 1 person, 8.0ms\n",
            "Speed: 3.4ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 727: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 727 saved to /content/drive/MyDrive/논문주제/ending/frame_727.png\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 2.9ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 728: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 728 saved to /content/drive/MyDrive/논문주제/ending/frame_728.png\n",
            "\n",
            "0: 384x640 1 person, 11.1ms\n",
            "Speed: 3.5ms preprocess, 11.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 729: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 729 saved to /content/drive/MyDrive/논문주제/ending/frame_729.png\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 3.0ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 730: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 730 saved to /content/drive/MyDrive/논문주제/ending/frame_730.png\n",
            "\n",
            "0: 384x640 1 person, 8.1ms\n",
            "Speed: 2.6ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 731: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 731 saved to /content/drive/MyDrive/논문주제/ending/frame_731.png\n",
            "\n",
            "0: 384x640 1 person, 10.9ms\n",
            "Speed: 3.4ms preprocess, 10.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 732: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 732 saved to /content/drive/MyDrive/논문주제/ending/frame_732.png\n",
            "\n",
            "0: 384x640 1 person, 8.2ms\n",
            "Speed: 2.5ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 733: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 733 saved to /content/drive/MyDrive/논문주제/ending/frame_733.png\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 734: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 734 saved to /content/drive/MyDrive/논문주제/ending/frame_734.png\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 2.6ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 735: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 735 saved to /content/drive/MyDrive/논문주제/ending/frame_735.png\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 2.9ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 736: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 736 saved to /content/drive/MyDrive/논문주제/ending/frame_736.png\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 737: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 737 saved to /content/drive/MyDrive/논문주제/ending/frame_737.png\n",
            "\n",
            "0: 384x640 1 person, 12.3ms\n",
            "Speed: 3.4ms preprocess, 12.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 738: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 738 saved to /content/drive/MyDrive/논문주제/ending/frame_738.png\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 3.0ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 739: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 739 saved to /content/drive/MyDrive/논문주제/ending/frame_739.png\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 2.6ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 740: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 740 saved to /content/drive/MyDrive/논문주제/ending/frame_740.png\n",
            "\n",
            "0: 384x640 1 person, 8.0ms\n",
            "Speed: 2.8ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 741: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 741 saved to /content/drive/MyDrive/논문주제/ending/frame_741.png\n",
            "\n",
            "0: 384x640 1 person, 8.5ms\n",
            "Speed: 3.4ms preprocess, 8.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 742: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 742 saved to /content/drive/MyDrive/논문주제/ending/frame_742.png\n",
            "\n",
            "0: 384x640 1 person, 7.2ms\n",
            "Speed: 2.8ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 743: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 743 saved to /content/drive/MyDrive/논문주제/ending/frame_743.png\n",
            "\n",
            "0: 384x640 1 person, 8.3ms\n",
            "Speed: 3.0ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 744: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 744 saved to /content/drive/MyDrive/논문주제/ending/frame_744.png\n",
            "\n",
            "0: 384x640 1 person, 7.8ms\n",
            "Speed: 2.8ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 745: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 745 saved to /content/drive/MyDrive/논문주제/ending/frame_745.png\n",
            "\n",
            "0: 384x640 1 person, 7.9ms\n",
            "Speed: 2.9ms preprocess, 7.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 746: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 746 saved to /content/drive/MyDrive/논문주제/ending/frame_746.png\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 3.1ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 747: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 747 saved to /content/drive/MyDrive/논문주제/ending/frame_747.png\n",
            "\n",
            "0: 384x640 1 person, 7.7ms\n",
            "Speed: 2.6ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 748: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 748 saved to /content/drive/MyDrive/논문주제/ending/frame_748.png\n",
            "\n",
            "0: 384x640 1 person, 9.0ms\n",
            "Speed: 3.6ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 749: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 749 saved to /content/drive/MyDrive/논문주제/ending/frame_749.png\n",
            "\n",
            "0: 384x640 1 person, 7.9ms\n",
            "Speed: 2.9ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 750: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 750 saved to /content/drive/MyDrive/논문주제/ending/frame_750.png\n",
            "\n",
            "0: 384x640 1 person, 7.7ms\n",
            "Speed: 2.8ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 751: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 751 saved to /content/drive/MyDrive/논문주제/ending/frame_751.png\n",
            "\n",
            "0: 384x640 1 person, 7.7ms\n",
            "Speed: 2.6ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 752: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 752 saved to /content/drive/MyDrive/논문주제/ending/frame_752.png\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 2.5ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 753: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 753 saved to /content/drive/MyDrive/논문주제/ending/frame_753.png\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 2.6ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 754: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 754 saved to /content/drive/MyDrive/논문주제/ending/frame_754.png\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 3.1ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 755: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 755 saved to /content/drive/MyDrive/논문주제/ending/frame_755.png\n",
            "\n",
            "0: 384x640 1 person, 9.8ms\n",
            "Speed: 3.4ms preprocess, 9.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 756: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 756 saved to /content/drive/MyDrive/논문주제/ending/frame_756.png\n",
            "\n",
            "0: 384x640 1 person, 7.7ms\n",
            "Speed: 2.5ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 757: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 757 saved to /content/drive/MyDrive/논문주제/ending/frame_757.png\n",
            "\n",
            "0: 384x640 1 person, 8.2ms\n",
            "Speed: 2.4ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 758: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 758 saved to /content/drive/MyDrive/논문주제/ending/frame_758.png\n",
            "\n",
            "0: 384x640 1 person, 7.7ms\n",
            "Speed: 2.7ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 759: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 759 saved to /content/drive/MyDrive/논문주제/ending/frame_759.png\n",
            "\n",
            "0: 384x640 1 person, 8.2ms\n",
            "Speed: 2.6ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 760: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 760 saved to /content/drive/MyDrive/논문주제/ending/frame_760.png\n",
            "\n",
            "0: 384x640 1 person, 8.1ms\n",
            "Speed: 2.6ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 761: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 761 saved to /content/drive/MyDrive/논문주제/ending/frame_761.png\n",
            "\n",
            "0: 384x640 1 person, 7.9ms\n",
            "Speed: 2.7ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 762: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 762 saved to /content/drive/MyDrive/논문주제/ending/frame_762.png\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 3.1ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 763: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 763 saved to /content/drive/MyDrive/논문주제/ending/frame_763.png\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 2.5ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 764: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 764 saved to /content/drive/MyDrive/논문주제/ending/frame_764.png\n",
            "\n",
            "0: 384x640 1 person, 7.9ms\n",
            "Speed: 3.1ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 765: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 765 saved to /content/drive/MyDrive/논문주제/ending/frame_765.png\n",
            "\n",
            "0: 384x640 1 person, 7.7ms\n",
            "Speed: 2.4ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 766: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 766 saved to /content/drive/MyDrive/논문주제/ending/frame_766.png\n",
            "\n",
            "0: 384x640 1 person, 11.2ms\n",
            "Speed: 3.6ms preprocess, 11.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 767: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 767 saved to /content/drive/MyDrive/논문주제/ending/frame_767.png\n",
            "\n",
            "0: 384x640 1 person, 11.2ms\n",
            "Speed: 3.4ms preprocess, 11.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 768: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 768 saved to /content/drive/MyDrive/논문주제/ending/frame_768.png\n",
            "\n",
            "0: 384x640 1 person, 7.8ms\n",
            "Speed: 2.8ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 769: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 769 saved to /content/drive/MyDrive/논문주제/ending/frame_769.png\n",
            "\n",
            "0: 384x640 1 person, 7.7ms\n",
            "Speed: 2.7ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 770: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 770 saved to /content/drive/MyDrive/논문주제/ending/frame_770.png\n",
            "\n",
            "0: 384x640 1 person, 7.7ms\n",
            "Speed: 3.1ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 771: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 771 saved to /content/drive/MyDrive/논문주제/ending/frame_771.png\n",
            "\n",
            "0: 384x640 1 person, 7.8ms\n",
            "Speed: 2.8ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 772: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 772 saved to /content/drive/MyDrive/논문주제/ending/frame_772.png\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 2.5ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 773: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 773 saved to /content/drive/MyDrive/논문주제/ending/frame_773.png\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 2.4ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 774: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 774 saved to /content/drive/MyDrive/논문주제/ending/frame_774.png\n",
            "\n",
            "0: 384x640 1 person, 8.1ms\n",
            "Speed: 3.1ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 775: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 775 saved to /content/drive/MyDrive/논문주제/ending/frame_775.png\n",
            "\n",
            "0: 384x640 1 person, 10.2ms\n",
            "Speed: 3.3ms preprocess, 10.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 776: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 776 saved to /content/drive/MyDrive/논문주제/ending/frame_776.png\n",
            "\n",
            "0: 384x640 1 person, 7.2ms\n",
            "Speed: 3.3ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 777: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 777 saved to /content/drive/MyDrive/논문주제/ending/frame_777.png\n",
            "\n",
            "0: 384x640 1 person, 10.6ms\n",
            "Speed: 3.1ms preprocess, 10.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 778: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 778 saved to /content/drive/MyDrive/논문주제/ending/frame_778.png\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 3.0ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 779: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 779 saved to /content/drive/MyDrive/논문주제/ending/frame_779.png\n",
            "\n",
            "0: 384x640 1 person, 7.7ms\n",
            "Speed: 3.1ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 780: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 780 saved to /content/drive/MyDrive/논문주제/ending/frame_780.png\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 2.8ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 781: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 781 saved to /content/drive/MyDrive/논문주제/ending/frame_781.png\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 2.7ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 782: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 782 saved to /content/drive/MyDrive/논문주제/ending/frame_782.png\n",
            "\n",
            "0: 384x640 1 person, 8.1ms\n",
            "Speed: 3.3ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 783: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 783 saved to /content/drive/MyDrive/논문주제/ending/frame_783.png\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 3.0ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 784: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 784 saved to /content/drive/MyDrive/논문주제/ending/frame_784.png\n",
            "\n",
            "0: 384x640 1 person, 7.7ms\n",
            "Speed: 2.6ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 785: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 785 saved to /content/drive/MyDrive/논문주제/ending/frame_785.png\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 2.7ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 786: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 786 saved to /content/drive/MyDrive/논문주제/ending/frame_786.png\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 2.9ms preprocess, 7.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 787: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 787 saved to /content/drive/MyDrive/논문주제/ending/frame_787.png\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 2.6ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 788: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 788 saved to /content/drive/MyDrive/논문주제/ending/frame_788.png\n",
            "\n",
            "0: 384x640 1 person, 8.5ms\n",
            "Speed: 2.9ms preprocess, 8.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 789: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 789 saved to /content/drive/MyDrive/논문주제/ending/frame_789.png\n",
            "\n",
            "0: 384x640 1 person, 7.7ms\n",
            "Speed: 2.7ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 790: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 790 saved to /content/drive/MyDrive/논문주제/ending/frame_790.png\n",
            "\n",
            "0: 384x640 1 person, 7.8ms\n",
            "Speed: 3.1ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 791: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 791 saved to /content/drive/MyDrive/논문주제/ending/frame_791.png\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 792: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 792 saved to /content/drive/MyDrive/논문주제/ending/frame_792.png\n",
            "\n",
            "0: 384x640 1 person, 11.5ms\n",
            "Speed: 3.3ms preprocess, 11.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 793: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 793 saved to /content/drive/MyDrive/논문주제/ending/frame_793.png\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 2.6ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 794: 한 명 감지됨. 이미지 저장만 진행합니다.\n",
            "Frame 794 saved to /content/drive/MyDrive/논문주제/ending/frame_794.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.4ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 795 saved to /content/drive/MyDrive/논문주제/ending/frame_795.png\n",
            "\n",
            "0: 384x640 2 persons, 8.6ms\n",
            "Speed: 2.9ms preprocess, 8.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 796 saved to /content/drive/MyDrive/논문주제/ending/frame_796.png\n",
            "\n",
            "0: 384x640 2 persons, 8.1ms\n",
            "Speed: 3.2ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 797 saved to /content/drive/MyDrive/논문주제/ending/frame_797.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.6ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 798 saved to /content/drive/MyDrive/논문주제/ending/frame_798.png\n",
            "\n",
            "0: 384x640 2 persons, 8.0ms\n",
            "Speed: 2.7ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 799 saved to /content/drive/MyDrive/논문주제/ending/frame_799.png\n",
            "\n",
            "0: 384x640 2 persons, 7.9ms\n",
            "Speed: 2.5ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 800 saved to /content/drive/MyDrive/논문주제/ending/frame_800.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 801 saved to /content/drive/MyDrive/논문주제/ending/frame_801.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.9ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 802 saved to /content/drive/MyDrive/논문주제/ending/frame_802.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.6ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 803 saved to /content/drive/MyDrive/논문주제/ending/frame_803.png\n",
            "\n",
            "0: 384x640 2 persons, 8.5ms\n",
            "Speed: 2.9ms preprocess, 8.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 804 saved to /content/drive/MyDrive/논문주제/ending/frame_804.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 3.1ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 805 saved to /content/drive/MyDrive/논문주제/ending/frame_805.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 3.2ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 806 saved to /content/drive/MyDrive/논문주제/ending/frame_806.png\n",
            "\n",
            "0: 384x640 2 persons, 8.3ms\n",
            "Speed: 2.6ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 807 saved to /content/drive/MyDrive/논문주제/ending/frame_807.png\n",
            "\n",
            "0: 384x640 2 persons, 7.9ms\n",
            "Speed: 2.9ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 808 saved to /content/drive/MyDrive/논문주제/ending/frame_808.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 3.0ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 809 saved to /content/drive/MyDrive/논문주제/ending/frame_809.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 2.6ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 810 saved to /content/drive/MyDrive/논문주제/ending/frame_810.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 3.1ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 811 saved to /content/drive/MyDrive/논문주제/ending/frame_811.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 3.0ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 812 saved to /content/drive/MyDrive/논문주제/ending/frame_812.png\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.6ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 813 saved to /content/drive/MyDrive/논문주제/ending/frame_813.png\n",
            "\n",
            "0: 384x640 2 persons, 10.8ms\n",
            "Speed: 3.4ms preprocess, 10.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 814 saved to /content/drive/MyDrive/논문주제/ending/frame_814.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 815 saved to /content/drive/MyDrive/논문주제/ending/frame_815.png\n",
            "\n",
            "0: 384x640 2 persons, 13.1ms\n",
            "Speed: 3.3ms preprocess, 13.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 816 saved to /content/drive/MyDrive/논문주제/ending/frame_816.png\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.4ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 817 saved to /content/drive/MyDrive/논문주제/ending/frame_817.png\n",
            "\n",
            "0: 384x640 2 persons, 8.0ms\n",
            "Speed: 2.7ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 818 saved to /content/drive/MyDrive/논문주제/ending/frame_818.png\n",
            "\n",
            "0: 384x640 2 persons, 8.3ms\n",
            "Speed: 2.5ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 819 saved to /content/drive/MyDrive/논문주제/ending/frame_819.png\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.4ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 820 saved to /content/drive/MyDrive/논문주제/ending/frame_820.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 3.0ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 821 saved to /content/drive/MyDrive/논문주제/ending/frame_821.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 3.0ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 822 saved to /content/drive/MyDrive/논문주제/ending/frame_822.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 3.0ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 823 saved to /content/drive/MyDrive/논문주제/ending/frame_823.png\n",
            "\n",
            "0: 384x640 2 persons, 7.9ms\n",
            "Speed: 3.2ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 824 saved to /content/drive/MyDrive/논문주제/ending/frame_824.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.5ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 825 saved to /content/drive/MyDrive/논문주제/ending/frame_825.png\n",
            "\n",
            "0: 384x640 2 persons, 11.3ms\n",
            "Speed: 3.3ms preprocess, 11.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 826 saved to /content/drive/MyDrive/논문주제/ending/frame_826.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 3.1ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 827 saved to /content/drive/MyDrive/논문주제/ending/frame_827.png\n",
            "\n",
            "0: 384x640 2 persons, 8.3ms\n",
            "Speed: 3.4ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 828 saved to /content/drive/MyDrive/논문주제/ending/frame_828.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 2.7ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 829 saved to /content/drive/MyDrive/논문주제/ending/frame_829.png\n",
            "\n",
            "0: 384x640 2 persons, 8.2ms\n",
            "Speed: 3.1ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 830 saved to /content/drive/MyDrive/논문주제/ending/frame_830.png\n",
            "\n",
            "0: 384x640 2 persons, 8.1ms\n",
            "Speed: 3.0ms preprocess, 8.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 831 saved to /content/drive/MyDrive/논문주제/ending/frame_831.png\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 3.2ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 832 saved to /content/drive/MyDrive/논문주제/ending/frame_832.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 2.5ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 833 saved to /content/drive/MyDrive/논문주제/ending/frame_833.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 3.4ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 834 saved to /content/drive/MyDrive/논문주제/ending/frame_834.png\n",
            "\n",
            "0: 384x640 2 persons, 10.9ms\n",
            "Speed: 3.5ms preprocess, 10.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 835 saved to /content/drive/MyDrive/논문주제/ending/frame_835.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 3.0ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 836 saved to /content/drive/MyDrive/논문주제/ending/frame_836.png\n",
            "\n",
            "0: 384x640 2 persons, 8.1ms\n",
            "Speed: 2.9ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 837 saved to /content/drive/MyDrive/논문주제/ending/frame_837.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.9ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 838 saved to /content/drive/MyDrive/논문주제/ending/frame_838.png\n",
            "\n",
            "0: 384x640 2 persons, 12.3ms\n",
            "Speed: 3.5ms preprocess, 12.3ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 839 saved to /content/drive/MyDrive/논문주제/ending/frame_839.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 3.2ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 840 saved to /content/drive/MyDrive/논문주제/ending/frame_840.png\n",
            "\n",
            "0: 384x640 2 persons, 7.9ms\n",
            "Speed: 3.0ms preprocess, 7.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 841 saved to /content/drive/MyDrive/논문주제/ending/frame_841.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 842 saved to /content/drive/MyDrive/논문주제/ending/frame_842.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.6ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 843 saved to /content/drive/MyDrive/논문주제/ending/frame_843.png\n",
            "\n",
            "0: 384x640 2 persons, 8.4ms\n",
            "Speed: 3.1ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 844 saved to /content/drive/MyDrive/논문주제/ending/frame_844.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.9ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 845 saved to /content/drive/MyDrive/논문주제/ending/frame_845.png\n",
            "\n",
            "0: 384x640 2 persons, 8.0ms\n",
            "Speed: 3.1ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 846 saved to /content/drive/MyDrive/논문주제/ending/frame_846.png\n",
            "\n",
            "0: 384x640 2 persons, 11.0ms\n",
            "Speed: 3.4ms preprocess, 11.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 847 saved to /content/drive/MyDrive/논문주제/ending/frame_847.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.6ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 848 saved to /content/drive/MyDrive/논문주제/ending/frame_848.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.8ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 849 saved to /content/drive/MyDrive/논문주제/ending/frame_849.png\n",
            "\n",
            "0: 384x640 2 persons, 8.0ms\n",
            "Speed: 2.5ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 850 saved to /content/drive/MyDrive/논문주제/ending/frame_850.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.5ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 851 saved to /content/drive/MyDrive/논문주제/ending/frame_851.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.5ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 852 saved to /content/drive/MyDrive/논문주제/ending/frame_852.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 3.0ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 853 saved to /content/drive/MyDrive/논문주제/ending/frame_853.png\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 3.6ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 854 saved to /content/drive/MyDrive/논문주제/ending/frame_854.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.5ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 855 saved to /content/drive/MyDrive/논문주제/ending/frame_855.png\n",
            "\n",
            "0: 384x640 2 persons, 11.3ms\n",
            "Speed: 3.4ms preprocess, 11.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 856 saved to /content/drive/MyDrive/논문주제/ending/frame_856.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 857 saved to /content/drive/MyDrive/논문주제/ending/frame_857.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.6ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 858 saved to /content/drive/MyDrive/논문주제/ending/frame_858.png\n",
            "\n",
            "0: 384x640 2 persons, 7.9ms\n",
            "Speed: 2.7ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 859 saved to /content/drive/MyDrive/논문주제/ending/frame_859.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 3.7ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 860 saved to /content/drive/MyDrive/논문주제/ending/frame_860.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 3.3ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 861 saved to /content/drive/MyDrive/논문주제/ending/frame_861.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 2.7ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 862 saved to /content/drive/MyDrive/논문주제/ending/frame_862.png\n",
            "\n",
            "0: 384x640 2 persons, 7.9ms\n",
            "Speed: 3.2ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 863 saved to /content/drive/MyDrive/논문주제/ending/frame_863.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 3.2ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 864 saved to /content/drive/MyDrive/논문주제/ending/frame_864.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.5ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 865 saved to /content/drive/MyDrive/논문주제/ending/frame_865.png\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.8ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 866 saved to /content/drive/MyDrive/논문주제/ending/frame_866.png\n",
            "\n",
            "0: 384x640 2 persons, 8.0ms\n",
            "Speed: 2.7ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 867 saved to /content/drive/MyDrive/논문주제/ending/frame_867.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 2.6ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 868 saved to /content/drive/MyDrive/논문주제/ending/frame_868.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.9ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 869 saved to /content/drive/MyDrive/논문주제/ending/frame_869.png\n",
            "\n",
            "0: 384x640 2 persons, 11.9ms\n",
            "Speed: 3.4ms preprocess, 11.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 870 saved to /content/drive/MyDrive/논문주제/ending/frame_870.png\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.7ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 871 saved to /content/drive/MyDrive/논문주제/ending/frame_871.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.9ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 872 saved to /content/drive/MyDrive/논문주제/ending/frame_872.png\n",
            "\n",
            "0: 384x640 2 persons, 8.1ms\n",
            "Speed: 2.4ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 873 saved to /content/drive/MyDrive/논문주제/ending/frame_873.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 3.1ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 874 saved to /content/drive/MyDrive/논문주제/ending/frame_874.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.9ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 875 saved to /content/drive/MyDrive/논문주제/ending/frame_875.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.9ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 876 saved to /content/drive/MyDrive/논문주제/ending/frame_876.png\n",
            "\n",
            "0: 384x640 2 persons, 11.0ms\n",
            "Speed: 3.3ms preprocess, 11.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 877 saved to /content/drive/MyDrive/논문주제/ending/frame_877.png\n",
            "\n",
            "0: 384x640 2 persons, 8.1ms\n",
            "Speed: 3.2ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 878 saved to /content/drive/MyDrive/논문주제/ending/frame_878.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 879 saved to /content/drive/MyDrive/논문주제/ending/frame_879.png\n",
            "\n",
            "0: 384x640 2 persons, 8.3ms\n",
            "Speed: 2.4ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 880 saved to /content/drive/MyDrive/논문주제/ending/frame_880.png\n",
            "\n",
            "0: 384x640 2 persons, 9.7ms\n",
            "Speed: 2.7ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 881 saved to /content/drive/MyDrive/논문주제/ending/frame_881.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.8ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 882 saved to /content/drive/MyDrive/논문주제/ending/frame_882.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 2.9ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 883 saved to /content/drive/MyDrive/논문주제/ending/frame_883.png\n",
            "\n",
            "0: 384x640 2 persons, 8.0ms\n",
            "Speed: 2.7ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 884 saved to /content/drive/MyDrive/논문주제/ending/frame_884.png\n",
            "\n",
            "0: 384x640 2 persons, 8.2ms\n",
            "Speed: 3.1ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 885 saved to /content/drive/MyDrive/논문주제/ending/frame_885.png\n",
            "\n",
            "0: 384x640 2 persons, 8.1ms\n",
            "Speed: 2.8ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 886 saved to /content/drive/MyDrive/논문주제/ending/frame_886.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.7ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 887 saved to /content/drive/MyDrive/논문주제/ending/frame_887.png\n",
            "\n",
            "0: 384x640 2 persons, 10.9ms\n",
            "Speed: 3.3ms preprocess, 10.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 888 saved to /content/drive/MyDrive/논문주제/ending/frame_888.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 3.2ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 889 saved to /content/drive/MyDrive/논문주제/ending/frame_889.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 2.5ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 890 saved to /content/drive/MyDrive/논문주제/ending/frame_890.png\n",
            "\n",
            "0: 384x640 2 persons, 8.2ms\n",
            "Speed: 3.4ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 891 saved to /content/drive/MyDrive/논문주제/ending/frame_891.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.6ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 892 saved to /content/drive/MyDrive/논문주제/ending/frame_892.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.6ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 893 saved to /content/drive/MyDrive/논문주제/ending/frame_893.png\n",
            "\n",
            "0: 384x640 2 persons, 8.2ms\n",
            "Speed: 2.7ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 894 saved to /content/drive/MyDrive/논문주제/ending/frame_894.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 895 saved to /content/drive/MyDrive/논문주제/ending/frame_895.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.6ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 896 saved to /content/drive/MyDrive/논문주제/ending/frame_896.png\n",
            "\n",
            "0: 384x640 2 persons, 8.4ms\n",
            "Speed: 3.1ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 897 saved to /content/drive/MyDrive/논문주제/ending/frame_897.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.9ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 898 saved to /content/drive/MyDrive/논문주제/ending/frame_898.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 2.7ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 899 saved to /content/drive/MyDrive/논문주제/ending/frame_899.png\n",
            "\n",
            "0: 384x640 2 persons, 9.6ms\n",
            "Speed: 2.6ms preprocess, 9.6ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 900 saved to /content/drive/MyDrive/논문주제/ending/frame_900.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 901 saved to /content/drive/MyDrive/논문주제/ending/frame_901.png\n",
            "\n",
            "0: 384x640 2 persons, 8.7ms\n",
            "Speed: 2.7ms preprocess, 8.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 902 saved to /content/drive/MyDrive/논문주제/ending/frame_902.png\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.5ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 903 saved to /content/drive/MyDrive/논문주제/ending/frame_903.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.6ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 904 saved to /content/drive/MyDrive/논문주제/ending/frame_904.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 3.1ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 905 saved to /content/drive/MyDrive/논문주제/ending/frame_905.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.9ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 906 saved to /content/drive/MyDrive/논문주제/ending/frame_906.png\n",
            "\n",
            "0: 384x640 2 persons, 8.1ms\n",
            "Speed: 2.8ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 907 saved to /content/drive/MyDrive/논문주제/ending/frame_907.png\n",
            "\n",
            "0: 384x640 2 persons, 10.6ms\n",
            "Speed: 3.4ms preprocess, 10.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 908 saved to /content/drive/MyDrive/논문주제/ending/frame_908.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 3.4ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 909 saved to /content/drive/MyDrive/논문주제/ending/frame_909.png\n",
            "\n",
            "0: 384x640 2 persons, 11.9ms\n",
            "Speed: 3.6ms preprocess, 11.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 910 saved to /content/drive/MyDrive/논문주제/ending/frame_910.png\n",
            "\n",
            "0: 384x640 2 persons, 8.6ms\n",
            "Speed: 3.0ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 911 saved to /content/drive/MyDrive/논문주제/ending/frame_911.png\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 3.0ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 912 saved to /content/drive/MyDrive/논문주제/ending/frame_912.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 3.0ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 913 saved to /content/drive/MyDrive/논문주제/ending/frame_913.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 914 saved to /content/drive/MyDrive/논문주제/ending/frame_914.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 3.7ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 915 saved to /content/drive/MyDrive/논문주제/ending/frame_915.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 3.1ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 916 saved to /content/drive/MyDrive/논문주제/ending/frame_916.png\n",
            "\n",
            "0: 384x640 2 persons, 9.1ms\n",
            "Speed: 2.9ms preprocess, 9.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 917 saved to /content/drive/MyDrive/논문주제/ending/frame_917.png\n",
            "\n",
            "0: 384x640 2 persons, 10.8ms\n",
            "Speed: 4.0ms preprocess, 10.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 918 saved to /content/drive/MyDrive/논문주제/ending/frame_918.png\n",
            "\n",
            "0: 384x640 2 persons, 7.0ms\n",
            "Speed: 2.7ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 919 saved to /content/drive/MyDrive/논문주제/ending/frame_919.png\n",
            "\n",
            "0: 384x640 2 persons, 11.2ms\n",
            "Speed: 3.5ms preprocess, 11.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 920 saved to /content/drive/MyDrive/논문주제/ending/frame_920.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 3.2ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 921 saved to /content/drive/MyDrive/논문주제/ending/frame_921.png\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 922 saved to /content/drive/MyDrive/논문주제/ending/frame_922.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.6ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 923 saved to /content/drive/MyDrive/논문주제/ending/frame_923.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.9ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 924 saved to /content/drive/MyDrive/논문주제/ending/frame_924.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 2.5ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 925 saved to /content/drive/MyDrive/논문주제/ending/frame_925.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 2.7ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 926 saved to /content/drive/MyDrive/논문주제/ending/frame_926.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 2.7ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 927 saved to /content/drive/MyDrive/논문주제/ending/frame_927.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 3.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 928 saved to /content/drive/MyDrive/논문주제/ending/frame_928.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 3.0ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 929 saved to /content/drive/MyDrive/논문주제/ending/frame_929.png\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.9ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 930 saved to /content/drive/MyDrive/논문주제/ending/frame_930.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 2.7ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 931 saved to /content/drive/MyDrive/논문주제/ending/frame_931.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.8ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 932 saved to /content/drive/MyDrive/논문주제/ending/frame_932.png\n",
            "\n",
            "0: 384x640 2 persons, 8.1ms\n",
            "Speed: 2.9ms preprocess, 8.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 933 saved to /content/drive/MyDrive/논문주제/ending/frame_933.png\n",
            "\n",
            "0: 384x640 2 persons, 8.0ms\n",
            "Speed: 2.7ms preprocess, 8.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 934 saved to /content/drive/MyDrive/논문주제/ending/frame_934.png\n",
            "\n",
            "0: 384x640 2 persons, 11.3ms\n",
            "Speed: 3.5ms preprocess, 11.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 935 saved to /content/drive/MyDrive/논문주제/ending/frame_935.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.6ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 936 saved to /content/drive/MyDrive/논문주제/ending/frame_936.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.7ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 937 saved to /content/drive/MyDrive/논문주제/ending/frame_937.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.9ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 938 saved to /content/drive/MyDrive/논문주제/ending/frame_938.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.5ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 939 saved to /content/drive/MyDrive/논문주제/ending/frame_939.png\n",
            "\n",
            "0: 384x640 2 persons, 10.8ms\n",
            "Speed: 3.4ms preprocess, 10.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 940 saved to /content/drive/MyDrive/논문주제/ending/frame_940.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 2.5ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 941 saved to /content/drive/MyDrive/논문주제/ending/frame_941.png\n",
            "\n",
            "0: 384x640 2 persons, 9.0ms\n",
            "Speed: 3.2ms preprocess, 9.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 942 saved to /content/drive/MyDrive/논문주제/ending/frame_942.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 2.8ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 943 saved to /content/drive/MyDrive/논문주제/ending/frame_943.png\n",
            "\n",
            "0: 384x640 2 persons, 8.1ms\n",
            "Speed: 3.4ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 944 saved to /content/drive/MyDrive/논문주제/ending/frame_944.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.6ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 945 saved to /content/drive/MyDrive/논문주제/ending/frame_945.png\n",
            "\n",
            "0: 384x640 2 persons, 7.9ms\n",
            "Speed: 3.4ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 946 saved to /content/drive/MyDrive/논문주제/ending/frame_946.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 947 saved to /content/drive/MyDrive/논문주제/ending/frame_947.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 2.7ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 948 saved to /content/drive/MyDrive/논문주제/ending/frame_948.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.9ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 949 saved to /content/drive/MyDrive/논문주제/ending/frame_949.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 3.1ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 950 saved to /content/drive/MyDrive/논문주제/ending/frame_950.png\n",
            "\n",
            "0: 384x640 2 persons, 8.0ms\n",
            "Speed: 3.1ms preprocess, 8.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 951 saved to /content/drive/MyDrive/논문주제/ending/frame_951.png\n",
            "\n",
            "0: 384x640 2 persons, 8.3ms\n",
            "Speed: 2.9ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 952 saved to /content/drive/MyDrive/논문주제/ending/frame_952.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.9ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 953 saved to /content/drive/MyDrive/논문주제/ending/frame_953.png\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.7ms preprocess, 7.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 954 saved to /content/drive/MyDrive/논문주제/ending/frame_954.png\n",
            "\n",
            "0: 384x640 2 persons, 7.9ms\n",
            "Speed: 2.8ms preprocess, 7.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 955 saved to /content/drive/MyDrive/논문주제/ending/frame_955.png\n",
            "\n",
            "0: 384x640 2 persons, 9.6ms\n",
            "Speed: 3.5ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 956 saved to /content/drive/MyDrive/논문주제/ending/frame_956.png\n",
            "\n",
            "0: 384x640 2 persons, 7.9ms\n",
            "Speed: 2.7ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 957 saved to /content/drive/MyDrive/논문주제/ending/frame_957.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 3.1ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 958 saved to /content/drive/MyDrive/논문주제/ending/frame_958.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.9ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 959 saved to /content/drive/MyDrive/논문주제/ending/frame_959.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.6ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 960 saved to /content/drive/MyDrive/논문주제/ending/frame_960.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 2.7ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 961 saved to /content/drive/MyDrive/논문주제/ending/frame_961.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.5ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 962 saved to /content/drive/MyDrive/논문주제/ending/frame_962.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.5ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 963 saved to /content/drive/MyDrive/논문주제/ending/frame_963.png\n",
            "\n",
            "0: 384x640 2 persons, 9.3ms\n",
            "Speed: 2.6ms preprocess, 9.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 964 saved to /content/drive/MyDrive/논문주제/ending/frame_964.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.4ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 965 saved to /content/drive/MyDrive/논문주제/ending/frame_965.png\n",
            "\n",
            "0: 384x640 2 persons, 8.6ms\n",
            "Speed: 3.3ms preprocess, 8.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 966 saved to /content/drive/MyDrive/논문주제/ending/frame_966.png\n",
            "\n",
            "0: 384x640 2 persons, 7.9ms\n",
            "Speed: 2.6ms preprocess, 7.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 967 saved to /content/drive/MyDrive/논문주제/ending/frame_967.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.6ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 968 saved to /content/drive/MyDrive/논문주제/ending/frame_968.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 2.8ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 969 saved to /content/drive/MyDrive/논문주제/ending/frame_969.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.5ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 970 saved to /content/drive/MyDrive/논문주제/ending/frame_970.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 2.5ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 971 saved to /content/drive/MyDrive/논문주제/ending/frame_971.png\n",
            "\n",
            "0: 384x640 2 persons, 7.9ms\n",
            "Speed: 3.0ms preprocess, 7.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 972 saved to /content/drive/MyDrive/논문주제/ending/frame_972.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.8ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 973 saved to /content/drive/MyDrive/논문주제/ending/frame_973.png\n",
            "\n",
            "0: 384x640 2 persons, 8.1ms\n",
            "Speed: 2.8ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 974 saved to /content/drive/MyDrive/논문주제/ending/frame_974.png\n",
            "\n",
            "0: 384x640 2 persons, 8.2ms\n",
            "Speed: 3.2ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 975 saved to /content/drive/MyDrive/논문주제/ending/frame_975.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 2.7ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 976 saved to /content/drive/MyDrive/논문주제/ending/frame_976.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.6ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 977 saved to /content/drive/MyDrive/논문주제/ending/frame_977.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 978 saved to /content/drive/MyDrive/논문주제/ending/frame_978.png\n",
            "\n",
            "0: 384x640 2 persons, 8.1ms\n",
            "Speed: 2.9ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 979 saved to /content/drive/MyDrive/논문주제/ending/frame_979.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 3.1ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 980 saved to /content/drive/MyDrive/논문주제/ending/frame_980.png\n",
            "\n",
            "0: 384x640 2 persons, 9.8ms\n",
            "Speed: 2.6ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 981 saved to /content/drive/MyDrive/논문주제/ending/frame_981.png\n",
            "\n",
            "0: 384x640 2 persons, 11.2ms\n",
            "Speed: 3.8ms preprocess, 11.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 982 saved to /content/drive/MyDrive/논문주제/ending/frame_982.png\n",
            "\n",
            "0: 384x640 2 persons, 11.0ms\n",
            "Speed: 3.5ms preprocess, 11.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 983 saved to /content/drive/MyDrive/논문주제/ending/frame_983.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 984 saved to /content/drive/MyDrive/논문주제/ending/frame_984.png\n",
            "\n",
            "0: 384x640 2 persons, 8.1ms\n",
            "Speed: 3.2ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 985 saved to /content/drive/MyDrive/논문주제/ending/frame_985.png\n",
            "\n",
            "0: 384x640 2 persons, 10.8ms\n",
            "Speed: 3.5ms preprocess, 10.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 986 saved to /content/drive/MyDrive/논문주제/ending/frame_986.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.6ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 987 saved to /content/drive/MyDrive/논문주제/ending/frame_987.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.8ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 988 saved to /content/drive/MyDrive/논문주제/ending/frame_988.png\n",
            "\n",
            "0: 384x640 2 persons, 7.9ms\n",
            "Speed: 2.6ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 989 saved to /content/drive/MyDrive/논문주제/ending/frame_989.png\n",
            "\n",
            "0: 384x640 2 persons, 10.6ms\n",
            "Speed: 3.4ms preprocess, 10.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 990 saved to /content/drive/MyDrive/논문주제/ending/frame_990.png\n",
            "\n",
            "0: 384x640 2 persons, 10.7ms\n",
            "Speed: 3.4ms preprocess, 10.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 991 saved to /content/drive/MyDrive/논문주제/ending/frame_991.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.5ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 992 saved to /content/drive/MyDrive/논문주제/ending/frame_992.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 3.0ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 993 saved to /content/drive/MyDrive/논문주제/ending/frame_993.png\n",
            "\n",
            "0: 384x640 2 persons, 8.3ms\n",
            "Speed: 2.9ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 994 saved to /content/drive/MyDrive/논문주제/ending/frame_994.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.8ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 995 saved to /content/drive/MyDrive/논문주제/ending/frame_995.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.4ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 996 saved to /content/drive/MyDrive/논문주제/ending/frame_996.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 3.2ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 997 saved to /content/drive/MyDrive/논문주제/ending/frame_997.png\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 3.5ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 998 saved to /content/drive/MyDrive/논문주제/ending/frame_998.png\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.4ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 999 saved to /content/drive/MyDrive/논문주제/ending/frame_999.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 3.1ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1000 saved to /content/drive/MyDrive/논문주제/ending/frame_1000.png\n",
            "\n",
            "0: 384x640 2 persons, 8.5ms\n",
            "Speed: 3.0ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1001 saved to /content/drive/MyDrive/논문주제/ending/frame_1001.png\n",
            "\n",
            "0: 384x640 2 persons, 8.4ms\n",
            "Speed: 2.5ms preprocess, 8.4ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1002 saved to /content/drive/MyDrive/논문주제/ending/frame_1002.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.6ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1003 saved to /content/drive/MyDrive/논문주제/ending/frame_1003.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 2.8ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1004 saved to /content/drive/MyDrive/논문주제/ending/frame_1004.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.8ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1005 saved to /content/drive/MyDrive/논문주제/ending/frame_1005.png\n",
            "\n",
            "0: 384x640 2 persons, 8.2ms\n",
            "Speed: 3.2ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1006 saved to /content/drive/MyDrive/논문주제/ending/frame_1006.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.7ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1007 saved to /content/drive/MyDrive/논문주제/ending/frame_1007.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.3ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1008 saved to /content/drive/MyDrive/논문주제/ending/frame_1008.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 3.0ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1009 saved to /content/drive/MyDrive/논문주제/ending/frame_1009.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.5ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1010 saved to /content/drive/MyDrive/논문주제/ending/frame_1010.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.6ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1011 saved to /content/drive/MyDrive/논문주제/ending/frame_1011.png\n",
            "\n",
            "0: 384x640 2 persons, 8.2ms\n",
            "Speed: 3.1ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1012 saved to /content/drive/MyDrive/논문주제/ending/frame_1012.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.9ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1013 saved to /content/drive/MyDrive/논문주제/ending/frame_1013.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 3.1ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1014 saved to /content/drive/MyDrive/논문주제/ending/frame_1014.png\n",
            "\n",
            "0: 384x640 2 persons, 10.9ms\n",
            "Speed: 3.4ms preprocess, 10.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1015 saved to /content/drive/MyDrive/논문주제/ending/frame_1015.png\n",
            "\n",
            "0: 384x640 2 persons, 7.9ms\n",
            "Speed: 2.8ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1016 saved to /content/drive/MyDrive/논문주제/ending/frame_1016.png\n",
            "\n",
            "0: 384x640 2 persons, 8.0ms\n",
            "Speed: 2.6ms preprocess, 8.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1017 saved to /content/drive/MyDrive/논문주제/ending/frame_1017.png\n",
            "\n",
            "0: 384x640 2 persons, 10.1ms\n",
            "Speed: 3.3ms preprocess, 10.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1018 saved to /content/drive/MyDrive/논문주제/ending/frame_1018.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.5ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1019 saved to /content/drive/MyDrive/논문주제/ending/frame_1019.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.5ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1020 saved to /content/drive/MyDrive/논문주제/ending/frame_1020.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.4ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1021 saved to /content/drive/MyDrive/논문주제/ending/frame_1021.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 3.2ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1022 saved to /content/drive/MyDrive/논문주제/ending/frame_1022.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 3.1ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1023 saved to /content/drive/MyDrive/논문주제/ending/frame_1023.png\n",
            "\n",
            "0: 384x640 2 persons, 8.4ms\n",
            "Speed: 3.5ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1024 saved to /content/drive/MyDrive/논문주제/ending/frame_1024.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.9ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1025 saved to /content/drive/MyDrive/논문주제/ending/frame_1025.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.8ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1026 saved to /content/drive/MyDrive/논문주제/ending/frame_1026.png\n",
            "\n",
            "0: 384x640 2 persons, 8.3ms\n",
            "Speed: 2.7ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1027 saved to /content/drive/MyDrive/논문주제/ending/frame_1027.png\n",
            "\n",
            "0: 384x640 2 persons, 8.1ms\n",
            "Speed: 3.3ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1028 saved to /content/drive/MyDrive/논문주제/ending/frame_1028.png\n",
            "\n",
            "0: 384x640 2 persons, 9.7ms\n",
            "Speed: 2.5ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1029 saved to /content/drive/MyDrive/논문주제/ending/frame_1029.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.6ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1030 saved to /content/drive/MyDrive/논문주제/ending/frame_1030.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.6ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1031 saved to /content/drive/MyDrive/논문주제/ending/frame_1031.png\n",
            "\n",
            "0: 384x640 2 persons, 7.9ms\n",
            "Speed: 3.0ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1032 saved to /content/drive/MyDrive/논문주제/ending/frame_1032.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.9ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1033 saved to /content/drive/MyDrive/논문주제/ending/frame_1033.png\n",
            "\n",
            "0: 384x640 2 persons, 7.9ms\n",
            "Speed: 2.6ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1034 saved to /content/drive/MyDrive/논문주제/ending/frame_1034.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.9ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1035 saved to /content/drive/MyDrive/논문주제/ending/frame_1035.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.7ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1036 saved to /content/drive/MyDrive/논문주제/ending/frame_1036.png\n",
            "\n",
            "0: 384x640 2 persons, 8.4ms\n",
            "Speed: 2.5ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1037 saved to /content/drive/MyDrive/논문주제/ending/frame_1037.png\n",
            "\n",
            "0: 384x640 2 persons, 11.3ms\n",
            "Speed: 3.4ms preprocess, 11.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1038 saved to /content/drive/MyDrive/논문주제/ending/frame_1038.png\n",
            "\n",
            "0: 384x640 2 persons, 8.6ms\n",
            "Speed: 3.3ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1039 saved to /content/drive/MyDrive/논문주제/ending/frame_1039.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.8ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1040 saved to /content/drive/MyDrive/논문주제/ending/frame_1040.png\n",
            "\n",
            "0: 384x640 2 persons, 8.0ms\n",
            "Speed: 2.9ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1041 saved to /content/drive/MyDrive/논문주제/ending/frame_1041.png\n",
            "\n",
            "0: 384x640 2 persons, 8.4ms\n",
            "Speed: 2.7ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1042 saved to /content/drive/MyDrive/논문주제/ending/frame_1042.png\n",
            "\n",
            "0: 384x640 2 persons, 9.3ms\n",
            "Speed: 3.5ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1043 saved to /content/drive/MyDrive/논문주제/ending/frame_1043.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 3.0ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1044 saved to /content/drive/MyDrive/논문주제/ending/frame_1044.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 2.5ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1045 saved to /content/drive/MyDrive/논문주제/ending/frame_1045.png\n",
            "\n",
            "0: 384x640 2 persons, 11.1ms\n",
            "Speed: 3.3ms preprocess, 11.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1046 saved to /content/drive/MyDrive/논문주제/ending/frame_1046.png\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.4ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1047 saved to /content/drive/MyDrive/논문주제/ending/frame_1047.png\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1048 saved to /content/drive/MyDrive/논문주제/ending/frame_1048.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1049 saved to /content/drive/MyDrive/논문주제/ending/frame_1049.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 3.3ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1050 saved to /content/drive/MyDrive/논문주제/ending/frame_1050.png\n",
            "\n",
            "0: 384x640 2 persons, 11.0ms\n",
            "Speed: 3.4ms preprocess, 11.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1051 saved to /content/drive/MyDrive/논문주제/ending/frame_1051.png\n",
            "\n",
            "0: 384x640 2 persons, 8.9ms\n",
            "Speed: 3.5ms preprocess, 8.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1052 saved to /content/drive/MyDrive/논문주제/ending/frame_1052.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.5ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1053 saved to /content/drive/MyDrive/논문주제/ending/frame_1053.png\n",
            "\n",
            "0: 384x640 2 persons, 7.9ms\n",
            "Speed: 2.7ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1054 saved to /content/drive/MyDrive/논문주제/ending/frame_1054.png\n",
            "\n",
            "0: 384x640 2 persons, 10.7ms\n",
            "Speed: 3.4ms preprocess, 10.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1055 saved to /content/drive/MyDrive/논문주제/ending/frame_1055.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.8ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1056 saved to /content/drive/MyDrive/논문주제/ending/frame_1056.png\n",
            "\n",
            "0: 384x640 2 persons, 10.5ms\n",
            "Speed: 3.4ms preprocess, 10.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1057 saved to /content/drive/MyDrive/논문주제/ending/frame_1057.png\n",
            "\n",
            "0: 384x640 2 persons, 10.7ms\n",
            "Speed: 3.4ms preprocess, 10.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1058 saved to /content/drive/MyDrive/논문주제/ending/frame_1058.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 3.0ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1059 saved to /content/drive/MyDrive/논문주제/ending/frame_1059.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.4ms preprocess, 7.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1060 saved to /content/drive/MyDrive/논문주제/ending/frame_1060.png\n",
            "\n",
            "0: 384x640 2 persons, 8.5ms\n",
            "Speed: 2.5ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1061 saved to /content/drive/MyDrive/논문주제/ending/frame_1061.png\n",
            "\n",
            "0: 384x640 2 persons, 8.5ms\n",
            "Speed: 2.3ms preprocess, 8.5ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1062 saved to /content/drive/MyDrive/논문주제/ending/frame_1062.png\n",
            "\n",
            "0: 384x640 2 persons, 9.9ms\n",
            "Speed: 3.3ms preprocess, 9.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1063 saved to /content/drive/MyDrive/논문주제/ending/frame_1063.png\n",
            "\n",
            "0: 384x640 2 persons, 10.2ms\n",
            "Speed: 3.3ms preprocess, 10.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1064 saved to /content/drive/MyDrive/논문주제/ending/frame_1064.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.6ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1065 saved to /content/drive/MyDrive/논문주제/ending/frame_1065.png\n",
            "\n",
            "0: 384x640 2 persons, 13.0ms\n",
            "Speed: 3.4ms preprocess, 13.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1066 saved to /content/drive/MyDrive/논문주제/ending/frame_1066.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1067 saved to /content/drive/MyDrive/논문주제/ending/frame_1067.png\n",
            "\n",
            "0: 384x640 2 persons, 7.9ms\n",
            "Speed: 2.7ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1068 saved to /content/drive/MyDrive/논문주제/ending/frame_1068.png\n",
            "\n",
            "0: 384x640 2 persons, 7.9ms\n",
            "Speed: 2.8ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1069 saved to /content/drive/MyDrive/논문주제/ending/frame_1069.png\n",
            "\n",
            "0: 384x640 2 persons, 8.3ms\n",
            "Speed: 3.0ms preprocess, 8.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1070 saved to /content/drive/MyDrive/논문주제/ending/frame_1070.png\n",
            "\n",
            "0: 384x640 2 persons, 11.3ms\n",
            "Speed: 3.3ms preprocess, 11.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1071 saved to /content/drive/MyDrive/논문주제/ending/frame_1071.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 3.4ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1072 saved to /content/drive/MyDrive/논문주제/ending/frame_1072.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 2.9ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1073 saved to /content/drive/MyDrive/논문주제/ending/frame_1073.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.8ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1074 saved to /content/drive/MyDrive/논문주제/ending/frame_1074.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1075 saved to /content/drive/MyDrive/논문주제/ending/frame_1075.png\n",
            "\n",
            "0: 384x640 2 persons, 10.8ms\n",
            "Speed: 3.5ms preprocess, 10.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1076 saved to /content/drive/MyDrive/논문주제/ending/frame_1076.png\n",
            "\n",
            "0: 384x640 2 persons, 10.8ms\n",
            "Speed: 3.3ms preprocess, 10.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1077 saved to /content/drive/MyDrive/논문주제/ending/frame_1077.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.9ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1078 saved to /content/drive/MyDrive/논문주제/ending/frame_1078.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.8ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1079 saved to /content/drive/MyDrive/논문주제/ending/frame_1079.png\n",
            "\n",
            "0: 384x640 2 persons, 8.5ms\n",
            "Speed: 2.6ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1080 saved to /content/drive/MyDrive/논문주제/ending/frame_1080.png\n",
            "\n",
            "0: 384x640 2 persons, 7.9ms\n",
            "Speed: 2.5ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1081 saved to /content/drive/MyDrive/논문주제/ending/frame_1081.png\n",
            "\n",
            "0: 384x640 2 persons, 8.0ms\n",
            "Speed: 2.5ms preprocess, 8.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1082 saved to /content/drive/MyDrive/논문주제/ending/frame_1082.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 3.1ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1083 saved to /content/drive/MyDrive/논문주제/ending/frame_1083.png\n",
            "\n",
            "0: 384x640 2 persons, 8.2ms\n",
            "Speed: 2.4ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1084 saved to /content/drive/MyDrive/논문주제/ending/frame_1084.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 2.9ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1085 saved to /content/drive/MyDrive/논문주제/ending/frame_1085.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 3.1ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1086 saved to /content/drive/MyDrive/논문주제/ending/frame_1086.png\n",
            "\n",
            "0: 384x640 2 persons, 8.1ms\n",
            "Speed: 2.7ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1087 saved to /content/drive/MyDrive/논문주제/ending/frame_1087.png\n",
            "\n",
            "0: 384x640 2 persons, 10.5ms\n",
            "Speed: 3.4ms preprocess, 10.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1088 saved to /content/drive/MyDrive/논문주제/ending/frame_1088.png\n",
            "\n",
            "0: 384x640 2 persons, 8.2ms\n",
            "Speed: 2.9ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1089 saved to /content/drive/MyDrive/논문주제/ending/frame_1089.png\n",
            "\n",
            "0: 384x640 2 persons, 11.0ms\n",
            "Speed: 3.4ms preprocess, 11.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1090 saved to /content/drive/MyDrive/논문주제/ending/frame_1090.png\n",
            "\n",
            "0: 384x640 2 persons, 8.1ms\n",
            "Speed: 2.4ms preprocess, 8.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1091 saved to /content/drive/MyDrive/논문주제/ending/frame_1091.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 3.1ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1092 saved to /content/drive/MyDrive/논문주제/ending/frame_1092.png\n",
            "\n",
            "0: 384x640 2 persons, 8.7ms\n",
            "Speed: 3.4ms preprocess, 8.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1093 saved to /content/drive/MyDrive/논문주제/ending/frame_1093.png\n",
            "\n",
            "0: 384x640 2 persons, 8.0ms\n",
            "Speed: 3.2ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1094 saved to /content/drive/MyDrive/논문주제/ending/frame_1094.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 3.3ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1095 saved to /content/drive/MyDrive/논문주제/ending/frame_1095.png\n",
            "\n",
            "0: 384x640 2 persons, 11.7ms\n",
            "Speed: 3.5ms preprocess, 11.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1096 saved to /content/drive/MyDrive/논문주제/ending/frame_1096.png\n",
            "\n",
            "0: 384x640 2 persons, 10.6ms\n",
            "Speed: 3.3ms preprocess, 10.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1097 saved to /content/drive/MyDrive/논문주제/ending/frame_1097.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 3.0ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1098 saved to /content/drive/MyDrive/논문주제/ending/frame_1098.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.9ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1099 saved to /content/drive/MyDrive/논문주제/ending/frame_1099.png\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1100 saved to /content/drive/MyDrive/논문주제/ending/frame_1100.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 3.2ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1101 saved to /content/drive/MyDrive/논문주제/ending/frame_1101.png\n",
            "\n",
            "0: 384x640 2 persons, 8.6ms\n",
            "Speed: 2.4ms preprocess, 8.6ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1102 saved to /content/drive/MyDrive/논문주제/ending/frame_1102.png\n",
            "\n",
            "0: 384x640 2 persons, 8.5ms\n",
            "Speed: 3.1ms preprocess, 8.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1103 saved to /content/drive/MyDrive/논문주제/ending/frame_1103.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 3.3ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1104 saved to /content/drive/MyDrive/논문주제/ending/frame_1104.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.4ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1105 saved to /content/drive/MyDrive/논문주제/ending/frame_1105.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.6ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1106 saved to /content/drive/MyDrive/논문주제/ending/frame_1106.png\n",
            "\n",
            "0: 384x640 2 persons, 7.9ms\n",
            "Speed: 2.7ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1107 saved to /content/drive/MyDrive/논문주제/ending/frame_1107.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1108 saved to /content/drive/MyDrive/논문주제/ending/frame_1108.png\n",
            "\n",
            "0: 384x640 2 persons, 8.5ms\n",
            "Speed: 3.2ms preprocess, 8.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1109 saved to /content/drive/MyDrive/논문주제/ending/frame_1109.png\n",
            "\n",
            "0: 384x640 2 persons, 7.9ms\n",
            "Speed: 3.2ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1110 saved to /content/drive/MyDrive/논문주제/ending/frame_1110.png\n",
            "\n",
            "0: 384x640 2 persons, 8.0ms\n",
            "Speed: 2.5ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1111 saved to /content/drive/MyDrive/논문주제/ending/frame_1111.png\n",
            "\n",
            "0: 384x640 2 persons, 8.1ms\n",
            "Speed: 3.0ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1112 saved to /content/drive/MyDrive/논문주제/ending/frame_1112.png\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.9ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1113 saved to /content/drive/MyDrive/논문주제/ending/frame_1113.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 3.2ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1114 saved to /content/drive/MyDrive/논문주제/ending/frame_1114.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.8ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1115 saved to /content/drive/MyDrive/논문주제/ending/frame_1115.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.7ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1116 saved to /content/drive/MyDrive/논문주제/ending/frame_1116.png\n",
            "\n",
            "0: 384x640 2 persons, 8.1ms\n",
            "Speed: 2.7ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1117 saved to /content/drive/MyDrive/논문주제/ending/frame_1117.png\n",
            "\n",
            "0: 384x640 2 persons, 8.0ms\n",
            "Speed: 2.6ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1118 saved to /content/drive/MyDrive/논문주제/ending/frame_1118.png\n",
            "\n",
            "0: 384x640 2 persons, 8.0ms\n",
            "Speed: 2.4ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1119 saved to /content/drive/MyDrive/논문주제/ending/frame_1119.png\n",
            "\n",
            "0: 384x640 2 persons, 8.4ms\n",
            "Speed: 2.8ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1120 saved to /content/drive/MyDrive/논문주제/ending/frame_1120.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 3.1ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1121 saved to /content/drive/MyDrive/논문주제/ending/frame_1121.png\n",
            "\n",
            "0: 384x640 2 persons, 9.3ms\n",
            "Speed: 3.2ms preprocess, 9.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1122 saved to /content/drive/MyDrive/논문주제/ending/frame_1122.png\n",
            "\n",
            "0: 384x640 2 persons, 8.8ms\n",
            "Speed: 3.0ms preprocess, 8.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1123 saved to /content/drive/MyDrive/논문주제/ending/frame_1123.png\n",
            "\n",
            "0: 384x640 2 persons, 8.0ms\n",
            "Speed: 3.0ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1124 saved to /content/drive/MyDrive/논문주제/ending/frame_1124.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 3.0ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1125 saved to /content/drive/MyDrive/논문주제/ending/frame_1125.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1126 saved to /content/drive/MyDrive/논문주제/ending/frame_1126.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.5ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1127 saved to /content/drive/MyDrive/논문주제/ending/frame_1127.png\n",
            "\n",
            "0: 384x640 2 persons, 8.1ms\n",
            "Speed: 2.9ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1128 saved to /content/drive/MyDrive/논문주제/ending/frame_1128.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.9ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1129 saved to /content/drive/MyDrive/논문주제/ending/frame_1129.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 3.0ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1130 saved to /content/drive/MyDrive/논문주제/ending/frame_1130.png\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.3ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1131 saved to /content/drive/MyDrive/논문주제/ending/frame_1131.png\n",
            "\n",
            "0: 384x640 2 persons, 8.0ms\n",
            "Speed: 2.8ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1132 saved to /content/drive/MyDrive/논문주제/ending/frame_1132.png\n",
            "\n",
            "0: 384x640 2 persons, 8.5ms\n",
            "Speed: 2.9ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1133 saved to /content/drive/MyDrive/논문주제/ending/frame_1133.png\n",
            "\n",
            "0: 384x640 2 persons, 8.0ms\n",
            "Speed: 2.7ms preprocess, 8.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1134 saved to /content/drive/MyDrive/논문주제/ending/frame_1134.png\n",
            "\n",
            "0: 384x640 2 persons, 7.9ms\n",
            "Speed: 2.6ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1135 saved to /content/drive/MyDrive/논문주제/ending/frame_1135.png\n",
            "\n",
            "0: 384x640 2 persons, 8.2ms\n",
            "Speed: 2.7ms preprocess, 8.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1136 saved to /content/drive/MyDrive/논문주제/ending/frame_1136.png\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1137 saved to /content/drive/MyDrive/논문주제/ending/frame_1137.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 3.4ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1138 saved to /content/drive/MyDrive/논문주제/ending/frame_1138.png\n",
            "\n",
            "0: 384x640 2 persons, 7.9ms\n",
            "Speed: 2.8ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1139 saved to /content/drive/MyDrive/논문주제/ending/frame_1139.png\n",
            "\n",
            "0: 384x640 2 persons, 8.5ms\n",
            "Speed: 3.3ms preprocess, 8.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1140 saved to /content/drive/MyDrive/논문주제/ending/frame_1140.png\n",
            "\n",
            "0: 384x640 2 persons, 8.3ms\n",
            "Speed: 2.7ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1141 saved to /content/drive/MyDrive/논문주제/ending/frame_1141.png\n",
            "\n",
            "0: 384x640 2 persons, 9.6ms\n",
            "Speed: 2.4ms preprocess, 9.6ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1142 saved to /content/drive/MyDrive/논문주제/ending/frame_1142.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 2.9ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1143 saved to /content/drive/MyDrive/논문주제/ending/frame_1143.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 3.1ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1144 saved to /content/drive/MyDrive/논문주제/ending/frame_1144.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.8ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1145 saved to /content/drive/MyDrive/논문주제/ending/frame_1145.png\n",
            "\n",
            "0: 384x640 2 persons, 7.9ms\n",
            "Speed: 2.4ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1146 saved to /content/drive/MyDrive/논문주제/ending/frame_1146.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 3.3ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1147 saved to /content/drive/MyDrive/논문주제/ending/frame_1147.png\n",
            "\n",
            "0: 384x640 2 persons, 8.5ms\n",
            "Speed: 2.9ms preprocess, 8.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1148 saved to /content/drive/MyDrive/논문주제/ending/frame_1148.png\n",
            "\n",
            "0: 384x640 2 persons, 11.1ms\n",
            "Speed: 3.3ms preprocess, 11.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1149 saved to /content/drive/MyDrive/논문주제/ending/frame_1149.png\n",
            "\n",
            "0: 384x640 2 persons, 10.5ms\n",
            "Speed: 3.7ms preprocess, 10.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1150 saved to /content/drive/MyDrive/논문주제/ending/frame_1150.png\n",
            "\n",
            "0: 384x640 2 persons, 8.3ms\n",
            "Speed: 3.2ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1151 saved to /content/drive/MyDrive/논문주제/ending/frame_1151.png\n",
            "\n",
            "0: 384x640 2 persons, 8.0ms\n",
            "Speed: 2.5ms preprocess, 8.0ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1152 saved to /content/drive/MyDrive/논문주제/ending/frame_1152.png\n",
            "\n",
            "0: 384x640 2 persons, 8.9ms\n",
            "Speed: 3.0ms preprocess, 8.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1153 saved to /content/drive/MyDrive/논문주제/ending/frame_1153.png\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1154 saved to /content/drive/MyDrive/논문주제/ending/frame_1154.png\n",
            "\n",
            "0: 384x640 2 persons, 11.1ms\n",
            "Speed: 3.3ms preprocess, 11.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1155 saved to /content/drive/MyDrive/논문주제/ending/frame_1155.png\n",
            "\n",
            "0: 384x640 2 persons, 8.0ms\n",
            "Speed: 2.5ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1156 saved to /content/drive/MyDrive/논문주제/ending/frame_1156.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 3.0ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1157 saved to /content/drive/MyDrive/논문주제/ending/frame_1157.png\n",
            "\n",
            "0: 384x640 2 persons, 7.9ms\n",
            "Speed: 2.3ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1158 saved to /content/drive/MyDrive/논문주제/ending/frame_1158.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 2.7ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1159 saved to /content/drive/MyDrive/논문주제/ending/frame_1159.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 3.1ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1160 saved to /content/drive/MyDrive/논문주제/ending/frame_1160.png\n",
            "\n",
            "0: 384x640 2 persons, 8.5ms\n",
            "Speed: 2.7ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1161 saved to /content/drive/MyDrive/논문주제/ending/frame_1161.png\n",
            "\n",
            "0: 384x640 2 persons, 10.6ms\n",
            "Speed: 3.2ms preprocess, 10.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1162 saved to /content/drive/MyDrive/논문주제/ending/frame_1162.png\n",
            "\n",
            "0: 384x640 2 persons, 7.9ms\n",
            "Speed: 2.6ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1163 saved to /content/drive/MyDrive/논문주제/ending/frame_1163.png\n",
            "\n",
            "0: 384x640 2 persons, 12.1ms\n",
            "Speed: 2.9ms preprocess, 12.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1164 saved to /content/drive/MyDrive/논문주제/ending/frame_1164.png\n",
            "\n",
            "0: 384x640 2 persons, 10.8ms\n",
            "Speed: 3.4ms preprocess, 10.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1165 saved to /content/drive/MyDrive/논문주제/ending/frame_1165.png\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.7ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1166 saved to /content/drive/MyDrive/논문주제/ending/frame_1166.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 2.9ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1167 saved to /content/drive/MyDrive/논문주제/ending/frame_1167.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1168 saved to /content/drive/MyDrive/논문주제/ending/frame_1168.png\n",
            "\n",
            "0: 384x640 2 persons, 8.0ms\n",
            "Speed: 2.9ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1169 saved to /content/drive/MyDrive/논문주제/ending/frame_1169.png\n",
            "\n",
            "0: 384x640 2 persons, 7.9ms\n",
            "Speed: 2.6ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1170 saved to /content/drive/MyDrive/논문주제/ending/frame_1170.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.6ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1171 saved to /content/drive/MyDrive/논문주제/ending/frame_1171.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.9ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1172 saved to /content/drive/MyDrive/논문주제/ending/frame_1172.png\n",
            "\n",
            "0: 384x640 2 persons, 12.1ms\n",
            "Speed: 3.1ms preprocess, 12.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1173 saved to /content/drive/MyDrive/논문주제/ending/frame_1173.png\n",
            "\n",
            "0: 384x640 2 persons, 8.4ms\n",
            "Speed: 2.6ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1174 saved to /content/drive/MyDrive/논문주제/ending/frame_1174.png\n",
            "\n",
            "0: 384x640 2 persons, 8.4ms\n",
            "Speed: 2.8ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1175 saved to /content/drive/MyDrive/논문주제/ending/frame_1175.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.8ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1176 saved to /content/drive/MyDrive/논문주제/ending/frame_1176.png\n",
            "\n",
            "0: 384x640 2 persons, 9.9ms\n",
            "Speed: 2.9ms preprocess, 9.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1177 saved to /content/drive/MyDrive/논문주제/ending/frame_1177.png\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.9ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1178 saved to /content/drive/MyDrive/논문주제/ending/frame_1178.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 2.4ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1179 saved to /content/drive/MyDrive/논문주제/ending/frame_1179.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.8ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1180 saved to /content/drive/MyDrive/논문주제/ending/frame_1180.png\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 3.0ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1181 saved to /content/drive/MyDrive/논문주제/ending/frame_1181.png\n",
            "\n",
            "0: 384x640 2 persons, 8.7ms\n",
            "Speed: 3.5ms preprocess, 8.7ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1182 saved to /content/drive/MyDrive/논문주제/ending/frame_1182.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.8ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1183 saved to /content/drive/MyDrive/논문주제/ending/frame_1183.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.4ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1184 saved to /content/drive/MyDrive/논문주제/ending/frame_1184.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.8ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1185 saved to /content/drive/MyDrive/논문주제/ending/frame_1185.png\n",
            "\n",
            "0: 384x640 2 persons, 8.9ms\n",
            "Speed: 2.6ms preprocess, 8.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1186 saved to /content/drive/MyDrive/논문주제/ending/frame_1186.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.6ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1187 saved to /content/drive/MyDrive/논문주제/ending/frame_1187.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.8ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1188 saved to /content/drive/MyDrive/논문주제/ending/frame_1188.png\n",
            "\n",
            "0: 384x640 2 persons, 8.1ms\n",
            "Speed: 3.3ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1189 saved to /content/drive/MyDrive/논문주제/ending/frame_1189.png\n",
            "\n",
            "0: 384x640 2 persons, 7.9ms\n",
            "Speed: 2.8ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1190 saved to /content/drive/MyDrive/논문주제/ending/frame_1190.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.8ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1191 saved to /content/drive/MyDrive/논문주제/ending/frame_1191.png\n",
            "\n",
            "0: 384x640 2 persons, 8.0ms\n",
            "Speed: 3.2ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1192 saved to /content/drive/MyDrive/논문주제/ending/frame_1192.png\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.9ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1193 saved to /content/drive/MyDrive/논문주제/ending/frame_1193.png\n",
            "\n",
            "0: 384x640 2 persons, 8.2ms\n",
            "Speed: 3.2ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1194 saved to /content/drive/MyDrive/논문주제/ending/frame_1194.png\n",
            "\n",
            "0: 384x640 2 persons, 8.9ms\n",
            "Speed: 3.0ms preprocess, 8.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1195 saved to /content/drive/MyDrive/논문주제/ending/frame_1195.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.8ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1196 saved to /content/drive/MyDrive/논문주제/ending/frame_1196.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.7ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1197 saved to /content/drive/MyDrive/논문주제/ending/frame_1197.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1198 saved to /content/drive/MyDrive/논문주제/ending/frame_1198.png\n",
            "\n",
            "0: 384x640 2 persons, 8.4ms\n",
            "Speed: 3.0ms preprocess, 8.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1199 saved to /content/drive/MyDrive/논문주제/ending/frame_1199.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 2.9ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1200 saved to /content/drive/MyDrive/논문주제/ending/frame_1200.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 3.2ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1201 saved to /content/drive/MyDrive/논문주제/ending/frame_1201.png\n",
            "\n",
            "0: 384x640 2 persons, 8.2ms\n",
            "Speed: 3.2ms preprocess, 8.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1202 saved to /content/drive/MyDrive/논문주제/ending/frame_1202.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1203 saved to /content/drive/MyDrive/논문주제/ending/frame_1203.png\n",
            "\n",
            "0: 384x640 2 persons, 8.0ms\n",
            "Speed: 2.8ms preprocess, 8.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1204 saved to /content/drive/MyDrive/논문주제/ending/frame_1204.png\n",
            "\n",
            "0: 384x640 2 persons, 8.0ms\n",
            "Speed: 3.0ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1205 saved to /content/drive/MyDrive/논문주제/ending/frame_1205.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 2.8ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1206 saved to /content/drive/MyDrive/논문주제/ending/frame_1206.png\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.8ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1207 saved to /content/drive/MyDrive/논문주제/ending/frame_1207.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 3.4ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1208 saved to /content/drive/MyDrive/논문주제/ending/frame_1208.png\n",
            "\n",
            "0: 384x640 2 persons, 8.2ms\n",
            "Speed: 2.3ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1209 saved to /content/drive/MyDrive/논문주제/ending/frame_1209.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.9ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1210 saved to /content/drive/MyDrive/논문주제/ending/frame_1210.png\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.5ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1211 saved to /content/drive/MyDrive/논문주제/ending/frame_1211.png\n",
            "\n",
            "0: 384x640 2 persons, 9.3ms\n",
            "Speed: 3.3ms preprocess, 9.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1212 saved to /content/drive/MyDrive/논문주제/ending/frame_1212.png\n",
            "\n",
            "0: 384x640 2 persons, 8.2ms\n",
            "Speed: 2.8ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1213 saved to /content/drive/MyDrive/논문주제/ending/frame_1213.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 3.1ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1214 saved to /content/drive/MyDrive/논문주제/ending/frame_1214.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 3.1ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1215 saved to /content/drive/MyDrive/논문주제/ending/frame_1215.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.3ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1216 saved to /content/drive/MyDrive/논문주제/ending/frame_1216.png\n",
            "\n",
            "0: 384x640 2 persons, 8.5ms\n",
            "Speed: 2.6ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1217 saved to /content/drive/MyDrive/논문주제/ending/frame_1217.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 3.1ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1218 saved to /content/drive/MyDrive/논문주제/ending/frame_1218.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.7ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1219 saved to /content/drive/MyDrive/논문주제/ending/frame_1219.png\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.6ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1220 saved to /content/drive/MyDrive/논문주제/ending/frame_1220.png\n",
            "\n",
            "0: 384x640 2 persons, 8.2ms\n",
            "Speed: 2.7ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1221 saved to /content/drive/MyDrive/논문주제/ending/frame_1221.png\n",
            "\n",
            "0: 384x640 2 persons, 12.5ms\n",
            "Speed: 3.3ms preprocess, 12.5ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1222 saved to /content/drive/MyDrive/논문주제/ending/frame_1222.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1223 saved to /content/drive/MyDrive/논문주제/ending/frame_1223.png\n",
            "\n",
            "0: 384x640 2 persons, 8.8ms\n",
            "Speed: 2.6ms preprocess, 8.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1224 saved to /content/drive/MyDrive/논문주제/ending/frame_1224.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.6ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1225 saved to /content/drive/MyDrive/논문주제/ending/frame_1225.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 2.7ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1226 saved to /content/drive/MyDrive/논문주제/ending/frame_1226.png\n",
            "\n",
            "0: 384x640 2 persons, 8.3ms\n",
            "Speed: 2.7ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1227 saved to /content/drive/MyDrive/논문주제/ending/frame_1227.png\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.6ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1228 saved to /content/drive/MyDrive/논문주제/ending/frame_1228.png\n",
            "\n",
            "0: 384x640 2 persons, 9.1ms\n",
            "Speed: 2.8ms preprocess, 9.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1229 saved to /content/drive/MyDrive/논문주제/ending/frame_1229.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 3.1ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1230 saved to /content/drive/MyDrive/논문주제/ending/frame_1230.png\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.5ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1231 saved to /content/drive/MyDrive/논문주제/ending/frame_1231.png\n",
            "\n",
            "0: 384x640 2 persons, 8.3ms\n",
            "Speed: 2.8ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1232 saved to /content/drive/MyDrive/논문주제/ending/frame_1232.png\n",
            "\n",
            "0: 384x640 2 persons, 7.9ms\n",
            "Speed: 3.0ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1233 saved to /content/drive/MyDrive/논문주제/ending/frame_1233.png\n",
            "\n",
            "0: 384x640 2 persons, 11.2ms\n",
            "Speed: 3.3ms preprocess, 11.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1234 saved to /content/drive/MyDrive/논문주제/ending/frame_1234.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.6ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1235 saved to /content/drive/MyDrive/논문주제/ending/frame_1235.png\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.9ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1236 saved to /content/drive/MyDrive/논문주제/ending/frame_1236.png\n",
            "\n",
            "0: 384x640 2 persons, 9.0ms\n",
            "Speed: 2.8ms preprocess, 9.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1237 saved to /content/drive/MyDrive/논문주제/ending/frame_1237.png\n",
            "\n",
            "0: 384x640 2 persons, 11.1ms\n",
            "Speed: 3.3ms preprocess, 11.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1238 saved to /content/drive/MyDrive/논문주제/ending/frame_1238.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.6ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1239 saved to /content/drive/MyDrive/논문주제/ending/frame_1239.png\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 2.8ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1240 saved to /content/drive/MyDrive/논문주제/ending/frame_1240.png\n",
            "\n",
            "0: 384x640 2 persons, 10.2ms\n",
            "Speed: 3.5ms preprocess, 10.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1241 saved to /content/drive/MyDrive/논문주제/ending/frame_1241.png\n",
            "\n",
            "0: 384x640 2 persons, 8.1ms\n",
            "Speed: 3.4ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1242 saved to /content/drive/MyDrive/논문주제/ending/frame_1242.png\n",
            "\n",
            "0: 384x640 2 persons, 10.7ms\n",
            "Speed: 3.3ms preprocess, 10.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1243 saved to /content/drive/MyDrive/논문주제/ending/frame_1243.png\n",
            "\n",
            "0: 384x640 2 persons, 10.3ms\n",
            "Speed: 2.8ms preprocess, 10.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1244 saved to /content/drive/MyDrive/논문주제/ending/frame_1244.png\n",
            "\n",
            "0: 384x640 2 persons, 8.6ms\n",
            "Speed: 3.3ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1245 saved to /content/drive/MyDrive/논문주제/ending/frame_1245.png\n",
            "\n",
            "0: 384x640 2 persons, 7.9ms\n",
            "Speed: 2.8ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1246 saved to /content/drive/MyDrive/논문주제/ending/frame_1246.png\n",
            "\n",
            "0: 384x640 2 persons, 8.0ms\n",
            "Speed: 3.2ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1247 saved to /content/drive/MyDrive/논문주제/ending/frame_1247.png\n",
            "\n",
            "0: 384x640 2 persons, 9.5ms\n",
            "Speed: 2.8ms preprocess, 9.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1248 saved to /content/drive/MyDrive/논문주제/ending/frame_1248.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 2.9ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1249 saved to /content/drive/MyDrive/논문주제/ending/frame_1249.png\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 3.2ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1250 saved to /content/drive/MyDrive/논문주제/ending/frame_1250.png\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.6ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1251 saved to /content/drive/MyDrive/논문주제/ending/frame_1251.png\n",
            "\n",
            "0: 384x640 2 persons, 8.1ms\n",
            "Speed: 3.3ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 1252 saved to /content/drive/MyDrive/논문주제/ending/frame_1252.png\n",
            "\n",
            "0: 384x640 2 persons, 8.5ms\n",
            "Speed: 3.1ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tP9o6dy9dvdb"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBPjOByodxIf"
      },
      "source": [
        "영상테스트"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}